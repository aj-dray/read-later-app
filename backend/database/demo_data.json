{"exported_at": "2025-10-02T09:02:59.601162+00:00", "source": {"base_url": "http://localhost:8000"}, "user": {"username": "demo", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3"}, "items": [{"id": "d1e2d227-0aa1-42c3-8d06-83aa8d73da37", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://www.anthropic.com/engineering/building-effective-agents", "canonical_url": null, "title": "Building Effective AI Agents", "source_site": "AnthropicAI", "publication_date": "2024-12-19T00:00:00Z", "favicon_url": "https://www.anthropic.com/favicon.ico", "content_markdown": "Over the past year, we've worked with dozens of teams building large language model (LLM) agents across industries. Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns.\n\nIn this post, we share what we’ve learned from working with our customers and building agents ourselves, and give practical advice for developers on building effective agents.\n\n## What are agents?\n\n\"Agent\" can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as **agentic systems**, but draw an important architectural distinction between **workflows **and** agents**:\n\n**Workflows**are systems where LLMs and tools are orchestrated through predefined code paths.**Agents**, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\n\nBelow, we will explore both types of agentic systems in detail. In Appendix 1 (“Agents in Practice”), we describe two domains where customers have found particular value in using these kinds of systems.\n\n## When (and when not) to use agents\n\nWhen building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense.\n\nWhen more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough.\n\n## When and how to use frameworks\n\nThere are many frameworks that make agentic systems easier to implement, including:\n\n[LangGraph](https://langchain-ai.github.io/langgraph/)from LangChain;- Amazon Bedrock's\n[AI Agent framework](https://aws.amazon.com/bedrock/agents/); [Rivet](https://rivet.ironcladapp.com/), a drag and drop GUI LLM workflow builder; and[Vellum](https://www.vellum.ai/), another GUI tool for building and testing complex workflows.\n\nThese frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.\n\nWe suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.\n\nSee our [cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents) for some sample implementations.\n\n## Building blocks, workflows, and agents\n\nIn this section, we’ll explore the common patterns for agentic systems we’ve seen in production. We'll start with our foundational building block—the augmented LLM—and progressively increase complexity, from simple compositional workflows to autonomous agents.\n\n### Building block: The augmented LLM\n\nThe basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilities—generating their own search queries, selecting appropriate tools, and determining what information to retain.\n\nWe recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently released [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol), which allows developers to integrate with a growing ecosystem of third-party tools with a simple [client implementation](https://modelcontextprotocol.io/tutorials/building-a-client#building-mcp-clients).\n\nFor the remainder of this post, we'll assume each LLM call has access to these augmented capabilities.\n\n### Workflow: Prompt chaining\n\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.\n\n**When to use this workflow:** This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\n\n**Examples where prompt chaining is useful:**\n\n- Generating Marketing copy, then translating it into a different language.\n- Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline.\n\n### Workflow: Routing\n\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\n\n**When to use this workflow:** Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\n\n**Examples where routing is useful:**\n\n- Directing different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools.\n- Routing easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to optimize cost and speed.\n\n### Workflow: Parallelization\n\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations:\n\n**Sectioning**: Breaking a task into independent subtasks run in parallel.**Voting:**Running the same task multiple times to get diverse outputs.\n\n**When to use this workflow:** Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\n\n**Examples where parallelization is useful:**\n\n**Sectioning**:- Implementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response.\n- Automating evals for evaluating LLM performance, where each LLM call evaluates a different aspect of the model’s performance on a given prompt.\n\n**Voting**:- Reviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem.\n- Evaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives.\n\n\n### Workflow: Orchestrator-workers\n\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\n\n**When to use this workflow:** This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren't pre-defined, but determined by the orchestrator based on the specific input.\n\n**Example where orchestrator-workers is useful:**\n\n- Coding products that make complex changes to multiple files each time.\n- Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information.\n\n### Workflow: Evaluator-optimizer\n\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\n\n**When to use this workflow:** This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\n\n**Examples where evaluator-optimizer is useful:**\n\n- Literary translation where there are nuances that the translator LLM might not capture initially, but where an evaluator LLM can provide useful critiques.\n- Complex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted.\n\n### Agents\n\nAgents are emerging in production as LLMs mature in key capabilities—understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain “ground truth” from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it’s also common to include stopping conditions (such as a maximum number of iterations) to maintain control.\n\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 (\"Prompt Engineering your Tools\").\n\n**When to use agents:** Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments.\n\nThe autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails.\n\n**Examples where agents are useful:**\n\nThe following examples are from our own implementations:\n\n- A coding Agent to resolve\n[SWE-bench tasks](https://www.anthropic.com/research/swe-bench-sonnet), which involve edits to many files based on a task description; - Our\n[“computer use” reference implementation](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo), where Claude uses a computer to accomplish tasks.\n\n## Combining and customizing these patterns\n\nThese building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity *only* when it demonstrably improves outcomes.\n\n## Summary\n\nSuccess in the LLM space isn't about building the most sophisticated system. It's about building the *right* system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short.\n\nWhen implementing agents, we try to follow three core principles:\n\n- Maintain\n**simplicity**in your agent's design. - Prioritize\n**transparency**by explicitly showing the agent’s planning steps. - Carefully craft your agent-computer interface (ACI) through thorough tool\n**documentation and testing**.\n\nFrameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production. By following these principles, you can create agents that are not only powerful but also reliable, maintainable, and trusted by their users.\n\n### Acknowledgements\n\nWritten by Erik Schluntz and Barry Zhang. This work draws upon our experiences building agents at Anthropic and the valuable insights shared by our customers, for which we're deeply grateful.\n\n## Appendix 1: Agents in practice\n\nOur work with customers has revealed two particularly promising applications for AI agents that demonstrate the practical value of the patterns discussed above. Both applications illustrate how agents add the most value for tasks that require both conversation and action, have clear success criteria, enable feedback loops, and integrate meaningful human oversight.\n\n### A. Customer support\n\nCustomer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:\n\n- Support interactions naturally follow a conversation flow while requiring access to external information and actions;\n- Tools can be integrated to pull customer data, order history, and knowledge base articles;\n- Actions such as issuing refunds or updating tickets can be handled programmatically; and\n- Success can be clearly measured through user-defined resolutions.\n\nSeveral companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness.\n\n### B. Coding agents\n\nThe software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:\n\n- Code solutions are verifiable through automated tests;\n- Agents can iterate on solutions using test results as feedback;\n- The problem space is well-defined and structured; and\n- Output quality can be measured objectively.\n\nIn our own implementation, agents can now solve real GitHub issues in the [SWE-bench Verified](https://www.anthropic.com/research/swe-bench-sonnet) benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.\n\n## Appendix 2: Prompt engineering your tools\n\nNo matter which agentic system you're building, tools will likely be an important part of your agent. [Tools](https://www.anthropic.com/news/tool-use-ga) enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a [tool use block](https://docs.anthropic.com/en/docs/build-with-claude/tool-use#example-api-response-with-a-tool-use-content-block) in the API response if it plans to invoke a tool. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. In this brief appendix, we describe how to prompt engineer your tools.\n\nThere are often several ways to specify the same action. For instance, you can specify a file edit by writing a diff, or by rewriting the entire file. For structured output, you can return code inside markdown or inside JSON. In software engineering, differences like these are cosmetic and can be converted losslessly from one to the other. However, some formats are much more difficult for an LLM to write than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes.\n\nOur suggestions for deciding on tool formats are the following:\n\n- Give the model enough tokens to \"think\" before it writes itself into a corner.\n- Keep the format close to what the model has seen naturally occurring in text on the internet.\n- Make sure there's no formatting \"overhead\" such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes.\n\nOne rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good *agent*-computer interfaces (ACI). Here are some thoughts on how to do so:\n\n- Put yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it’s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other tools.\n- How can you change parameter names or descriptions to make things more obvious? Think of this as writing a great docstring for a junior developer on your team. This is especially important when using many similar tools.\n- Test how the model uses your tools: Run many example inputs in our\n[workbench](https://console.anthropic.com/workbench)to see what mistakes the model makes, and iterate. [Poka-yoke](https://en.wikipedia.org/wiki/Poka-yoke)your tools. Change the arguments so that it is harder to make mistakes.\n\nWhile building our agent for [SWE-bench](https://www.anthropic.com/research/swe-bench-sonnet), we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths—and we found that the model used this method flawlessly.", "content_text": "We've worked with dozens of teams building LLM agents across industries. Consistently, the most successful implementations use simple, composable patterns rather than complex frameworks.\nOver the past year, we've worked with dozens of teams building large language model (LLM) agents across industries. Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns.\nIn this post, we share what we’ve learned from working with our customers and building agents ourselves, and give practical advice for developers on building effective agents.\nWhat are agents?\n\"Agent\" can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as agentic systems, but draw an important architectural distinction between workflows and agents:\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\nBelow, we will explore both types of agentic systems in detail. In Appendix 1 (“Agents in Practice”), we describe two domains where customers have found particular value in using these kinds of systems.\nWhen (and when not) to use agents\nWhen building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense.\nWhen more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough.\nWhen and how to use frameworks\nThere are many frameworks that make agentic systems easier to implement, including:\nRivet, a drag and drop GUI LLM workflow builder; and\nVellum, another GUI tool for building and testing complex workflows.\nThese frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.\nWe suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.\nIn this section, we’ll explore the common patterns for agentic systems we’ve seen in production. We'll start with our foundational building block—the augmented LLM—and progressively increase complexity, from simple compositional workflows to autonomous agents.\nBuilding block: The augmented LLM\nThe basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilities—generating their own search queries, selecting appropriate tools, and determining what information to retain.\nWe recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently released Model Context Protocol, which allows developers to integrate with a growing ecosystem of third-party tools with a simple client implementation.\nFor the remainder of this post, we'll assume each LLM call has access to these augmented capabilities.\nWorkflow: Prompt chaining\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.\nWhen to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\nExamples where prompt chaining is useful:\nGenerating Marketing copy, then translating it into a different language.\nWriting an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline.\nWorkflow: Routing\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\nWhen to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\nExamples where routing is useful:\nDirecting different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools.\nRouting easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to optimize cost and speed.\nWorkflow: Parallelization\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations:\nSectioning: Breaking a task into independent subtasks run in parallel.\nVoting: Running the same task multiple times to get diverse outputs.\nWhen to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\nExamples where parallelization is useful:\nSectioning:\nImplementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response.\nAutomating evals for evaluating LLM performance, where each LLM call evaluates a different aspect of the model’s performance on a given prompt.\nVoting:\nReviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem.\nEvaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives.\nWorkflow: Orchestrator-workers\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\nWhen to use this workflow: This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren't pre-defined, but determined by the orchestrator based on the specific input.\nExample where orchestrator-workers is useful:\nCoding products that make complex changes to multiple files each time.\nSearch tasks that involve gathering and analyzing information from multiple sources for possible relevant information.\nWorkflow: Evaluator-optimizer\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\nWhen to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\nExamples where evaluator-optimizer is useful:\nLiterary translation where there are nuances that the translator LLM might not capture initially, but where an evaluator LLM can provide useful critiques.\nComplex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted.\nAgents\nAgents are emerging in production as LLMs mature in key capabilities—understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain “ground truth” from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it’s also common to include stopping conditions (such as a maximum number of iterations) to maintain control.\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 (\"Prompt Engineering your Tools\").\nWhen to use agents: Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments.\nThe autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails.\nExamples where agents are useful:\nThe following examples are from our own implementations:\nA coding Agent to resolve SWE-bench tasks, which involve edits to many files based on a task description;\nThese building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity only when it demonstrably improves outcomes.\nSummary\nSuccess in the LLM space isn't about building the most sophisticated system. It's about building the right system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short.\nWhen implementing agents, we try to follow three core principles:\nMaintain simplicity in your agent's design.\nPrioritize transparency by explicitly showing the agent’s planning steps.\nCarefully craft your agent-computer interface (ACI) through thorough tool documentation and testing.\nFrameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production. By following these principles, you can create agents that are not only powerful but also reliable, maintainable, and trusted by their users.\nAcknowledgements\nWritten by Erik Schluntz and Barry Zhang. This work draws upon our experiences building agents at Anthropic and the valuable insights shared by our customers, for which we're deeply grateful.\nAppendix 1: Agents in practice\nOur work with customers has revealed two particularly promising applications for AI agents that demonstrate the practical value of the patterns discussed above. Both applications illustrate how agents add the most value for tasks that require both conversation and action, have clear success criteria, enable feedback loops, and integrate meaningful human oversight.\nA. Customer support\nCustomer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:\nSupport interactions naturally follow a conversation flow while requiring access to external information and actions;\nTools can be integrated to pull customer data, order history, and knowledge base articles;\nActions such as issuing refunds or updating tickets can be handled programmatically; and\nSuccess can be clearly measured through user-defined resolutions.\nSeveral companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness.\nB. Coding agents\nThe software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:\nCode solutions are verifiable through automated tests;\nAgents can iterate on solutions using test results as feedback;\nThe problem space is well-defined and structured; and\nOutput quality can be measured objectively.\nIn our own implementation, agents can now solve real GitHub issues in the SWE-bench Verified benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.\nAppendix 2: Prompt engineering your tools\nNo matter which agentic system you're building, tools will likely be an important part of your agent. Tools enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a tool use block in the API response if it plans to invoke a tool. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. In this brief appendix, we describe how to prompt engineer your tools.\nThere are often several ways to specify the same action. For instance, you can specify a file edit by writing a diff, or by rewriting the entire file. For structured output, you can return code inside markdown or inside JSON. In software engineering, differences like these are cosmetic and can be converted losslessly from one to the other. However, some formats are much more difficult for an LLM to write than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes.\nOur suggestions for deciding on tool formats are the following:\nGive the model enough tokens to \"think\" before it writes itself into a corner.\nKeep the format close to what the model has seen naturally occurring in text on the internet.\nMake sure there's no formatting \"overhead\" such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes.\nOne rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good agent-computer interfaces (ACI). Here are some thoughts on how to do so:\nPut yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it’s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other tools.\nHow can you change parameter names or descriptions to make things more obvious? Think of this as writing a great docstring for a junior developer on your team. This is especially important when using many similar tools.\nTest how the model uses your tools: Run many example inputs in our workbench to see what mistakes the model makes, and iterate.\nPoka-yoke your tools. Change the arguments so that it is harder to make mistakes.\nWhile building our agent for SWE-bench, we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths—and we found that the model used this method flawlessly.\nGet the developer newsletter\nProduct updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.", "content_token_count": 3268, "client_status": "queued", "server_status": "embedded", "summary": "The article from AnthropicAI discusses the development and implementation of effective AI agents using large language models (LLMs). It explores various types of agentic systems, including workflows and agents, and provides practical advice for developers on when and how to use these systems, emphasizing simplicity and transparency.", "expiry_score": 0.3, "mistral_embedding": "[-0.053863525,0.046295166,0.03390503,0.0048675537,0.018981934,0.012298584,0.0519104,-0.0104599,-0.0158844,0.011383057,-0.02519226,0.08135986,-0.058898926,-0.00075387955,-0.045532227,0.05038452,-0.0057640076,0.027114868,0.05618286,0.023834229,-0.009689331,-0.0008778572,-0.055419922,-0.004940033,-0.0033168793,-0.017242432,0.016860962,-0.085998535,-0.0418396,-0.016662598,0.0035591125,-0.02519226,0.017333984,-0.0003633499,0.017044067,0.0041885376,-0.014526367,-0.046295166,0.0003466606,-0.021896362,0.01637268,-0.007167816,-0.0030994415,0.0015859604,-0.00920105,-0.0014896393,0.006126404,-0.010803223,-0.0109939575,0.0024585724,-0.035461426,0.04107666,-0.029830933,-0.013465881,-0.0284729,0.026153564,0.014526367,0.015594482,-0.045135498,0.012062073,-0.028671265,-0.010658264,0.03274536,-0.018692017,0.029251099,0.040496826,-0.009300232,-0.009925842,-0.0073623657,-0.06317139,0.013656616,0.01235199,0.0018892288,0.018310547,-0.03274536,-0.026733398,0.01966858,0.022766113,0.024307251,-0.018112183,-0.024795532,0.023834229,0.06781006,-0.017150879,-0.037200928,-0.015792847,-0.0129776,0.0019006729,-0.004650116,0.007507324,0.039123535,0.012886047,0.026931763,-0.034301758,0.047454834,0.029251099,0.0011806488,0.010894775,-0.005886078,0.060455322,0.033325195,-0.0050621033,0.085632324,-0.018692017,0.046691895,-0.02305603,0.05154419,0.007118225,0.005519867,-0.046691895,-0.1038208,-0.016464233,0.014335632,-0.07672119,-0.019180298,0.009880066,-0.013267517,0.05657959,-0.057739258,-0.06237793,0.037963867,0.04727173,-0.03060913,0.015403748,-0.04067993,-0.0041885376,0.017242432,0.050750732,-0.0030879974,-0.0076522827,-0.026931763,-0.009735107,-0.03466797,-0.03970337,0.03817749,-0.021896362,0.047851562,-0.011917114,-0.027114868,-0.02557373,0.026153564,-0.023925781,-0.01626587,-0.009010315,0.0038013458,-0.018508911,-0.006248474,0.035064697,-0.0010719299,-0.014625549,0.0045776367,0.029052734,0.054229736,-0.005619049,0.0010290146,-0.026733398,-0.027893066,-0.0076522827,0.05230713,-0.004627228,-0.02178955,0.004917145,-0.015975952,0.021209717,-0.010559082,-0.02809143,-0.022567749,-0.0057868958,-0.027114868,-0.025375366,0.03817749,-0.03970337,0.01423645,0.03604126,-0.012397766,0.013076782,0.0073127747,0.0024814606,0.06317139,0.017623901,-0.008377075,-0.042419434,0.028289795,0.028289795,-0.0053520203,-0.040283203,-0.068603516,0.00035119057,-0.00039958954,0.029647827,-0.0076522827,-0.022567749,-0.0368042,0.012397766,0.0016222,0.03060913,0.019561768,-0.037384033,0.03274536,-0.028289795,0.029251099,0.029449463,-0.017333984,0.06317139,0.0003964901,0.03274536,0.006877899,0.013465881,0.021896362,-0.022964478,-0.00920105,0.020141602,-0.013465881,-0.013557434,-0.029830933,-0.0259552,0.0076522827,-0.019760132,-0.02557373,0.009155273,-0.0309906,-0.042633057,-0.013656616,-0.012252808,0.035827637,0.0635376,0.012107849,0.012496948,-0.022277832,0.004261017,0.034088135,0.047851562,-0.0035114288,-0.00053596497,-0.020141602,-0.01637268,0.03274536,0.04473877,0.0016946793,0.027313232,0.004989624,0.027511597,-0.03970337,0.039123535,0.024124146,-0.0368042,-0.013267517,-0.0107040405,-0.008377075,0.05734253,-0.06781006,-0.003414154,-0.040496826,-0.008575439,-0.08679199,-0.026931763,-0.0184021,-0.042236328,0.05114746,-0.049987793,-0.03466797,0.022369385,0.0063934326,0.01966858,0.005592346,0.024124146,-0.043395996,0.017333984,0.049987793,0.003753662,-0.011917114,-0.030807495,0.0112838745,0.015304565,0.059295654,-0.031188965,-0.002325058,0.021987915,-0.0013685226,-0.0064888,-0.0045051575,0.033325195,0.010658264,0.0184021,-0.037200928,-0.011528015,0.0036087036,-0.0009202957,-0.0044555664,-0.048614502,0.0104599,0.03970337,0.055786133,-0.018508911,-0.005569458,-0.0184021,0.0317688,0.006877899,0.025772095,0.040496826,0.04824829,-0.026733398,-0.026931763,0.004940033,0.058898926,-0.0020942688,-0.0519104,-0.023544312,-0.021408081,-0.05307007,-0.004043579,-0.01966858,0.00774765,-0.013076782,-0.013946533,-0.004650116,-0.055419922,-0.033721924,-0.008430481,-0.014045715,-0.01626587,-0.009010315,0.058898926,-0.014335632,0.00522995,0.026535034,-0.0029296875,-0.04473877,0.0309906,-0.0069732666,0.01927185,-0.012062073,0.029830933,0.05618286,0.027893066,0.006828308,0.048828125,0.0184021,-0.04473877,0.022369385,-0.03060913,0.00038599968,0.03894043,-0.013946533,-0.009010315,-0.024414062,-0.01927185,-0.01675415,0.005279541,-0.0035362244,0.0054244995,0.013076782,-0.021026611,0.05734253,-0.0070724487,0.09063721,-0.031188965,-0.037597656,-0.01423645,0.031188965,-0.012207031,0.0050621033,0.042236328,0.014823914,0.009635925,0.011672974,-0.008377075,0.0037059784,-0.0050849915,0.024597168,0.025375366,0.030807495,0.059661865,0.04321289,-0.0016832352,0.014526367,-0.0025672913,-0.029449463,0.018203735,-0.010559082,-0.044158936,0.015975952,0.04067993,0.055419922,-0.00818634,0.0002603531,-0.048614502,-0.013656616,0.00983429,0.07788086,0.0569458,0.042419434,0.0066833496,-0.010025024,0.013076782,-0.028671265,-0.0041160583,0.035064697,0.0519104,0.0073127747,0.0022640228,-0.04611206,0.02809143,0.0317688,0.04940796,-0.029251099,0.021987915,0.017532349,0.060455322,0.0104599,0.03970337,-0.017730713,-0.026931763,0.031585693,-0.0158844,-0.027709961,-0.032165527,0.016174316,-0.010848999,-0.022369385,0.019958496,-0.026535034,-0.060058594,0.029052734,0.06549072,-0.0385437,-0.029251099,0.027114868,-0.035461426,0.022857666,-0.029052734,-0.045318604,0.058502197,-0.0317688,-0.031188965,0.0259552,0.0066337585,-0.029647827,-0.012207031,-0.048614502,0.026351929,0.0017681122,0.0022525787,0.024124146,0.005519867,-0.029830933,0.017440796,0.03488159,-0.0030384064,-0.0317688,0.039123535,-0.036621094,-0.041259766,0.026535034,0.035827637,0.008476257,-0.026931763,-0.013557434,0.03060913,-0.0121536255,-0.03466797,0.033325195,-0.04901123,-0.07244873,-0.032928467,0.07672119,0.026733398,0.023345947,0.0072631836,-0.07904053,0.046295166,-0.05734253,-0.0037059784,-0.012786865,0.004261017,-0.015975952,0.043395996,0.004310608,0.0041656494,-0.0013866425,0.042053223,-0.0051841736,0.0064888,-0.0118637085,0.03970337,0.035827637,-0.042053223,-0.027114868,0.024993896,0.012496948,0.0619812,-0.032928467,-0.015686035,-0.010124207,-0.06628418,-0.05618286,0.017044067,-0.02053833,-0.036621094,0.010169983,0.01637268,-0.0023975372,0.0013198853,0.0031490326,0.016860962,0.040496826,0.0051574707,-0.04473877,0.029251099,-0.05230713,-0.05230713,-0.0060539246,0.0368042,-0.031585693,0.0016946793,-0.022659302,-0.07672119,-0.021606445,-0.028869629,-0.021026611,0.010269165,-0.010124207,-0.03564453,-0.04473877,0.01675415,0.022476196,0.037017822,0.020431519,0.012298584,-0.039520264,0.0063438416,0.035064697,-0.015686035,-0.019561768,0.009056091,0.035247803,-0.0041160583,0.020431519,-0.020431519,-0.035827637,-0.06161499,0.028869629,-0.015113831,-0.0435791,-0.015113831,-0.01889038,-0.04727173,-0.008522034,0.020736694,0.0001052022,0.017242432,-0.04321289,0.021026611,-0.0051841736,-0.028289795,0.039123535,0.0023002625,0.03466797,0.011329651,0.009101868,0.017730713,-0.006149292,0.022567749,0.01423645,0.02519226,-0.027893066,0.018203735,-0.020339966,-0.054229736,-0.03488159,-0.03466797,0.012252808,-0.014724731,-0.03970337,0.012252808,0.0060043335,0.021896362,0.020248413,-0.053863525,-0.007411957,-0.008087158,0.055419922,-0.017044067,-0.037963867,0.03274536,-0.0034637451,0.037017822,0.015686035,-0.016662598,-0.022964478,0.03274536,0.027114868,-0.029647827,-0.04321289,-0.039520264,0.039123535,0.00068998337,0.05657959,0.011672974,0.042999268,0.0284729,0.00088977814,0.012298584,0.05657959,0.024414062,-0.024307251,-0.02809143,-0.017333984,-0.016860962,-0.05230713,-0.037963867,0.0309906,0.027114868,-0.06896973,-0.02809143,-0.010025024,-0.009254456,0.009689331,0.028869629,0.015014648,0.07092285,0.0158844,-0.025375366,-0.0028705597,0.035064697,-0.0118637085,0.04940796,0.039916992,0.005592346,0.012252808,0.009010315,0.009155273,-0.042419434,0.006175995,0.0079422,0.03604126,0.036224365,0.049194336,-0.011383057,-0.039520264,0.0184021,0.019760132,0.03894043,0.03137207,0.009544373,0.017044067,0.0044555664,0.009399414,0.007411957,0.0012769699,-0.017822266,-0.005256653,-0.059295654,0.019088745,-0.014724731,0.01259613,0.037017822,-0.01966858,-0.0034389496,0.02809143,-0.0035362244,-0.06317139,0.031951904,-0.024017334,-0.023147583,0.00018393993,-0.05230713,0.03274536,-0.008377075,-0.06817627,0.01626587,0.046691895,0.005012512,-0.02218628,-0.004383087,0.0069274902,0.016860962,-0.004989624,-0.042236328,-0.010025024,0.0007991791,-0.060821533,-0.006298065,-0.017333984,-0.0112838745,0.006587982,-0.026153564,-0.0335083,0.034484863,-0.030227661,0.037017822,0.019378662,-0.009300232,0.02470398,0.06506348,-0.014526367,0.0115737915,-0.019851685,-0.064697266,0.0002361536,0.017333984,0.014045715,-0.06317139,-0.008720398,0.06274414,-0.045135498,0.021896362,0.026931763,-0.011528015,0.005279541,0.037017822,-0.012687683,0.004722595,0.048034668,-0.015113831,-0.029449463,0.010314941,-0.06237793,0.054626465,0.015014648,0.05154419,-0.039123535,0.011672974,-0.022964478,-0.019561768,0.004699707,-0.09185791,-0.024017334,0.036621094,-0.008132935,-0.012397766,0.017913818,-0.003583908,-0.027709961,-0.048828125,0.022659302,0.007457733,0.00554657,-0.027893066,-0.021408081,-0.01423645,-0.048431396,-0.02809143,-0.06506348,0.032348633,0.02053833,-0.008087158,0.02305603,-0.0034866333,0.015975952,-0.0014047623,0.022369385,-0.015205383,0.024993896,-0.0067329407,0.020736694,0.010070801,0.012886047,-0.04611206,0.01235199,0.03564453,0.014625549,0.02557373,0.025772095,-0.003293991,-0.037017822,0.015792847,-0.059295654,0.00818634,-0.0022029877,0.031951904,0.0027236938,0.03604126,-0.079833984,0.057739258,-0.034301758,0.004699707,-0.008132935,-0.0021076202,-0.059295654,-0.06817627,-0.0435791,0.0069732666,-0.024993896,0.024993896,0.023147583,-0.004650116,-0.02519226,0.011672974,-0.0028095245,0.016082764,0.006298065,-0.008041382,0.068603516,-0.015113831,0.007797241,-0.037200928,0.049987793,0.012107849,-0.017440796,-6.055832e-05,-0.018508911,0.036224365,0.028289795,-0.020431519,0.03604126,0.016464233,-0.036621094,-0.0041885376,-0.0051841736,0.008285522,-0.0066337585,-0.016174316,-0.013755798,-0.030227661,-0.06896973,-0.0154953,-0.036621094,-0.035827637,-0.0259552,0.013656616,0.00038290024,-0.036621094,0.03314209,0.020141602,-0.011627197,-0.020248413,0.01675415,-0.043395996,0.0036087036,0.021408081,-0.044555664,0.040863037,0.0019493103,-0.0013799667,-0.010749817,0.014526367,-0.0075569153,0.028671265,-0.0335083,-0.016860962,0.0009384155,-0.020431519,0.018112183,0.023635864,0.03970337,-0.0129776,-0.04940796,0.010124207,0.009780884,-0.054626465,0.0073623657,0.004261017,-0.01637268,0.0054244995,0.00019526482,0.013946533,0.054626465,0.0021305084,-0.013175964,0.010025024,-0.007411957,0.015594482,-0.0028820038,0.06585693,0.0066833496,0.018203735,-0.015014648,-0.08911133,0.013557434,0.0035591125,-0.06390381,0.026153564,0.05734253,0.027511597,0.027893066,0.02178955,0.033325195,0.07244873,0.009590149,-0.06628418,-0.047851562,-0.012496948,0.0385437,-0.0067329407,-0.08947754,0.052703857,0.0519104,-0.020339966,-0.022766113,0.0019006729,-0.009300232,0.0569458,0.0368042,0.0021915436,-0.03933716,0.024795532,0.029830933,-0.0017318726,-0.012687683,-0.007987976,0.003921509,0.044158936,-0.048828125,0.020050049,-0.0104599,0.09918213,-0.009155273,-0.0635376,0.01675415,0.04397583,0.0066833496,-0.013267517,0.0017194748,-0.008766174,-0.010025024,-0.007896423,0.020629883,0.00920105,0.07904053,0.012298584,-0.019760132,-0.0015745163,-0.010604858,0.018981934,-0.0047454834,-0.014823914,0.00093221664,-0.06161499,0.0519104,0.0035114288,0.036224365,0.002046585,-0.055023193,-0.0016584396,0.014915466,0.035064697,0.0054969788,0.0317688,-0.0005083084,0.008522034,0.04147339,0.038757324,0.0045776367,0.016952515,-0.05114746,-0.010124207,-0.043792725,0.006877899,-0.0054969788,0.0010538101,0.0015134811,0.00035572052,0.03314209,-0.030227661,-0.02557373,-0.0259552,0.031951904,-0.05810547,0.032165527,0.0069274902,-0.004962921,0.008522034,-0.05734253,0.05734253,0.012786865,0.03640747,-0.0104599,0.019561768,0.037963867,-0.015304565,0.009925842,0.0619812,-0.0038986206,0.05114746,0.048614502,0.047851562,-0.01966858,0.023544312,0.0057144165,-0.05038452,0.023254395,0.010025024,0.037200928,-0.0104599,-0.019561768,0.0026760101,-0.016174316,0.06274414,0.037963867,-0.015205383,0.03640747,-0.05114746,0.05114746,-0.009056091,0.07208252,-0.019760132,0.027893066,0.008232117,-0.012062073,-0.043395996,-0.009544373,-0.0069732666,0.021026611,-0.017623901,0.050750732,0.037017822,0.024414062,-0.06390381,-0.010414124,0.0032444,-0.022277832,0.020050049,0.007507324,-0.054229736,0.035461426,0.022079468,0.028869629,0.0070724487,0.03604126,0.028869629,-0.024414062,-0.0066833496,-0.006877899]", "client_status_at": "2025-10-02T09:53:30.701011Z", "server_status_at": "2025-10-02T09:53:30.700996Z", "created_at": "2025-10-02T08:53:26.876505Z"}, {"id": "cb3a0131-2904-4b4e-abaf-459b86cb70c7", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://www.cyclingnews.com/races/uci-road-world-championships-2025/elite-men-road-race/results/", "canonical_url": "https://www.cyclingnews.com/races/uci-road-world-championships-2025/elite-men-road-race/results/", "title": "Road World Championships: Unstoppable Tadej Pogačar seals consecutive men’s road race victories after 66km solo break", "source_site": "Cyclingnews", "publication_date": "2025-09-28T00:00:00Z", "favicon_url": "https://www.cyclingnews.com/favicon.ico", "content_markdown": "[Tadej Pogačar](https://www.cyclingnews.com/riders/tadej-pogacar/) turned in yet another performance for the ages at the elite men's road race at the [UCI Road World Championships](https://www.cyclingnews.com/uci-road-world-championships/) on Sunday, securing his second gold in the category in as many years with a trademark solo long-distance breakaway in Kigali, Rwanda.\n\nHaving first opened up the throttle on the dauntingly difficult Mont Kigali and caused the field to shatter behind him, the Slovenian then dropped the last rider to stay on his wheel, Isaac del Toro (Mexico), with 66 kilometres left and headed away for another epic lone win.\n\nSlovenia's Pogačar finally crossed the finish line with over a minute's advantage over Remco Evenepoel (Belgium), who staged a remarkable comeback following numerous mechanical difficulties to claim silver. Ben Healy (Ireland) dropped Mattias Skjelmose (Denmark) on the final climb to take the bronze medal.\n\nBut if Evenepoel's ability to turn misfortune into an honourable podium position could not be ignored, neither could Pogačar's latest stunning success, following up a fourth Tour de France title this summer with a second World Championships victory.\n\nHis latest triumph, too, further confirmed his hegemony as the world's number one, and - coming on a course widely rated as the most difficult World Championships course ever - propelled Pogačar even closer to the unofficial title of cycling's all-time greatest in history.\n\nAsked about why he had launched his first attack so early, with 105 kilometres to go, Pogačar answered, \"I think the course was designed for this, but I was hoping that a small group would form, like we did with Juan [Ayuso, Spain] and [Isaac] Del Toro [Mexico]. It was a perfect combo'.\"\n\nAfter breaking away with two trade teammates from UAE Team Emirates, Pogačar said, \"I was like - this is a dream, no, to ride together as far as possible, as a trio, but Juan had a problem like soon on the Mur and Del Toro had some stomach problems in the race.\"\n\nThe latest race content, interviews, features, reviews and expert buying guides, direct to your inbox!\n\n\"So I was left alone quite early, and I was solo, fighting by myself, but I'm so happy I made it.\"\n\nAs for whether he had moments of doubt in what looked, from the outside, to be such a convincing performance, Pogačar said, \"For sure. Because the climbs were getting harder and harder on every lap, but on the downhills, I had to pedal a lot too. Towards the end, the energy resources were going.\"\n\n\"It was so hard, the last final laps. Of course, you doubt, but you have to push through and hope for the best.\"\n\nVictorious for the second year running in Africa's first-ever Road World Championships, Pogačar rounded off by saying - with some considerable understatement - \"It was an incredible experience altogether. Let's say it was a successful week.\"\n\n### How it unfolded\n\nThe opening move of the day came after just 500 metres courtesy of Red Walters, Grenada's sole representative in the 92nd edition of the World Championships Road Race. But in a high-octane start, Walters was quickly succeeded by a much more numerous move containing Andrea Foldager (Denmark), Menno Huising (Netherlands), Ivo Oliveira (Portugal), Fabio Christen (Switzerland), and Marius Mayrhofer (Germany). The six were then joined by Julien Bernard (France) as they went through the finish for the first of 16 laps on the toughest Worlds circuit - 5475 metres of elevation gain - since the Nurburgring's 5,844 metres of climbing in Germany, way back in 1966.\n\nWhile the very fast start to the elite men's race proved too much for former double World Champion Julien Alaphilippe (France), who abandoned, ill, a move by Raúl García Pierna (Spain) to bridge across to the six leaders on the opening 10 laps of the Kigali city centre circuit sparked a strong response by numerous nations. Slovenia and Belgium, both fielding major favourites, kept things firmly under control, the latter despite the early loss of outside contender Ilan van Wilder to a crash, whilst the USA were also sticking close to the front.\n\nAfter 100 kilometres of steadily hard riding, the seven had an advantage of just 2:33, a very small gap for an early break in the Worlds. That scant margin for hope drained away quickly, too, as the peloton headed out of the city at a relentlessly high pace for the race's single ascents of the deceptively difficult Mont Kigali and the ultra-steep Mur de Kigali, immediately afterwards.\n\nGreeted by enormous crowds, seemingly even bigger than on the city centre finishing circuit, the three strongest riders from the break - Foldager, Oliveira and Bernard - hit the Mont Kigali with barely two minutes' advantage. To judge by the ferocious fight in a pack already reduced to just 80 riders to reach the foot of the Kigali as well-placed as possible, it was clear that nobody wanted to lose out on the hardest climb of the entire race. But that said, given the way Pogačar promptly proceeded to leave the field for dust at its summit, perhaps they'd have been wiser to save their energy.\n\nAs Bernard pressed home a fast-dimishing advantage on the upper slopes of the Mont Kigali, both Oliveira and Foldager were dropped, while Slovenia still kept a firm control on the pack, with Pogačar preceding a seemingly ever-vigilant Evenepoel and Juan Ayuso (Spain). But then, very close to the summit, the first key moment of the race suddenly materialised as Pogačar notably raised the pace, whilst Evenepoel, despite being so close behind, abruptly became one of the biggest victims of his searing acceleration. Then when the Slovenian closed in on the break over the top of the hill and powered past Bernard, only Ayuso could follow.\n\nWith 104 kilometres to go, such a long-distance show of strength looked like a mirror move to Zurich's Road World Championships 12 months earlier, albeit with the difference that this time the Slovenian was not (yet) alone. Furthermore, after blasting down the far side of the Mont Kigali at speed, Isaac Del Toro (Mexico) bridged across, briefly making it a trio of UAE riders ahead.\n\nThe next big development in what proved to be the winning move of the race came when Del Toro launched an emphatic challenge on the lungburstingly steep Mur de Kigali, testing Pogačar and pushing Ayuso out of the running. Pogačar was only a couple of bike lengths back, though, at the top, and as he rejoined Ayuso and the two headed back to the final six laps in Kigali city centre together, the joint commitment of the Mexican and the defending World Champion to their long-distance bid for glory was clear.\n\nStill, with a first chase group at 40 seconds containing rivals as important as Ben Healy (Ireland), Carapaz, Ayuso and Jai Hindley (Australia), not to mention Evenepoel pushing to close the gap from a little further back, all was not lost for the opposition. But by the time Pogačar and Del Toro went under the finish line for the final six laps to go, their advantage over Sivakov and Healy had risen to 50 seconds, and the organisation in a chase group behind of just 30 riders was not yet good enough to try and reverse the gap. Last but not least, the presence of Pogačar's teammate Primož Roglič, cheerfully chasing down counter-attacks to ensure Pogačar gained as much of a margin as possible, can hardly have been encouraging, either.\n\nAnother of multiple setbacks for Evenepoel followed, too, when after hitting a pothole on the Mont Kigali that put his bike out of synch, he had no choice but to stop for a bike change.\n\nBelgium initially battled on, though, with Evenepoel gamely following teammate Quinten Hermans up the Côte de Kigali, even as the number of significant abandons - Michael Matthews (Australia), Egan Bernal (Colombia) and Fred Wright (Great Britain) amongst them - began to multiply. However, Evenepoel's predicament worsened sharply again as he was delayed in a much more significant fashion when needing a second bike change, forcing him to spend the rest of the race battling to regain any chance of contention.\n\nThe sense that events were running out of control for Evenepoel increased even further when the first cohesive counter-attack finally formed behind Pogačar, courtesy of Sivakov, Healy and Mikkel Honoré (Denmark), but lacking the Belgian, still weaving through the team cars and jumping from group to group in a furious game of catch-up. Then just as the Belgian was on the point of joining the Healy group, yet more bad news filtered through for his chances: far from losing steam as Evenepoel surely would have wanted, ahead Pogačar had dropped Del Toro and gone for it alone.\n\nThe definitive turning point of the battle for gold took place at 66.6 kilometres to go, on the Côte de Kigali, not rated as being as hard as the cobbled Côte de Kimihurara, but for a rider with Pogačar's condition and strength, you'd suspect that was largely irrelevant. Indeed, there was seemingly no exceptional acceleration by Pogačar as he moved into solo-break mode, just a powerful drive that Del Toro, shadowing him as best he could, proved unable to follow. Once again, we were back in the same scenario as Zurich, and if Pogačar later said this latest long-distance move was unplanned, the end result was destined to be exactly the same.\n\nIt was true that as Pogačar was fulfilling the script predicted by so many to perfection, grinding steadily up the daunting Kigali slopes without any seeming effort, behind Evenepoel was able to stage a remarkable comeback. Having pushed his way through numerous chasing groups, the Belgian had eventually ended up with the closest pursuers behind the Slovenian, alongside Mattias Skjelmose (Denmark) and Healy. But even if Evenepoel's determination, making a massive contribution to the break and constantly cajoling the other two to push harder, was admirable and ended up with the trio distancing lone chaser Tom Pidcock (Great Britain) for good, their chances of catching a Pogačar in blazingly good form never looked more than minimal, either.\n\nOf course, the key question remained if Pogačar's latest long-distance charge was too ambitious, even for him. As his margin remained stubbornly at around a minute for nearly 20 kilometres, the memory of how he struggled slightly in the finale of Zurich last September was inescapable, not to mention his sudden dip in strength at Amstel Gold this spring - with Skjelmose, present alongside Healy and Evenepoel, the man who defeated him in April too. Yet as the Slovenian team car was finally let through the convoy and up to Pogačar with around 40 kilometres to go, it was lost on nobody watching that his gap was not coming down, either.\n\nAs the trio of chasers headed towards the bell lap, basically set on going for a podium placing given Pogačar's evident superiority, Evenepoel darted clear, now moving into his favoured time trial mode as he went for it alone. It had been a nightmare scenario for the Belgian in the first two-thirds of the race, yet his ability to return to the fray and push as hard as he could in the finale could only be admired.\n\nThe biggest plaudits, though, had to be reserved for Pogačar, grimacing slightly with the effort as he blasted through the last lap, but with the prospects of a gold medal surely easing the pain a little. Visibly putting everything into it, the defending World Champion might well have had the memory of being overtaken by Evenepoel in the elite men's time trial the last week in mind as he ramped up the pace towards a certain victory for no apparent reason. But whatever the motivation for that last push to gain more time, his gap was once again increasing with every turn of the pedals.\n\nBehind, Healy managed to follow Evenepoel up the road and drop Skjelmose on the final ascent of the Côte de Kigali. But by this point, though, Pogačar and his team car were already all but celebrating the win.\n\nThe final ascent of the Côte de Kimihurara, so decisive in the other Worlds' races this week, was tackled by Pogačar with no indication of last-minute weakening. If the last six laps were effectively laps of honour for Pogačar after such a dominant ride, the last time up, he could continue to relax and even smile a little.\n\nStanding out of the saddle on the last little rise to the finish, Pogačar celebrated his second straight rainbow title with his arms spread wide and a subsequent exchange of high-fives with his support staff. The contrast with silver medallist Evenepoel, after he reached the finish 1:28 later, slumped against the barriers for long minutes afterwards and left wondering what might have been without those bike changes and his bad luck, could not have been clearer.\n\nWhile Healy was delighted with Ireland's first road race medal since Sean Kelly's bronze back in 1989, the truth was that merely reaching the line on such a tough course with just 30 finishers was a major achievement in itself. As for Pogačar, the Slovenian could celebrate becoming the eighth rider in history to take back-to-back Worlds wins - and yet another massive milestone in his career.\n\n### Results\n\n**Results powered by**** FirstCycling**\n\n\nAlasdair Fotheringham has been reporting on cycling since 1991. He has covered every Tour de France since 1992 bar one, as well as numerous other bike races of all shapes and sizes, ranging from the Olympic Games in 2008 to the now sadly defunct Subida a Urkiola hill climb in Spain. As well as working for *Cyclingnews*, he has also written for *The Independent*, *The Guardian*, *ProCycling*, *The Express* and *Reuters*.\n\nYou must confirm your public display name before commenting\n\nPlease logout and then login again, you will then be prompted to enter your display name.\n\n## Latest on Cyclingnews\n\n-\n### 'The market is both flooded and in limbo' – Uncertainty around Intermarché-Lotto merger, Arkéa-B&B Hotels' future mucking up transfer works\n\nOver 400 riders and staff still waiting for answers as end of season nears -\n### 'Still fighting for the win' – Big goals, a near miss, shifting goal posts and hopes for a victory celebration as Alexander Kristoff bids farewell at Le Tour de Langkawi\n\nNorwegian to end his career after the Malaysian race -\n### 'In the reverse of Christina Aguilera, our hearts were saying yes, but our bodies saying no' - Larry Warbasse reflects on savage racing and scenic safaris in Kigali\n\nThe US-team rider writes about his unique experience at the World Championships for Cyclingnews", "content_text": "Tadej Pogačar turned in yet another performance for the ages at the elite men's road race at the UCI Road World Championships on Sunday, securing his second gold in the category in as many years with a trademark solo long-distance breakaway in Kigali, Rwanda.\nHaving first opened up the throttle on the dauntingly difficult Mont Kigali and caused the field to shatter behind him, the Slovenian then dropped the last rider to stay on his wheel, Isaac del Toro (Mexico), with 66 kilometres left and headed away for another epic lone win.\nSlovenia's Pogačar finally crossed the finish line with over a minute's advantage over Remco Evenepoel (Belgium), who staged a remarkable comeback following numerous mechanical difficulties to claim silver. Ben Healy (Ireland) dropped Mattias Skjelmose (Denmark) on the final climb to take the bronze medal.\nBut if Evenepoel's ability to turn misfortune into an honourable podium position could not be ignored, neither could Pogačar's latest stunning success, following up a fourth Tour de France title this summer with a second World Championships victory.\nHis latest triumph, too, further confirmed his hegemony as the world's number one, and - coming on a course widely rated as the most difficult World Championships course ever - propelled Pogačar even closer to the unofficial title of cycling's all-time greatest in history.\nAsked about why he had launched his first attack so early, with 105 kilometres to go, Pogačar answered, \"I think the course was designed for this, but I was hoping that a small group would form, like we did with Juan [Ayuso, Spain] and [Isaac] Del Toro [Mexico]. It was a perfect combo'.\"\nAfter breaking away with two trade teammates from UAE Team Emirates, Pogačar said, \"I was like - this is a dream, no, to ride together as far as possible, as a trio, but Juan had a problem like soon on the Mur and Del Toro had some stomach problems in the race.\"\nThe latest race content, interviews, features, reviews and expert buying guides, direct to your inbox!\n\"So I was left alone quite early, and I was solo, fighting by myself, but I'm so happy I made it.\"\nAs for whether he had moments of doubt in what looked, from the outside, to be such a convincing performance, Pogačar said, \"For sure. Because the climbs were getting harder and harder on every lap, but on the downhills, I had to pedal a lot too. Towards the end, the energy resources were going.\"\n\"It was so hard, the last final laps. Of course, you doubt, but you have to push through and hope for the best.\"\nVictorious for the second year running in Africa's first-ever Road World Championships, Pogačar rounded off by saying - with some considerable understatement - \"It was an incredible experience altogether. Let's say it was a successful week.\"\nHow it unfolded\nThe opening move of the day came after just 500 metres courtesy of Red Walters, Grenada's sole representative in the 92nd edition of the World Championships Road Race. But in a high-octane start, Walters was quickly succeeded by a much more numerous move containing Andrea Foldager (Denmark), Menno Huising (Netherlands), Ivo Oliveira (Portugal), Fabio Christen (Switzerland), and Marius Mayrhofer (Germany). The six were then joined by Julien Bernard (France) as they went through the finish for the first of 16 laps on the toughest Worlds circuit - 5475 metres of elevation gain - since the Nurburgring's 5,844 metres of climbing in Germany, way back in 1966.\nWhile the very fast start to the elite men's race proved too much for former double World Champion Julien Alaphilippe (France), who abandoned, ill, a move by Raúl García Pierna (Spain) to bridge across to the six leaders on the opening 10 laps of the Kigali city centre circuit sparked a strong response by numerous nations. Slovenia and Belgium, both fielding major favourites, kept things firmly under control, the latter despite the early loss of outside contender Ilan van Wilder to a crash, whilst the USA were also sticking close to the front.\nAfter 100 kilometres of steadily hard riding, the seven had an advantage of just 2:33, a very small gap for an early break in the Worlds. That scant margin for hope drained away quickly, too, as the peloton headed out of the city at a relentlessly high pace for the race's single ascents of the deceptively difficult Mont Kigali and the ultra-steep Mur de Kigali, immediately afterwards.\nGreeted by enormous crowds, seemingly even bigger than on the city centre finishing circuit, the three strongest riders from the break - Foldager, Oliveira and Bernard - hit the Mont Kigali with barely two minutes' advantage. To judge by the ferocious fight in a pack already reduced to just 80 riders to reach the foot of the Kigali as well-placed as possible, it was clear that nobody wanted to lose out on the hardest climb of the entire race. But that said, given the way Pogačar promptly proceeded to leave the field for dust at its summit, perhaps they'd have been wiser to save their energy.\nAs Bernard pressed home a fast-dimishing advantage on the upper slopes of the Mont Kigali, both Oliveira and Foldager were dropped, while Slovenia still kept a firm control on the pack, with Pogačar preceding a seemingly ever-vigilant Evenepoel and Juan Ayuso (Spain). But then, very close to the summit, the first key moment of the race suddenly materialised as Pogačar notably raised the pace, whilst Evenepoel, despite being so close behind, abruptly became one of the biggest victims of his searing acceleration. Then when the Slovenian closed in on the break over the top of the hill and powered past Bernard, only Ayuso could follow.\nWith 104 kilometres to go, such a long-distance show of strength looked like a mirror move to Zurich's Road World Championships 12 months earlier, albeit with the difference that this time the Slovenian was not (yet) alone. Furthermore, after blasting down the far side of the Mont Kigali at speed, Isaac Del Toro (Mexico) bridged across, briefly making it a trio of UAE riders ahead.\nThe next big development in what proved to be the winning move of the race came when Del Toro launched an emphatic challenge on the lungburstingly steep Mur de Kigali, testing Pogačar and pushing Ayuso out of the running. Pogačar was only a couple of bike lengths back, though, at the top, and as he rejoined Ayuso and the two headed back to the final six laps in Kigali city centre together, the joint commitment of the Mexican and the defending World Champion to their long-distance bid for glory was clear.\nStill, with a first chase group at 40 seconds containing rivals as important as Ben Healy (Ireland), Carapaz, Ayuso and Jai Hindley (Australia), not to mention Evenepoel pushing to close the gap from a little further back, all was not lost for the opposition. But by the time Pogačar and Del Toro went under the finish line for the final six laps to go, their advantage over Sivakov and Healy had risen to 50 seconds, and the organisation in a chase group behind of just 30 riders was not yet good enough to try and reverse the gap. Last but not least, the presence of Pogačar's teammate Primož Roglič, cheerfully chasing down counter-attacks to ensure Pogačar gained as much of a margin as possible, can hardly have been encouraging, either.\nAnother of multiple setbacks for Evenepoel followed, too, when after hitting a pothole on the Mont Kigali that put his bike out of synch, he had no choice but to stop for a bike change.\nBelgium initially battled on, though, with Evenepoel gamely following teammate Quinten Hermans up the Côte de Kigali, even as the number of significant abandons - Michael Matthews (Australia), Egan Bernal (Colombia) and Fred Wright (Great Britain) amongst them - began to multiply. However, Evenepoel's predicament worsened sharply again as he was delayed in a much more significant fashion when needing a second bike change, forcing him to spend the rest of the race battling to regain any chance of contention.\nThe sense that events were running out of control for Evenepoel increased even further when the first cohesive counter-attack finally formed behind Pogačar, courtesy of Sivakov, Healy and Mikkel Honoré (Denmark), but lacking the Belgian, still weaving through the team cars and jumping from group to group in a furious game of catch-up. Then just as the Belgian was on the point of joining the Healy group, yet more bad news filtered through for his chances: far from losing steam as Evenepoel surely would have wanted, ahead Pogačar had dropped Del Toro and gone for it alone.\nThe definitive turning point of the battle for gold took place at 66.6 kilometres to go, on the Côte de Kigali, not rated as being as hard as the cobbled Côte de Kimihurara, but for a rider with Pogačar's condition and strength, you'd suspect that was largely irrelevant. Indeed, there was seemingly no exceptional acceleration by Pogačar as he moved into solo-break mode, just a powerful drive that Del Toro, shadowing him as best he could, proved unable to follow. Once again, we were back in the same scenario as Zurich, and if Pogačar later said this latest long-distance move was unplanned, the end result was destined to be exactly the same.\nIt was true that as Pogačar was fulfilling the script predicted by so many to perfection, grinding steadily up the daunting Kigali slopes without any seeming effort, behind Evenepoel was able to stage a remarkable comeback. Having pushed his way through numerous chasing groups, the Belgian had eventually ended up with the closest pursuers behind the Slovenian, alongside Mattias Skjelmose (Denmark) and Healy. But even if Evenepoel's determination, making a massive contribution to the break and constantly cajoling the other two to push harder, was admirable and ended up with the trio distancing lone chaser Tom Pidcock (Great Britain) for good, their chances of catching a Pogačar in blazingly good form never looked more than minimal, either.\nOf course, the key question remained if Pogačar's latest long-distance charge was too ambitious, even for him. As his margin remained stubbornly at around a minute for nearly 20 kilometres, the memory of how he struggled slightly in the finale of Zurich last September was inescapable, not to mention his sudden dip in strength at Amstel Gold this spring - with Skjelmose, present alongside Healy and Evenepoel, the man who defeated him in April too. Yet as the Slovenian team car was finally let through the convoy and up to Pogačar with around 40 kilometres to go, it was lost on nobody watching that his gap was not coming down, either.\nAs the trio of chasers headed towards the bell lap, basically set on going for a podium placing given Pogačar's evident superiority, Evenepoel darted clear, now moving into his favoured time trial mode as he went for it alone. It had been a nightmare scenario for the Belgian in the first two-thirds of the race, yet his ability to return to the fray and push as hard as he could in the finale could only be admired.\nThe biggest plaudits, though, had to be reserved for Pogačar, grimacing slightly with the effort as he blasted through the last lap, but with the prospects of a gold medal surely easing the pain a little. Visibly putting everything into it, the defending World Champion might well have had the memory of being overtaken by Evenepoel in the elite men's time trial the last week in mind as he ramped up the pace towards a certain victory for no apparent reason. But whatever the motivation for that last push to gain more time, his gap was once again increasing with every turn of the pedals.\nBehind, Healy managed to follow Evenepoel up the road and drop Skjelmose on the final ascent of the Côte de Kigali. But by this point, though, Pogačar and his team car were already all but celebrating the win.\nThe final ascent of the Côte de Kimihurara, so decisive in the other Worlds' races this week, was tackled by Pogačar with no indication of last-minute weakening. If the last six laps were effectively laps of honour for Pogačar after such a dominant ride, the last time up, he could continue to relax and even smile a little.\nStanding out of the saddle on the last little rise to the finish, Pogačar celebrated his second straight rainbow title with his arms spread wide and a subsequent exchange of high-fives with his support staff. The contrast with silver medallist Evenepoel, after he reached the finish 1:28 later, slumped against the barriers for long minutes afterwards and left wondering what might have been without those bike changes and his bad luck, could not have been clearer.\nWhile Healy was delighted with Ireland's first road race medal since Sean Kelly's bronze back in 1989, the truth was that merely reaching the line on such a tough course with just 30 finishers was a major achievement in itself. As for Pogačar, the Slovenian could celebrate becoming the eighth rider in history to take back-to-back Worlds wins - and yet another massive milestone in his career.\nResults\nResults powered by FirstCycling\nAlasdair Fotheringham has been reporting on cycling since 1991. He has covered every Tour de France since 1992 bar one, as well as numerous other bike races of all shapes and sizes, ranging from the Olympic Games in 2008 to the now sadly defunct Subida a Urkiola hill climb in Spain. As well as working for Cyclingnews, he has also written for The Independent, The Guardian, ProCycling, The Express and Reuters.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name.\nLatest on Cyclingnews\n-\n'The market is both flooded and in limbo' – Uncertainty around Intermarché-Lotto merger, Arkéa-B&B Hotels' future mucking up transfer works\nOver 400 riders and staff still waiting for answers as end of season nears -\n'Still fighting for the win' – Big goals, a near miss, shifting goal posts and hopes for a victory celebration as Alexander Kristoff bids farewell at Le Tour de Langkawi\nNorwegian to end his career after the Malaysian race -\n'In the reverse of Christina Aguilera, our hearts were saying yes, but our bodies saying no' - Larry Warbasse reflects on savage racing and scenic safaris in Kigali\nThe US-team rider writes about his unique experience at the World Championships for Cyclingnews", "content_token_count": 3230, "client_status": "queued", "server_status": "embedded", "summary": "Tadej Pogačar secured his second consecutive gold medal in the elite men's road race at the UCI Road World Championships with a solo long-distance breakaway in Kigali, Rwanda. Despite challenges from competitors like Remco Evenepoel and Ben Healy, Pogačar's dominant performance confirmed his status as the world's top cyclist.", "expiry_score": 0.3, "mistral_embedding": "[-0.047210693,0.04257202,0.054870605,0.0060691833,0.016799927,0.022949219,0.027267456,-0.025604248,0.015129089,-0.03326416,-0.015960693,0.08581543,-0.016952515,-0.024612427,-0.059509277,0.02053833,0.011558533,0.038085938,0.01737976,0.026275635,-0.032409668,-0.011222839,-0.036895752,0.019363403,-0.010803223,-0.0035743713,-0.030090332,-0.05984497,-0.054870605,-0.00028324127,0.022949219,0.00036358833,0.02909851,0.0010910034,-0.002254486,0.013381958,0.0056533813,-0.024932861,0.02659607,0.017791748,-0.015129089,-0.022109985,0.00299263,-0.031433105,-0.017959595,-0.018127441,0.035583496,-0.020446777,0.02859497,-0.008148193,0.020446777,0.04257202,0.027435303,0.02709961,-0.02659607,0.015464783,0.034088135,-0.020858765,-0.010185242,0.05154419,-0.019042969,0.002576828,-0.013053894,-0.023773193,0.055877686,0.016540527,-0.009185791,-0.0021095276,-0.009681702,0.012138367,0.0019435883,0.043548584,-0.0030555725,0.04888916,-0.02760315,-0.057861328,0.025268555,0.010726929,0.08911133,0.013137817,-0.019210815,-0.010391235,0.051208496,-0.014633179,-0.005569458,0.02078247,0.020706177,-0.009147644,-0.008354187,0.012466431,0.043884277,-0.040740967,0.035095215,-0.019363403,0.025772095,-0.0007171631,-0.0076904297,0.009979248,-0.018295288,0.03125,0.03475952,-0.029754639,0.07550049,-0.04107666,0.030426025,-0.02960205,-0.0052986145,0.0070648193,-0.026763916,-0.06286621,-0.035583496,0.0028057098,0.00415802,-0.06451416,-0.04321289,-0.014381409,-0.022109985,0.02027893,-0.043548584,-0.033081055,0.0069007874,0.02809143,0.0012311935,0.038726807,-0.036895752,-0.052215576,0.040893555,0.014465332,0.018203735,0.0063591003,0.010269165,0.00068569183,-0.019454956,0.013298035,0.015380859,-0.01663208,0.024276733,-0.041229248,-0.060516357,0.0039482117,-0.0032634735,-0.019210815,-0.035736084,-0.014633179,-0.053527832,0.0045318604,-0.014129639,-0.0027008057,0.012718201,-0.008148193,-0.010269165,0.03540039,0.059509277,0.034088135,0.0028476715,0.00434494,-0.01171875,-0.013214111,0.046539307,-0.008148193,-0.013717651,-0.027770996,-0.023605347,0.009269714,-0.020111084,-0.043548584,-0.023269653,-0.020111084,0.012054443,0.030090332,0.044891357,-0.030761719,0.044555664,0.04171753,-0.012466431,0.053527832,0.00198555,-0.041412354,0.029754639,0.03756714,-0.03741455,-0.015464783,-0.004219055,-0.001288414,0.005569458,-0.030761719,-0.042907715,-0.027435303,-0.023117065,0.012550354,-0.028762817,0.0006132126,-0.033416748,0.015213013,0.0027427673,0.029434204,0.02027893,0.013969421,-0.013137817,-0.02027893,0.0054016113,-0.032409668,0.0007534027,0.02709961,0.0014343262,0.034576416,-0.026931763,-0.015625,0.010971069,-0.00033521652,0.011474609,-0.0008883476,-0.012718201,-0.006816864,-0.022109985,-0.01928711,-0.016464233,0.021194458,-0.021026611,-0.006942749,0.025100708,-0.014801025,0.003929138,0.008773804,-0.012138367,0.06951904,-0.024612427,0.0077323914,-0.016540527,-0.003408432,0.024612427,0.009475708,-0.0395813,-0.0007638931,0.009681702,-0.014129639,0.07080078,-0.0031375885,0.032073975,0.021194458,-0.023605347,0.05319214,-0.02709961,0.045562744,0.0158844,0.0026397705,0.0027008057,-0.014381409,0.0011224747,0.08081055,-0.1184082,-0.022445679,-0.0317688,-0.0068588257,-0.070495605,0.0027637482,-0.011222839,-0.023605347,0.047210693,-0.0060272217,0.018707275,-0.02909851,0.021194458,-0.013137817,0.035736084,0.0054855347,-0.04888916,-0.009307861,0.049560547,-0.0070228577,0.015045166,0.0074386597,0.035247803,-0.010269165,-0.00818634,-0.0073165894,-0.007106781,0.054870605,-0.003616333,0.0010290146,-0.011138916,-0.01953125,0.0234375,0.08709717,-0.024932861,0.0013713837,-0.011054993,-0.018707275,-0.00598526,-0.05319214,-0.007896423,0.029434204,-0.013465881,0.023773193,-0.02027893,-0.04107666,0.018127441,0.019454956,0.026107788,0.037231445,0.022781372,-0.0011796951,-0.07116699,0.0061531067,-0.018447876,-0.026428223,-0.06085205,-0.021118164,-0.013969421,-0.04421997,-0.039245605,0.018875122,0.027938843,0.05718994,0.029754639,-0.02760315,-0.02909851,-0.06451416,0.035247803,-0.04788208,-0.0234375,0.042053223,0.043884277,8.279085e-05,-0.010643005,0.045562744,0.007648468,0.020950317,0.0033874512,0.0030975342,0.018875122,-0.027938843,0.031433105,0.02859497,0.048553467,0.015296936,0.032409668,0.06185913,-0.04522705,-0.0041770935,-0.031097412,-0.039245605,0.057525635,-0.0034713745,-0.004634857,-0.026763916,-0.016204834,0.0026397705,0.030426025,0.024276733,0.04888916,-0.024780273,-0.020858765,0.0076904297,-0.027938843,0.08178711,0.029266357,-0.027938843,-0.0055274963,0.05618286,-0.042236328,0.02027893,0.036254883,0.031097412,-0.019866943,0.036895752,-0.01612854,0.02760315,-0.025772095,-0.022781372,0.013465881,0.0020046234,0.011222839,0.0025463104,-0.00046253204,-0.008354187,-0.020111084,-0.043884277,0.022613525,0.0031585693,-0.06384277,-0.01612854,0.025100708,0.0635376,0.013381958,-0.02003479,-0.058532715,-0.026763916,-0.0056533813,0.099731445,0.060516357,0.0028686523,0.035736084,0.011802673,0.0036582947,0.022781372,0.012634277,0.00868988,0.06317139,0.0035743713,-0.021942139,-0.030426025,0.045898438,-0.013969421,0.0029296875,0.007980347,0.0062332153,-0.00025320053,0.010101318,-0.008811951,0.02128601,0.039245605,-0.047546387,0.042907715,-0.010887146,-0.013137817,-0.06781006,-0.005695343,0.012138367,0.029754639,0.032409668,0.017288208,-0.044555664,0.02809143,0.015296936,-0.077819824,-0.0032634735,-0.012550354,-0.02960205,0.015716553,-0.053863525,-0.027267456,0.048217773,-0.0552063,-0.07116699,0.028930664,0.0234375,-0.05154419,-0.04321289,-0.048217773,0.00843811,-0.02960205,-0.006111145,0.025436401,-0.013465881,0.011886597,0.037231445,0.05618286,0.00044941902,-0.0027217865,0.03591919,-0.005279541,-0.02394104,0.0012674332,0.030761719,0.0037612915,-0.029434204,-0.0035114288,0.042907715,-0.004447937,-0.004760742,0.048553467,-0.049865723,-0.09375,-0.04522705,0.062194824,0.0070228577,0.026107788,-0.019699097,-0.04888916,0.022277832,-0.0625,0.020706177,0.054870605,-0.02053833,-0.031585693,0.013801575,-0.03225708,-0.014877319,-0.013298035,0.046539307,-0.057861328,-0.02709961,-0.019119263,0.010559082,0.04888916,-0.060516357,-0.04522705,0.0030136108,0.035095215,0.025604248,-0.022277832,-0.026275635,-0.026428223,-0.06585693,-0.0009455681,-0.027938843,-0.030258179,-0.030929565,0.042053223,0.014297485,0.036895752,0.022277832,0.00081062317,0.024780273,0.0005583763,-0.012969971,-0.0592041,0.016799927,-0.017547607,-0.021606445,-0.0234375,0.01687622,-0.020370483,0.0017871857,-0.007858276,-0.02394104,0.029266357,0.033081055,-0.0016422272,-0.015792847,-0.0071487427,-0.032928467,-0.013298035,0.017868042,0.039733887,-0.011886597,0.014549255,0.0068588257,-0.021453857,0.0029296875,0.032409668,0.002275467,0.02444458,0.014549255,-0.01222229,-0.0009403229,-0.001496315,-0.035247803,-0.04888916,-0.025604248,0.018203735,-0.029754639,-0.02444458,-0.00409317,-0.042236328,-0.051208496,-0.031097412,0.019866943,-0.028259277,-0.008895874,-0.00868988,-0.0060272217,-0.004863739,-0.023773193,0.05419922,0.0051956177,0.0026493073,-0.009933472,-0.019210815,0.040893555,-0.033416748,0.017959595,0.036895752,0.015213013,-0.0007171631,0.014129639,-0.012138367,-0.026428223,-0.000623703,-0.05319214,0.012466431,-0.021606445,-0.010887146,0.0033874512,0.018035889,0.012306213,-0.020950317,-0.04888916,0.00806427,0.021942139,0.011634827,-0.029266357,-0.016296387,-0.013214111,-0.008148193,0.03375244,0.018371582,0.021942139,-0.011138916,-0.018371582,-0.0055274963,-0.027435303,-0.025772095,-0.04623413,-0.004261017,-0.016296387,0.023773193,0.026428223,0.05718994,0.030761719,0.0068588257,0.044891357,0.08514404,0.048217773,-0.002046585,-0.034576416,-0.026763916,0.011634827,-0.042388916,-0.047210693,0.025100708,0.015464783,-0.07348633,-0.018951416,0.014045715,0.0053215027,0.026428223,-0.002784729,0.01737976,0.101745605,0.019210815,-0.014961243,0.012306213,-0.014465332,-0.0158844,0.013717651,0.031585693,-0.009521484,-0.017623901,-0.014045715,-0.0076904297,-0.024612427,0.035247803,0.0014028549,0.008773804,0.016296387,0.06982422,-0.027267456,-0.02078247,0.011886597,0.02909851,0.03326416,0.10241699,0.0007586479,-0.019210815,-0.03640747,-0.0035114288,0.0038661957,0.039245605,0.018035889,-0.034088135,-0.048553467,0.025939941,-0.044555664,0.02444458,0.02027893,0.022613525,-0.025436401,0.020858765,-0.043548584,-0.05154419,-0.022781372,0.0037612915,-0.054534912,-0.039245605,0.009933472,0.046875,-0.027938843,-0.040222168,-0.020370483,0.04888916,-0.01197052,-0.022445679,-0.005256653,-0.031097412,0.003929138,-0.020202637,-0.045562744,-0.007858276,0.018203735,-0.072143555,0.016708374,-0.04006958,-0.031097412,-0.035736084,0.0017356873,-0.028259277,0.034576416,-0.014297485,0.0062332153,0.01222229,-7.3730946e-05,0.059509277,0.06518555,-0.0016622543,-0.017044067,0.0076904297,-0.052215576,-0.0029506683,0.011634827,0.0027637482,-0.047210693,0.039398193,0.037078857,-0.049224854,-0.0063591003,0.0059432983,-0.018707275,-0.040740967,0.0015792847,-0.0014028549,-0.015213013,0.05419922,0.022109985,-0.009643555,-0.013381958,-0.042388916,0.023605347,0.0011844635,0.049224854,-0.028930664,0.027770996,0.008773804,-0.023605347,-0.0061950684,-0.05618286,0.0028266907,0.024108887,0.026931763,0.016708374,0.012138367,0.037078857,-0.003200531,-0.037902832,-0.027435303,-0.029434204,-0.0007843971,-0.015045166,-0.009727478,-0.0031375885,-0.06951904,-0.04788208,-0.029266357,0.049865723,0.018539429,-0.02760315,0.023269653,-0.034423828,0.019042969,-0.003929138,0.036071777,-0.036895752,0.020706177,-0.03326416,0.016799927,0.031433105,0.009933472,0.00843811,0.022277832,0.04107666,-0.024780273,0.08215332,0.03540039,0.019363403,-0.064819336,-0.017623901,-0.02394104,0.025604248,-0.03756714,0.036590576,-0.0068588257,-0.021026611,-0.05319214,0.04623413,-0.024108887,0.031097412,0.014717102,-0.028930664,-0.06414795,-0.08111572,-0.038909912,-2.43783e-05,-0.011634827,0.011306763,-0.008476257,-0.022445679,0.019363403,-0.004989624,0.027938843,0.048553467,-0.010429382,-0.022949219,0.068847656,-0.009269714,0.0054016113,-0.044555664,-0.0065689087,0.008605957,0.00024163723,-0.029922485,-0.0075645447,0.01637268,-0.023117065,-0.053527832,0.023117065,-0.008354187,-0.048217773,-0.02394104,-0.018295288,-0.02003479,0.009811401,0.014213562,-0.011474609,-0.009643555,-0.07318115,-0.008018494,-0.017211914,-0.05053711,-0.011390686,-0.02394104,0.030426025,-0.008522034,0.029266357,0.015464783,0.0025463104,-0.022949219,0.025939941,-0.035247803,0.015213013,-0.0007481575,-0.07446289,0.036590576,0.03640747,0.0026493073,-0.009391785,0.012390137,-0.024108887,0.0042381287,-0.035095215,-0.02659607,-0.007858276,-0.006942749,-0.013298035,-0.034576416,0.023269653,-0.027435303,-0.023605347,0.026275635,-0.027938843,-0.03225708,-0.002161026,-0.024612427,-0.018875122,-0.009223938,-0.00024938583,0.020202637,0.018447876,0.005092621,0.015045166,0.01687622,-0.013053894,-0.0056114197,-0.014877319,0.05419922,-0.012634277,-7.855892e-05,-0.03475952,-0.08343506,0.028762817,-0.0058174133,-0.08709717,-0.030761719,0.05984497,0.037750244,8.21352e-05,0.026275635,0.046875,0.016540527,-0.0030345917,-0.02960205,-0.0234375,0.0005168915,0.014801025,0.004966736,-0.036254883,0.00040006638,0.056518555,0.024276733,-0.05886841,-0.034088135,-0.012718201,0.04257202,0.052856445,-0.0043029785,-0.06317139,0.018951416,0.022109985,0.0064849854,0.0063171387,6.3955784e-05,0.0060691833,0.022949219,-0.07446289,0.028427124,-0.013214111,0.053527832,-0.017959595,-0.035247803,0.03591919,0.013465881,0.027435303,-0.047546387,-0.01878357,0.011558533,-0.03375244,0.027938843,0.017623901,-0.0016517639,0.03326416,0.0043029785,-0.021453857,0.006526947,0.014465332,0.04171753,-0.0012369156,-0.044891357,-0.025268555,-0.038085938,0.031433105,0.017700195,-0.008895874,0.02444458,-0.042388916,-0.0050697327,0.0057373047,0.018539429,0.03640747,0.0043640137,0.0005950928,-0.0004911423,0.007522583,0.04171753,0.009521484,0.031433105,-0.064819336,0.010971069,-0.054870605,-0.025100708,0.02128601,-0.030593872,0.010643005,0.021942139,0.039398193,-0.019210815,-0.00026631355,-0.04421997,0.013137817,-0.038238525,-0.0031375885,0.0011739731,0.0073165894,0.029922485,-0.013969421,0.089782715,0.0234375,-0.012550354,0.011054993,0.020614624,0.03375244,0.019699097,-0.029266357,0.0317688,0.04623413,0.02859497,0.026275635,0.00027656555,-0.046539307,0.007358551,0.022109985,-0.051208496,0.021453857,-0.005695343,0.0031375885,-0.023773193,-0.015548706,-0.021774292,-0.042236328,0.06185913,0.042388916,-0.027435303,0.034088135,-0.031921387,0.04888916,-0.028427124,0.05819702,-0.07348633,0.023269653,-0.011390686,0.025604248,-0.015548706,0.008148193,0.0035533905,0.07714844,0.013381958,0.064819336,0.012886047,0.01637268,-0.052856445,-0.0038871765,-0.0028266907,-0.015625,0.02027893,-0.030593872,-0.022949219,0.0061950684,-0.0032844543,-0.0026397705,0.051879883,0.047210693,-0.0026283264,-0.039733887,-0.014961243,-0.004863739]", "client_status_at": "2025-10-02T09:53:05.994166Z", "server_status_at": "2025-10-02T09:53:05.994146Z", "created_at": "2025-10-02T08:53:02.488417Z"}, {"id": "9bc1ced6-828c-4705-a479-170757a5a2aa", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://www.dailydoseofds.com/gaussian-mixture-models-gmm/", "canonical_url": "https://www.dailydoseofds.com/gaussian-mixture-models-gmm/", "title": "Gaussian Mixture Models (GMMs)", "source_site": "Daily Dose of Data Science", "publication_date": "2023-08-16T00:00:00Z", "favicon_url": "https://www.dailydoseofds.com/favicon.ico", "content_markdown": "## Introduction\n\nKMeans is an unsupervised clustering algorithm that groups data based on distances. It is widely recognized for its simplicity and effectiveness as a clustering algorithm.\n\nEssentially, the core idea is to partition a dataset into distinct clusters, with each point belonging to the cluster whose centroid is closest to it.\n\nWhile its simplicity often makes it the most preferred clustering algorithm, KMeans has many limitations that hinder its effectiveness in many scenarios.\n\n## The shortcomings of KMeans Clustering\n\nKMeans comes with its own set of limitations that can restrict its performance in certain situations.\n\n### #1) KMeans does not account for cluster variance\n\nOne of the primary limitations is its assumption of spherical clusters.\n\nOne intuitive and graphical way to understand the KMeans algorithm is to place a **circle** at the center of each cluster, which encloses the points.\n\nAs KMeans is all about placing circles, its results aren’t ideal when the dataset has irregular shapes or varying sizes, as shown below:\n\nInstead, an ideal clustering should cluster the data as follows:\n\nThis rigidity of KMeans to only cluster globular clusters often leads to misclassification and suboptimal cluster assignments.\n\n### #2) KMeans only relies on distance\n\nThis limitation is somewhat connected to the one we discussed above.\n\nImagine we have the following dataset:\n\nClearly, the blue cluster has a larger spread.\n\nTherefore, ideally, its influence should be larger as well.\n\nHowever, when assigning a new data point to a cluster, KMeans only considers the distance to the centroid.\n\nThis means that it creates a margin for assigning a new data point to a cluster that is equidistant from both centroids.\n\nBut considering the area of influence of the right cluster, having this margin more to the left makes more sense.\n\n### #3) KMeans does not output probabilities\n\nKMeans clustering performs hard assignments.\n\nIn simple words, this means that **a specific data point can belong to only one cluster**.\n\nThus, it does not provide probabilistic estimates of a given data point belonging to each possible cluster.\n\nAlthough this might be a problem per se, it limits its usefulness in uncertainty estimation and downstream applications that require a probabilistic interpretation.\n\nThese limitations often make KMeans a non-ideal choice for clustering.\n\nTherefore, learning about other better algorithms that we can use to address these limitations is extremely important.\n\n**Therefore, in this article, we will learn about Gaussian mixture models.**\n\nMore specifically, we shall cover:\n\n- Shortcomings of KMeans (already covered above)\n- What is the motivation behind GMMs?\n- How do GMMs work?\n- The intuition behind GMMs.\n- Plotting some dummy multivariate Gaussian distributions to better understand GMMs.\n- The entire mathematical formulation of GMMs.\n- How to use Expectation-Maximization to model data using GMMs?\n**Coding a GMM from scratch (without sklearn).**- Comparing results of GMMs with KMeans.\n- How to determine the optimal number of clusters for GMMs?\n- Some practical use cases of GMMs.\n- Takeaways.\n\nLet's begin!\n\n## Gaussian Mixture Models\n\nAs the name suggests, a **Gaussian mixture model** clusters a dataset that has a mixture of multiple Gaussian distributions.\n\nThey can be thought of as a generalized twin of KMeans.\n\n**Simply put, in 2 dimensions, while KMeans only create circular clusters, gaussian mixture models can create oval-shaped clusters.**\n\nBut how do they do that?\n\nLet's understand in more detail.", "content_text": "Introduction\nKMeans is an unsupervised clustering algorithm that groups data based on distances. It is widely recognized for its simplicity and effectiveness as a clustering algorithm.\nEssentially, the core idea is to partition a dataset into distinct clusters, with each point belonging to the cluster whose centroid is closest to it.\nWhile its simplicity often makes it the most preferred clustering algorithm, KMeans has many limitations that hinder its effectiveness in many scenarios.\nThe shortcomings of KMeans Clustering\nKMeans comes with its own set of limitations that can restrict its performance in certain situations.\n#1) KMeans does not account for cluster variance\nOne of the primary limitations is its assumption of spherical clusters.\nOne intuitive and graphical way to understand the KMeans algorithm is to place a circle at the center of each cluster, which encloses the points.\nAs KMeans is all about placing circles, its results aren’t ideal when the dataset has irregular shapes or varying sizes, as shown below:\nInstead, an ideal clustering should cluster the data as follows:\nThis rigidity of KMeans to only cluster globular clusters often leads to misclassification and suboptimal cluster assignments.\n#2) KMeans only relies on distance\nThis limitation is somewhat connected to the one we discussed above.\nImagine we have the following dataset:\nClearly, the blue cluster has a larger spread.\nTherefore, ideally, its influence should be larger as well.\nHowever, when assigning a new data point to a cluster, KMeans only considers the distance to the centroid.\nThis means that it creates a margin for assigning a new data point to a cluster that is equidistant from both centroids.\nBut considering the area of influence of the right cluster, having this margin more to the left makes more sense.\n#3) KMeans does not output probabilities\nKMeans clustering performs hard assignments.\nIn simple words, this means that a specific data point can belong to only one cluster.\nThus, it does not provide probabilistic estimates of a given data point belonging to each possible cluster.\nAlthough this might be a problem per se, it limits its usefulness in uncertainty estimation and downstream applications that require a probabilistic interpretation.\nThese limitations often make KMeans a non-ideal choice for clustering.\nTherefore, learning about other better algorithms that we can use to address these limitations is extremely important.\nTherefore, in this article, we will learn about Gaussian mixture models.\nMore specifically, we shall cover:\n- Shortcomings of KMeans (already covered above)\n- What is the motivation behind GMMs?\n- How do GMMs work?\n- The intuition behind GMMs.\n- Plotting some dummy multivariate Gaussian distributions to better understand GMMs.\n- The entire mathematical formulation of GMMs.\n- How to use Expectation-Maximization to model data using GMMs?\n- Coding a GMM from scratch (without sklearn).\n- Comparing results of GMMs with KMeans.\n- How to determine the optimal number of clusters for GMMs?\n- Some practical use cases of GMMs.\n- Takeaways.\nLet's begin!\nGaussian Mixture Models\nAs the name suggests, a Gaussian mixture model clusters a dataset that has a mixture of multiple Gaussian distributions.\nThey can be thought of as a generalized twin of KMeans.\nSimply put, in 2 dimensions, while KMeans only create circular clusters, gaussian mixture models can create oval-shaped clusters.\nBut how do they do that?\nLet's understand in more detail.", "content_token_count": 698, "client_status": "queued", "server_status": "embedded", "summary": "The article discusses the limitations of KMeans clustering, such as its assumption of spherical clusters, reliance solely on distance, and lack of probabilistic outputs. It introduces Gaussian Mixture Models (GMMs) as an alternative that addresses these shortcomings by allowing for more flexible, oval-shaped clusters and providing probabilistic estimates.", "expiry_score": 0.3, "mistral_embedding": "[-0.028396606,0.026626587,0.05529785,0.005443573,0.044708252,-0.027801514,0.04208374,-0.025741577,-0.00023329258,-0.0075035095,-0.041778564,0.042663574,-0.034118652,-0.00459671,-0.06768799,0.04031372,0.012138367,0.0030155182,0.037963867,0.007797241,-0.036621094,-0.045013428,-0.008682251,0.012283325,-0.01448822,-0.020599365,-0.011398315,-0.058563232,-0.02279663,0.001701355,0.009857178,-0.014198303,-0.0009150505,0.029281616,-0.010810852,-0.033233643,-0.019424438,-0.05618286,0.0065460205,-0.02722168,0.009857178,-0.0028324127,0.008934021,-0.021621704,-0.010223389,-0.02104187,0.002483368,-0.014419556,0.030014038,-0.024719238,-0.009117126,0.045318604,0.02279663,-0.026184082,-0.016326904,0.057678223,0.03515625,-0.0008044243,-0.070007324,0.030151367,-0.013389587,0.00856781,-0.0052604675,-0.008865356,0.021484375,0.06500244,0.0129470825,0.0021152496,0.0035858154,-0.017654419,0.044128418,-0.007537842,0.0061416626,0.014122009,-0.036468506,-0.01802063,0.00072193146,0.013389587,0.029571533,0.018173218,-0.0211792,-0.0010986328,0.07531738,-0.011619568,-0.04119873,-0.00065755844,0.02897644,0.028533936,-0.037078857,0.0023174286,0.02029419,0.0029792786,0.039123535,-0.0050735474,0.060028076,0.06149292,-0.037506104,-0.0032539368,-0.02809143,0.01802063,0.043548584,-0.0038795471,0.07354736,0.004524231,0.02104187,-0.01537323,0.015449524,0.011772156,-0.0066566467,-0.0423584,-0.07208252,-0.007797241,-0.010223389,-0.053253174,-0.006362915,0.023239136,-0.04296875,0.044708252,-0.07531738,-0.053833008,0.03353882,0.04058838,0.0022068024,0.009307861,-0.04119873,-0.024856567,-0.0011396408,0.025299072,-0.03074646,-0.013389587,-0.019714355,-0.009117126,-0.019714355,-0.03149414,0.016479492,-0.03161621,0.0158844,-0.021484375,-0.022659302,-0.016693115,0.03353882,-0.014854431,-0.02722168,0.009559631,0.011985779,-0.0037879944,0.03765869,0.03265381,-0.040893555,-0.010299683,-0.00856781,0.020599365,0.06060791,-0.010223389,-0.029129028,-0.020889282,-0.01676941,-0.023834229,0.05795288,-0.02015686,-0.031341553,0.03265381,0.011985779,0.05206299,-0.00022184849,-0.030014038,-0.019119263,0.001912117,-0.01448822,0.009559631,0.044433594,-0.008529663,0.016616821,0.070617676,0.023971558,0.052947998,0.015449524,-0.0001988411,0.011177063,0.05090332,0.012870789,-0.05206299,-0.0031261444,0.0033283234,0.022216797,0.0028495789,-0.026779175,-0.009307861,-0.01676941,-0.011329651,-0.01676941,-0.048553467,-0.030593872,0.045898438,-0.016479492,0.05206299,0.011398315,-0.032073975,0.024719238,0.008903503,0.009712219,0.026031494,0.013168335,0.05444336,0.01802063,0.012359619,0.017501831,0.048828125,0.03942871,-0.0211792,-0.0009560585,0.005958557,-0.020736694,-0.0034561157,-0.055023193,-0.019714355,-0.0039520264,0.0023441315,-0.0018482208,-0.016174316,0.023971558,-0.014419556,-0.025299072,-0.0043029785,-0.003107071,0.06237793,0.032073975,0.0061798096,-0.013977051,0.022216797,-0.004928589,0.031188965,0.016616821,-0.0385437,0.034729004,-0.018539429,0.024276733,0.02722168,0.039123535,-0.0065078735,-0.013824463,0.03149414,-0.053253174,0.039733887,0.01838684,-0.026626587,-0.025161743,-0.032073975,0.00039315224,0.047943115,-0.06561279,-0.003768921,-0.044128418,-0.04119873,-0.037506104,-0.011619568,-0.031036377,-0.041778564,0.035308838,-0.032073975,-0.0005836487,0.026473999,0.028839111,-0.0152282715,0.022064209,0.0057373047,-0.017807007,-0.013015747,0.09240723,0.005554199,-0.024856567,-0.019866943,0.011917114,0.006290436,0.0597229,-0.009559631,0.012649536,0.0036411285,0.0051498413,-0.0009379387,-0.019714355,-0.018829346,0.0065078735,0.040893555,-0.035888672,-0.008529663,0.029418945,-0.029571533,0.032226562,-0.04031372,-0.0041389465,0.05206299,0.0057373047,-0.009857178,-0.057678223,-0.001241684,0.068847656,0.00198555,-0.0017652512,0.023971558,0.03515625,-0.047668457,-0.0132369995,-0.0012044907,0.0703125,-0.03604126,-0.022354126,-0.03338623,-0.015670776,-0.045013428,-0.011177063,-0.027648926,-0.017288208,-0.0048179626,-0.008644104,0.016845703,-0.012504578,-0.018981934,-0.033111572,-0.040893555,-0.0076141357,-0.0070610046,0.056793213,0.0066566467,0.037078857,-0.014854431,-0.031341553,-0.0635376,0.012649536,0.0105896,0.0030345917,0.019561768,0.03265381,0.051208496,-0.014266968,-0.0038070679,0.054718018,0.06060791,-0.06060791,-0.026473999,0.009559631,-0.017578125,0.0087890625,0.029281616,-0.0069122314,-0.017654419,-0.021911621,0.0087890625,0.03692627,0.023544312,0.0043411255,-0.025161743,-0.0073928833,0.057678223,0.012802124,0.1071167,-0.027511597,-0.040893555,0.009712219,0.04031372,-0.019119263,0.0033836365,0.04647827,0.02015686,0.022506714,-0.025741577,-0.020446777,0.005958557,0.009780884,-0.009269714,0.017288208,2.4020672e-05,0.01838684,0.026916504,0.023391724,0.01927185,0.010368347,-0.02545166,0.018676758,-0.06414795,-0.07122803,0.008346558,0.008163452,0.01574707,-0.015007019,-0.012283325,-0.056488037,-0.009857178,-0.017364502,0.113586426,0.03353882,0.027954102,0.037963867,0.00340271,0.01184082,0.0047798157,0.011108398,0.019866943,0.04296875,0.00340271,-0.010810852,-0.037078857,0.038238525,0.00021266937,0.0016736984,-0.038238525,0.013824463,0.020599365,8.046627e-05,-0.025741577,-0.021621704,0.019119263,-0.027648926,0.024276733,-0.023101807,0.024414062,-0.019561768,0.023101807,0.036468506,0.008644104,0.021621704,-0.0022792816,-0.04824829,0.018753052,0.014854431,-0.045013428,0.010887146,-0.003145218,-0.04324341,0.036193848,-0.028396606,-0.049713135,0.018829346,-0.036468506,-0.045318604,0.033233643,0.03353882,-0.019714355,-0.035736084,-0.033691406,-0.0158844,-0.014854431,0.007133484,0.02456665,-0.015007019,0.021331787,0.018981934,0.023971558,0.02368164,-0.021911621,0.028396606,-0.018753052,-0.030593872,0.034851074,0.016036987,0.026473999,-0.07147217,-0.030014038,0.0025196075,0.0017929077,0.012802124,0.020889282,-0.034729004,-0.0770874,-0.031341553,0.034851074,0.0069503784,0.04119873,-0.033111572,-0.07122803,0.025894165,-0.058258057,-0.018829346,0.031036377,0.018096924,-0.0016736984,0.062683105,-0.022506714,-0.03250122,-0.025299072,0.06915283,-0.00040221214,-0.02104187,-0.030303955,0.057678223,0.03149414,-0.05444336,-0.019866943,0.051483154,0.028396606,0.044708252,-0.030593872,-0.012580872,-0.018234253,-0.039733887,-0.030303955,-0.031341553,-0.041778564,-0.035003662,0.02633667,0.012428284,0.0030899048,0.0087509155,-0.00032186508,0.021774292,-0.009376526,-0.005001068,-0.008422852,0.047668457,-0.04559326,-0.030014038,-0.0042648315,0.023391724,-0.010520935,0.014122009,-0.023391724,-0.035003662,0.011398315,-0.0076141357,0.019714355,0.0076141357,0.0014247894,-0.05444336,-0.018753052,0.028533936,0.034576416,0.0107421875,0.020446777,0.001783371,-0.05001831,0.006767273,0.018463135,-0.011398315,0.0065460205,0.05178833,0.008056641,-0.024276733,0.033233643,-0.031036377,-0.035888672,-0.075927734,0.052947998,-0.027801514,-0.032806396,-0.03427124,0.010887146,-0.03765869,-0.024276733,0.03515625,0.0035114288,0.0129470825,0.0027770996,0.0035305023,-0.025009155,-0.027359009,0.012870789,-0.03250122,-0.0061416626,-0.030456543,-0.0014982224,0.0423584,-0.004966736,0.039123535,0.03677368,0.032226562,-0.0073928833,-0.0030155182,-0.017654419,-0.011550903,-0.0033092499,-0.023544312,0.03515625,-0.01802063,-0.0069503784,0.009712219,-0.008346558,0.001830101,0.027954102,-0.037963867,0.0423584,0.013092041,0.0211792,-0.02986145,-0.03427124,-0.0017194748,0.0013513565,0.04058838,0.026031494,-0.012870789,-0.01625061,0.000620842,0.0005264282,-0.046783447,-0.03161621,-0.06149292,0.04147339,0.047088623,0.0770874,-0.0026855469,0.04208374,0.030014038,0.00012302399,0.029418945,0.05001831,0.04208374,-0.010223389,-0.07824707,0.01713562,0.021621704,-0.02809143,-0.034851074,0.066223145,0.01625061,-0.03942871,-0.04208374,0.006324768,0.0070228577,-0.0155181885,0.0423584,0.009780884,0.057373047,0.015007019,-0.008934021,-0.01272583,0.013313293,-0.008460999,0.020004272,0.01713562,-0.0061035156,0.021621704,0.03765869,0.025894165,0.028533936,0.041778564,0.0064735413,0.013679504,0.009780884,0.059417725,0.007873535,-0.01184082,-0.025604248,0.035461426,0.016036987,0.055023193,-0.013458252,-0.020736694,-0.0027942657,-0.04058838,0.003566742,0.03149414,-0.003768921,-0.031921387,-0.044128418,0.04208374,-0.036468506,0.0032367706,0.017578125,0.0010204315,0.0020217896,0.033691406,0.013389587,-0.050323486,0.012580872,-0.013389587,-0.04647827,0.0047073364,-0.051483154,0.036346436,-0.05001831,-0.072387695,0.023239136,0.047668457,-0.023834229,-0.062072754,-0.023391724,0.008056641,-0.0057754517,0.0026474,-0.072387695,-0.01448822,0.029708862,-0.01361084,-0.019119263,0.024414062,0.020446777,0.02633667,-0.00022757053,-0.02809143,0.050323486,-0.02809143,0.05090332,0.009117126,-0.009307861,0.01448822,0.05529785,0.037353516,-0.0034751892,-0.01007843,-0.038848877,0.02456665,0.017944336,0.03427124,-0.07385254,-0.003419876,0.038238525,-0.047088623,-0.0026664734,-0.014564514,-0.016998291,-0.038238525,-0.010299683,-0.04031372,-0.010299683,0.05001831,-0.019561768,-0.031921387,0.018234253,-0.032348633,0.07147217,0.009490967,0.034851074,-0.06530762,-0.003162384,0.03250122,-0.04296875,0.0043945312,-0.07501221,0.0030155182,0.045898438,-0.0074272156,-0.04559326,0.025604248,-0.017944336,0.0029239655,-0.062683105,0.011917114,0.0088272095,-0.005332947,-0.07470703,-0.00724411,-0.031921387,-0.04147339,-0.011917114,-0.05267334,0.012580872,-0.0211792,-0.0491333,0.012580872,0.024719238,0.023101807,-0.03515625,0.019714355,-0.014419556,0.016693115,0.0070228577,0.022354126,-0.0073928833,0.06149292,-0.016555786,-0.010002136,0.040008545,-0.020004272,0.020599365,0.066467285,-0.02104187,-0.055023193,0.026626587,-0.068237305,0.025894165,0.0051498413,0.049713135,-0.018173218,0.011398315,-0.07647705,0.07824707,-0.0211792,0.033996582,0.02368164,-0.031341553,-0.019561768,-0.027648926,-0.04736328,-0.0014715195,-0.0022792816,0.011253357,0.023239136,-0.006324768,0.017211914,0.012870789,0.015296936,-0.014930725,0.05795288,0.002243042,0.06149292,-0.0055885315,-0.0010576248,-0.015960693,0.0317688,-0.02104187,0.0017557144,-0.049438477,-0.02015686,0.03265381,0.056488037,-0.050323486,-0.007797241,-0.021484375,-0.0018482208,0.004890442,-0.04058838,0.015594482,-0.0023899078,-0.047668457,-0.016693115,-0.039733887,-0.089416504,-0.03353882,-0.0158844,-0.045318604,0.0025749207,0.0061035156,-0.012504578,-0.0087890625,0.0385437,-0.011917114,0.024719238,-0.037078857,-0.031036377,-0.0016736984,0.00067567825,0.007648468,-0.054718018,0.029418945,0.004558563,0.0029964447,-0.011177063,0.044708252,-0.007133484,0.016845703,-0.068847656,-0.01007843,0.0074272156,-0.01713562,0.006839752,-0.011108398,0.033111572,-0.017364502,-0.040008545,0.0211792,0.013389587,-0.051483154,0.01713562,0.01713562,-0.035888672,0.011917114,-0.027511597,-0.0070610046,0.06854248,0.003768921,-0.016479492,0.0012140274,-0.028244019,0.040893555,0.00459671,0.051483154,-0.026184082,-0.014419556,0.0073547363,-0.051208496,0.027648926,-0.011550903,-0.046783447,-0.027511597,0.060302734,0.02633667,0.0056648254,0.03265381,0.012062073,0.033233643,0.011253357,-0.023101807,-0.04559326,-0.00012981892,0.04296875,-0.0211792,-0.08648682,0.043823242,0.056793213,0.012207031,0.01625061,0.0385437,-0.01574707,0.052947998,0.04324341,-0.022064209,-0.06738281,0.023391724,0.011917114,-0.018829346,-0.01537323,-0.04647827,-0.011253357,0.054718018,-0.05444336,-0.0020599365,-0.009048462,0.07092285,-0.009414673,-0.023971558,0.009559631,0.028533936,0.012580872,-0.030151367,-0.007209778,-0.01927185,-0.012138367,0.012062073,0.0088272095,-0.0051498413,0.07293701,-0.02015686,-0.0070610046,0.01838684,-0.004322052,-0.0054779053,-0.0061035156,-0.035003662,-0.044708252,-0.045318604,0.044128418,0.0059928894,-0.017364502,0.023971558,-0.04208374,0.0056266785,0.026626587,0.023239136,-0.005554199,0.022354126,0.019424438,-0.011253357,0.027801514,0.003566742,-0.022506714,0.0054779053,-0.039123535,-0.0065460205,-0.030593872,-0.013092041,0.0007677078,-0.004043579,0.0037326813,0.034729004,0.011550903,0.0041923523,-0.040008545,0.008125305,0.04559326,-0.052947998,0.021484375,-0.0020217896,-0.012580872,0.016555786,-0.040008545,0.05090332,0.0010433197,0.04296875,-0.030151367,0.010444641,0.019119263,-0.0051841736,-0.049713135,0.036468506,0.027359009,0.017059326,0.039733887,0.009338379,-0.01802063,0.02368164,0.0009560585,-0.07824707,0.009231567,0.034118652,0.046783447,-0.011329651,-0.016555786,0.0023899078,-0.006324768,0.0423584,0.010955811,-0.030303955,0.026031494,-0.04647827,0.048828125,-0.018981934,0.033996582,-0.016921997,0.019561768,-0.012138367,-0.0033092499,-0.00034475327,0.02986145,0.034851074,0.06738281,-0.03074646,0.024856567,-0.027359009,0.00018155575,-0.037506104,-0.029129028,0.022659302,0.0018568039,0.018539429,-0.037963867,-0.054138184,-0.0027389526,0.0055885315,0.026031494,0.007873535,0.04736328,0.037963867,-0.026473999,0.017944336,-0.003660202]", "client_status_at": "2025-10-02T09:52:59.217015Z", "server_status_at": "2025-10-02T09:52:59.216999Z", "created_at": "2025-10-02T08:52:56.516680Z"}, {"id": "d86fbd20-fd8a-4588-bfd3-0f10e1fc6281", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://www.theverge.com/news/785193/google-deepmind-gemini-ai-robotics-web-search", "canonical_url": "https://www.theverge.com/news/785193/google-deepmind-gemini-ai-robotics-web-search", "title": "Google DeepMind’s new AI models can search the web to help robots complete tasks", "source_site": "The Verge", "publication_date": "2025-09-25T00:00:00Z", "favicon_url": "https://www.theverge.com/favicon.ico", "content_markdown": "Google DeepMind says its upgraded AI models enable robots to complete more complex tasks — and even tap into the web for help. During a press briefing, Google DeepMind’s head of robotics, Carolina Parada, told reporters that the company’s new AI models work in tandem to allow robots to “think multiple steps ahead” before taking action in the physical world.\n\n# Google DeepMind’s new AI models can search the web to help robots complete tasks\n\nThe new Gemini Robotics 1.5 models enable robots to carry out multistep tasks and even learn from each other.\n\nThe new Gemini Robotics 1.5 models enable robots to carry out multistep tasks and even learn from each other.\n\nThe system is powered by the newly launched Gemini Robotics 1.5 alongside the embodied reasoning model, Gemini Robotics-ER 1.5, which are updates to AI models that Google DeepMind [introduced in March](/news/628021/google-deepmind-gemini-robotics-ai-models). Now robots can perform more than just singular tasks, such as folding a piece of paper or unzipping a bag. They can now do things like separate laundry by dark and light colors, pack a suitcase based on the current weather in London, as well as help someone sort trash, compost, and recyclables based on a web search tailored to a location’s specific requirements.\n\n“The models up to now were able to do really well at doing one instruction at a time in a way that is very general,” Parada said. “With this update, we’re now moving from one instruction to actually genuine understanding and problem-solving for physical tasks.”\n\nTo do this, robots can use the upgraded Gemini Robotics-ER 1.5 model to form an understanding of their surroundings, and use digital tools like Google Search to find more information. Gemini Robotics-ER 1.5 then translates those findings into natural language instructions for Gemini Robotics 1.5, allowing the robot to use the model’s vision and language understanding to carry out each step.\n\nAdditionally, Google DeepMind announced that Gemini Robotics 1.5 can help robots “learn” from each other, even if they have different configurations. Google DeepMind found that tasks presented to the ALOHA2 robot, which consists of two mechanical arms, “just work” on the [bi-arm Franka robot](https://franka.de/franka-research-3-arm), as well as [Apptronik’s humanoid robot Apollo](https://apptronik.com/apollo). “This enables two things for us: one is to control very different robots — including a humanoid — with a single model,” Google DeepMind software engineer Kanishka Rao said during the briefing. “And secondly, skills that are learned on one robot can now be transferred to another robot.”\n\nAs part of the update, Google DeepMind is rolling out Gemini Robotics-ER 1.5 to developers through the Gemini API in Google AI Studio, while only select partners can access Gemini Robotics 1.5.\n\n**Follow topics and authors**from this story to see more like this in your personalized homepage feed and to receive email updates.\n\n## Most Popular\n\n- Microsoft revamps Xbox Game Pass plans and hikes Ultimate to $29.99 a month\n- Satya Nadella appoints a new CEO to run Microsoft’s biggest businesses\n- Google is blocking AI searches for Trump and dementia\n- Behold: The Lego Game Boy has already been modded to play games for real\n- Google’s Home Speaker looks and sounds better — but you can’t get it yet", "content_text": "Google DeepMind says its upgraded AI models enable robots to complete more complex tasks — and even tap into the web for help. During a press briefing, Google DeepMind’s head of robotics, Carolina Parada, told reporters that the company’s new AI models work in tandem to allow robots to “think multiple steps ahead” before taking action in the physical world.\nGoogle DeepMind’s new AI models can search the web to help robots complete tasks\nThe new Gemini Robotics 1.5 models enable robots to carry out multistep tasks and even learn from each other.\nThe system is powered by the newly launched Gemini Robotics 1.5 alongside the embodied reasoning model, Gemini Robotics-ER 1.5, which are updates to AI models that Google DeepMind introduced in March. Now robots can perform more than just singular tasks, such as folding a piece of paper or unzipping a bag. They can now do things like separate laundry by dark and light colors, pack a suitcase based on the current weather in London, as well as help someone sort trash, compost, and recyclables based on a web search tailored to a location’s specific requirements.\n“The models up to now were able to do really well at doing one instruction at a time in a way that is very general,” Parada said. “With this update, we’re now moving from one instruction to actually genuine understanding and problem-solving for physical tasks.”\nTo do this, robots can use the upgraded Gemini Robotics-ER 1.5 model to form an understanding of their surroundings, and use digital tools like Google Search to find more information. Gemini Robotics-ER 1.5 then translates those findings into natural language instructions for Gemini Robotics 1.5, allowing the robot to use the model’s vision and language understanding to carry out each step.\nAdditionally, Google DeepMind announced that Gemini Robotics 1.5 can help robots “learn” from each other, even if they have different configurations. Google DeepMind found that tasks presented to the ALOHA2 robot, which consists of two mechanical arms, “just work” on the bi-arm Franka robot, as well as Apptronik’s humanoid robot Apollo. “This enables two things for us: one is to control very different robots — including a humanoid — with a single model,” Google DeepMind software engineer Kanishka Rao said during the briefing. “And secondly, skills that are learned on one robot can now be transferred to another robot.”\nAs part of the update, Google DeepMind is rolling out Gemini Robotics-ER 1.5 to developers through the Gemini API in Google AI Studio, while only select partners can access Gemini Robotics 1.5.\nMost Popular\n- Microsoft revamps Xbox Game Pass plans and hikes Ultimate to $29.99 a month\n- Satya Nadella appoints a new CEO to run Microsoft’s biggest businesses\n- Google is blocking AI searches for Trump and dementia\n- Behold: The Lego Game Boy has already been modded to play games for real\n- Google’s Home Speaker looks and sounds better — but you can’t get it yet", "content_token_count": 629, "client_status": "queued", "server_status": "embedded", "summary": "Google DeepMind has introduced upgraded AI models, Gemini Robotics 1.5 and Gemini Robotics-ER 1.5, that enable robots to perform complex, multistep tasks and learn from each other. These models allow robots to use web searches and adapt to different configurations, significantly enhancing their problem-solving capabilities.", "expiry_score": 0.6, "mistral_embedding": "[-0.028335571,0.05166626,0.041900635,-0.0045814514,0.036376953,-0.012062073,0.050079346,0.013244629,0.021087646,-0.055358887,-0.029129028,0.061157227,-0.04031372,0.018981934,-0.06036377,-0.010345459,0.01725769,-0.00642395,0.038757324,-0.027801514,-0.044281006,0.02293396,-0.027023315,0.0053710938,0.009094238,-0.0044822693,-0.03717041,-0.039001465,-0.06036377,0.0044136047,0.013244629,-0.043762207,0.019104004,-0.0054359436,0.035583496,-0.020553589,0.00029850006,-0.03375244,0.01007843,0.004875183,-0.020828247,0.027542114,0.0012769699,-0.023986816,-0.033996582,-0.00579834,-0.000385046,-0.03717041,0.024902344,-0.007774353,-0.027679443,0.045074463,0.009162903,-0.029251099,-0.015945435,-0.010803223,0.012588501,-0.017654419,-0.03479004,0.0118637085,-0.041107178,-0.0013504028,0.013114929,-0.021224976,0.018325806,0.029251099,0.024246216,-0.005962372,0.01449585,0.0035095215,-0.0021095276,0.03955078,-0.016540527,0.027023315,-0.00856781,-0.04901123,-0.0099487305,-0.0084991455,0.062194824,0.0023727417,-0.010414124,0.014167786,0.04901123,-0.05718994,-0.03717041,-0.029922485,-0.032287598,-0.030044556,-0.0051078796,-0.0012025833,0.047973633,0.021743774,-0.014099121,-0.009162903,0.09069824,0.06088257,-0.01159668,0.044006348,-0.021087646,0.04928589,0.039001465,-0.020553589,0.088012695,0.0022907257,0.013771057,-0.024902344,0.027542114,-0.02305603,-0.026229858,-0.030319214,-0.05166626,-0.044281006,0.02015686,-0.08062744,-0.03717041,0.0068206787,-0.030319214,0.05984497,-0.06298828,-0.029647827,0.047180176,0.025436401,-0.01977539,0.006160736,-0.018585205,-0.06958008,-0.017654419,0.016281128,-0.007411957,-0.04244995,0.012191772,-0.0021247864,-0.016403198,-0.010345459,0.030044556,-0.035583496,0.03375244,-0.01133728,0.0031967163,-0.0016145706,0.016281128,-0.04824829,0.0033779144,0.00579834,-0.005569458,0.010017395,0.02557373,0.050598145,-0.0032615662,-0.041656494,-0.0036907196,0.05429077,0.035858154,-0.018188477,-0.027542114,-0.014694214,-0.059051514,-0.0015153885,0.032684326,-0.029525757,-0.04321289,-0.0075798035,0.015419006,0.036376953,-0.019241333,-0.015090942,-0.007347107,0.0036239624,-0.029129028,-0.015808105,0.053497314,-0.024642944,0.04824829,0.056152344,-0.04534912,0.041900635,0.011199951,-0.0025043488,0.06903076,0.026489258,-0.007972717,0.00029850006,0.038757324,0.022140503,0.03665161,-0.014762878,-0.038482666,0.0096206665,-0.010612488,0.013969421,-0.024124146,-0.03451538,-0.0128479,0.023849487,-0.0036411285,0.04928589,0.028076172,-0.014961243,0.011665344,-0.017654419,0.03768921,0.04006958,-0.003921509,0.05508423,0.041656494,0.04244995,0.011795044,0.027420044,0.016799927,-0.01436615,-0.015945435,0.041656494,0.026489258,-0.02267456,-0.041656494,-0.012390137,0.03665161,-0.021881104,-0.015159607,-0.031097412,0.0065574646,-0.016998291,0.04348755,0.0038051605,0.04348755,0.025039673,-0.014297485,-0.0044136047,0.024246216,0.020553589,0.006095886,0.008369446,0.00069618225,-8.523464e-06,0.008399963,-0.01739502,0.035064697,-0.013442993,0.036895752,0.0063934326,-0.012649536,0.0065574646,-0.036895752,0.036102295,0.018981934,-0.0036907196,-0.029251099,0.0002501011,-0.016998291,0.055603027,-0.08276367,0.032165527,-0.052978516,-0.014564514,-0.04534912,-0.029922485,-0.00869751,-0.033599854,0.04296875,-0.041900635,-0.0020179749,-0.0256958,0.0043144226,-0.01739502,0.030181885,0.011398315,-0.03741455,0.011138916,0.06323242,-0.0056991577,-0.027420044,-0.029922485,0.06323242,-0.019241333,0.047180176,0.008239746,0.0018949509,0.032287598,-0.027282715,0.011726379,-0.018585205,0.022537231,0.037963867,0.023986816,-0.023727417,-0.018981934,0.018844604,0.01739502,0.008071899,-0.056152344,-0.006587982,0.04321289,0.052703857,0.0021247864,-0.0054016113,-0.03479004,-0.004383087,0.033203125,0.014038086,0.029129028,0.05984497,-0.03955078,-0.047454834,-0.016403198,-0.005962372,-0.036102295,-0.050079346,-0.0289917,0.0035419464,-0.072753906,-0.013046265,-0.023986816,-0.0287323,-0.008766174,-0.0096206665,-0.00024700165,-0.052978516,-0.029525757,0.018188477,-0.041900635,-0.041656494,-0.02279663,0.058776855,-0.021484375,0.028335571,0.017929077,0.0035247803,-0.0317688,0.017654419,0.0039367676,0.0034599304,-0.013969421,0.014099121,0.04586792,-0.022003174,0.0033111572,0.060638428,0.04058838,-0.030181885,-0.021347046,-0.0158844,-0.030044556,0.027420044,-0.004219055,-0.016998291,-0.018585205,-0.03201294,-0.023849487,0.026489258,-0.010017395,0.012588501,-0.017929077,0.0023231506,0.07116699,-0.010612488,0.1149292,-0.00069618225,-0.06036377,-0.02293396,0.047454834,-0.011199951,0.0019273758,-0.015090942,0.023986816,0.0022411346,-0.0016222,-0.039794922,0.0037727356,0.046905518,-0.019638062,0.03451538,-0.007904053,0.052459717,0.024383545,0.002866745,0.0019760132,0.024642944,-0.039001465,-0.0063934326,-0.041656494,-0.039001465,0.014694214,0.009292603,0.041381836,-0.03665161,-0.029922485,-0.07800293,-0.0289917,-0.0025367737,0.07800293,0.05404663,0.033599854,0.014892578,-0.00047373772,0.023590088,-0.009162903,0.019897461,0.008003235,0.05166626,0.01739502,0.019638062,-0.018844604,0.053771973,-0.0039863586,0.030319214,-0.010147095,0.039794922,0.020553589,0.006855011,0.017791748,0.028198242,0.04321289,-0.032806396,0.018325806,0.018325806,0.0053367615,-0.026626587,0.03201294,-0.013969421,0.019104004,0.044006348,-0.02305603,-0.04031372,0.020950317,0.033996582,-0.06903076,-0.005962372,0.009552002,-0.015289307,0.026351929,-0.043762207,-0.03768921,0.04269409,-0.012390137,-0.047454834,0.033355713,0.015289307,-0.029251099,-0.044555664,-0.047180176,-0.0038375854,-0.011726379,0.011993408,0.01713562,-0.026489258,0.036376953,0.0059318542,0.044555664,-0.000790596,-0.042175293,0.03665161,-0.01977539,0.0038719177,0.02609253,0.0034751892,0.007774353,-0.067993164,0.013839722,0.013900757,0.011528015,-0.021484375,0.017532349,-0.008766174,-0.0748291,-0.04824829,0.052703857,0.033355713,0.044006348,-0.016540527,-0.05984497,0.044006348,-0.027542114,-0.05508423,-0.039794922,0.015617371,-0.022003174,0.03189087,-0.012252808,0.004283905,-0.0287323,0.023452759,-0.047180176,-0.00869751,-0.044006348,0.06378174,0.031234741,-0.041656494,-0.02029419,0.032958984,0.025039673,0.0284729,0.003080368,-0.020690918,-0.024124146,-0.047973633,-0.03375244,-0.019638062,-0.02557373,-0.024246216,0.013374329,0.0256958,0.005039215,0.0031795502,-0.0088272095,-0.013900757,0.019378662,0.056152344,-0.02319336,0.058776855,-0.04611206,-0.062469482,-0.0055351257,0.0067863464,-0.01713562,-0.0016965866,-0.023330688,-0.013710022,0.014823914,-0.04928589,0.007446289,-0.007774353,-0.005897522,-0.055603027,-0.03427124,0.03137207,0.0061950684,0.013969421,-0.0071487427,-3.4987926e-05,-0.02609253,0.023727417,0.005634308,-0.022140503,0.01133728,0.019897461,-0.00097608566,0.0084991455,0.05508423,-0.033996582,-0.057739258,-0.06561279,0.020828247,-0.008628845,0.011665344,-0.02003479,-0.025299072,-0.03741455,0.015090942,0.03201294,0.009750366,-0.0034275055,-0.0287323,0.016479492,-0.03741455,-0.019104004,0.031097412,-0.027023315,-0.00051498413,0.00086069107,-0.0015077591,0.010673523,-0.01146698,0.005962372,-0.023590088,0.010215759,-0.0118637085,0.026748657,-0.016677856,-0.0317688,-0.033996582,-0.026351929,0.009750366,-0.0060310364,-0.012123108,0.02293396,0.0045814514,0.0040016174,0.03189087,-0.033477783,0.026489258,-0.031097412,0.039794922,-0.02293396,-0.04559326,0.009094238,-0.01713562,0.03955078,0.033355713,0.013710022,-0.0076446533,0.0317688,0.0016307831,-0.035583496,-0.027420044,-0.052459717,0.04876709,0.03189087,0.06323242,-0.02319336,0.017532349,0.04058838,0.012390137,0.03161621,0.054840088,0.032409668,-0.016998291,-0.12597656,-0.0256958,-0.00566864,-0.07537842,-0.009689331,0.03768921,0.061157227,-0.06958008,-0.012321472,-0.007118225,-0.006324768,0.010017395,0.027023315,0.027023315,0.04824829,0.0037403107,-0.02029419,-0.0049095154,-0.04348755,-0.008171082,0.04559326,0.0519104,-0.009353638,0.006755829,0.013900757,0.0005478859,0.0035743713,0.01977539,0.04534912,0.016403198,0.016479492,0.033203125,-0.0033931732,0.000749588,0.0049095154,0.028076172,0.023452759,0.053497314,-0.0017547607,-0.03717041,0.013900757,-0.024505615,0.0077438354,0.022399902,0.0003707409,-0.033996582,-0.03741455,0.02609253,-0.050598145,0.030319214,-0.0017137527,-0.032287598,-0.014823914,0.0569458,-0.013175964,-0.041656494,0.029647827,-0.029922485,-0.006755829,0.011001587,-0.033081055,0.04849243,-0.029251099,-0.07696533,-0.016998291,0.04638672,0.013900757,-0.030319214,0.01423645,-0.0024375916,-0.005138397,-0.02279663,-0.03161621,-0.015350342,0.02029419,-0.047973633,-0.0107421875,-0.006259918,-0.018585205,0.004283905,0.000374794,-0.079589844,0.005832672,-0.03665161,0.066711426,0.0029659271,-0.013313293,0.0635376,0.04638672,0.0513916,-0.010414124,-0.024124146,-0.015617371,-0.0046463013,0.02015686,0.022537231,-0.047698975,-0.005897522,0.010673523,-0.03955078,0.0013093948,0.030181885,-0.033996582,-0.023330688,0.016204834,-0.0005311966,0.00289917,0.035308838,-0.0033607483,-0.030044556,-0.01423645,-0.02319336,0.03189087,0.012062073,0.019104004,-0.044555664,-0.014823914,0.004447937,-0.0317688,0.006755829,-0.07171631,-0.0016717911,0.059570312,-0.013839722,-0.00015032291,0.013771057,-0.0051727295,-0.0032615662,-0.021224976,0.024780273,0.007709503,-0.004711151,-0.021484375,-0.021224976,-0.008628845,-0.05984497,-0.0013999939,-0.03189087,-0.004875183,0.011268616,-0.0569458,0.03427124,-0.015945435,0.02279663,0.009819031,0.013442993,0.003162384,-0.020431519,-0.01687622,0.03479004,0.0079422,0.0023880005,-0.008140564,0.026885986,0.014762878,-0.014099121,0.028869629,0.026229858,-0.023330688,-0.050598145,0.004119873,-0.056396484,0.053497314,-0.0023384094,0.055358887,-0.007873535,0.0049743652,-0.07910156,0.07220459,-0.020431519,-0.033355713,0.033203125,-0.006324768,-0.050872803,-0.095947266,-0.047180176,0.00869751,-0.02029419,0.027679443,0.03149414,0.019897461,-0.015289307,0.027938843,-0.004776001,0.0005354881,-0.00050640106,0.009750366,0.062194824,0.0040016174,-0.008399963,-0.024383545,0.047180176,-0.0021247864,0.016601562,-0.013313293,0.013175964,0.03479004,-0.007347107,-0.062469482,-0.003063202,0.0016145706,-0.047698975,-0.030578613,-0.019897461,-0.007446289,-0.023727417,-0.0088272095,0.0074806213,-0.033996582,-0.07330322,-0.016204834,-0.015090942,-0.039794922,0.017654419,-0.0012025833,0.0012435913,-0.038757324,0.0033111572,0.026489258,-0.0022735596,-0.02319336,-0.010215759,-0.008270264,0.019500732,-0.028076172,-0.070617676,0.038208008,-0.029388428,-0.010612488,-0.050872803,0.0088272095,-0.0031471252,-0.0035915375,-0.028076172,-0.023452759,-0.016799927,-0.0041007996,0.0003232956,-0.016799927,0.053771973,-0.022537231,-0.03768921,0.0256958,-0.01574707,-0.046905518,-0.0018119812,0.02293396,-0.032806396,-0.0065231323,0.03768921,-0.023849487,0.037963867,-0.0020923615,0.019638062,0.0018205643,-0.029251099,0.033599854,0.011993408,0.043762207,0.01713562,0.005207062,-0.0022239685,-0.053771973,0.032958984,-0.028869629,-0.06616211,-0.014038086,0.04824829,0.015487671,-0.01977539,0.054840088,0.038208008,0.064819336,0.001367569,-0.027282715,-0.04244995,0.004676819,0.014564514,0.016799927,-0.059570312,0.041107178,0.035583496,0.008628845,-0.0025691986,0.0256958,-0.009422302,0.04638672,0.03717041,0.047698975,-0.047973633,0.0048103333,0.06378174,-0.0084991455,-0.0259552,-0.0066223145,0.009887695,0.043762207,-0.062194824,-0.0016059875,0.008766174,0.04638672,-0.0063934326,-0.01574707,-0.022399902,0.049804688,0.005897522,-0.026626587,-0.0019187927,-0.010612488,0.0026187897,0.0037403107,0.019104004,0.0099487305,0.06008911,-0.007610321,-0.0013017654,0.04611206,0.018844604,0.0118637085,0.0027503967,-0.017532349,0.0021915436,-0.07537842,0.038482666,0.011993408,0.00856781,-0.011398315,-0.04534912,-0.015617371,-0.013114929,-0.0040016174,0.0128479,0.004776001,-0.006259918,-0.02305603,0.024383545,0.037963867,0.00047564507,-0.0044822693,-0.021484375,-0.0066871643,-0.058502197,-0.0019521713,0.00021314621,-0.01977539,0.027023315,0.030700684,-0.008628845,-0.032165527,-0.035583496,0.0013093948,0.03479004,-0.033599854,0.026885986,-0.007347107,-0.013641357,0.058502197,-0.073791504,0.066711426,0.0050086975,0.038482666,0.0016059875,0.033081055,0.016677856,-0.0013837814,0.0020599365,0.02293396,0.03717041,0.058776855,0.018707275,0.004348755,-0.04534912,0.02319336,0.004711151,-0.0635376,-0.024505615,0.0025367737,0.015090942,0.002866745,-0.026885986,-0.01133728,-0.04876709,0.046905518,0.028076172,-0.024505615,0.018188477,-0.044799805,0.025177002,-0.0024871826,0.035583496,-0.03768921,0.036376953,-0.006259918,0.010871887,-0.010803223,-0.0008687973,0.00012505054,0.020950317,-0.019897461,0.0519104,0.008895874,-0.015686035,-0.0848999,-0.021347046,0.012916565,-0.037963867,0.030975342,-0.018981934,-0.055603027,0.029525757,0.022399902,0.0024871826,0.018844604,0.022003174,0.015159607,0.0046463013,0.0016059875,0.00072050095]", "client_status_at": "2025-10-02T09:52:51.066015Z", "server_status_at": "2025-10-02T09:52:51.066001Z", "created_at": "2025-10-02T08:52:47.828517Z"}, {"id": "44368d42-55ab-43c5-af64-d88f40a36639", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://www.deeplearning.ai/the-batch/issue-305/", "canonical_url": "https://www.deeplearning.ai/the-batch/issue-305/", "title": "FLUX.1 Kontext’s Consistent Characters, Benchmarking Costs Climb, and more...", "source_site": "FLUX.1 Kontext’s Consistent Characters, Benchmarking Costs Climb, and more...", "publication_date": "2025-06-11T00:00:00Z", "favicon_url": "https://www.deeplearning.ai/favicon.ico", "content_markdown": "Dear friends,\n\nThere’s a new breed of GenAI Application Engineers who can build more-powerful applications faster than was possible before, thanks to generative AI. Individuals who can play this role are highly sought-after by businesses, but the job description is still coming into focus. Let me describe their key skills, as well as the sorts of interview questions I use to identify them.\n\nSkilled GenAI Application Engineers meet two primary criteria: (i) They are able to use the new AI building blocks to quickly build powerful applications. (ii) They are able to use AI assistance to carry out rapid engineering, building software systems in dramatically less time than was possible before. In addition, good product/design instincts are a significant bonus.\n\n**AI building blocks.** If you own a lot of copies of only a single type of Lego brick, you might be able to build some basic structures. But if you own many types of bricks, you can combine them rapidly to form complex, functional structures. Software frameworks, SDKs, and other such tools are like that. If all you know is how to call a large language model (LLM) API, that's a great start. But if you have a broad range of building block types — such as prompting techniques, agentic frameworks, evals, guardrails, RAG, voice stack, async programming, data extraction, embeddings/vectorDBs, model fine tuning, graphDB usage with LLMs, agentic browser/computer use, MCP, reasoning models, and so on — then you can create much richer combinations of building blocks.\n\nThe number of powerful AI building blocks continues to grow rapidly. But as open-source contributors and businesses make more building blocks available, staying on top of what is available helps you keep on expanding what you can build. Even though new building blocks are created, many building blocks from 1 to 2 years ago (such as eval techniques or frameworks for using vectorDBs) are still very relevant today.\n\n**AI-assisted coding.** AI-assisted coding tools enable developers to be far more productive, and such tools are advancing rapidly. Github Copilot, first announced in 2021 (and made widely available in 2022), pioneered modern code autocompletion. But shortly after, a new breed of AI-enabled IDEs such as Cursor and Windsurf offered much better code-QA and code generation. As LLMs improved, these AI-assisted coding tools that were built on them improved as well.\n\nNow we have highly agentic coding assistants such as OpenAI’s Codex and Anthropic’s Claude Code (which I really enjoy using and find impressive in its ability to write code, test, and debug autonomously for many iterations). In the hands of skilled engineers — who don’t just “vibe code” but deeply understand AI and software architecture fundamentals and can steer a system toward a thoughtfully selected product goal — these tools make it possible to build software with unmatched speed and efficiency.\n\nI find that AI-assisted coding techniques become obsolete much faster than AI building blocks, and techniques from 1 or 2 years ago are far from today's best practices. Part of the reason for this might be that, while AI builders might use dozens (hundreds?) of different building blocks, they aren’t likely to use dozens of different coding assistance tools at once, and so the forces of Darwinian competition are stronger among tools. Given the massive investments in this space by Anthropic, Google, OpenAI, and other players, I expect the frenetic pace of development to continue, but keeping up with the latest developments in AI-assisted coding tools will pay off, since each generation is much better than the last.\n\n**Bonus: Product skills.** In some companies, engineers are expected to take pixel-perfect drawings of a product, specified in great detail, and write code to implement it. But if a product manager has to specify even the smallest detail, this slows down the team. The shortage of [AI product managers](https://www.deeplearning.ai/the-batch/ai-product-managers-will-be-in-demand/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) exacerbates this problem. I see teams move much faster if GenAI Engineers also have some user empathy as well at basic skill at designing products, so that, given only high-level guidance on what to build (“a user interface that lets users see their profiles and change their passwords”), they can make a lot of decisions themselves and build at least a prototype to iterate from.\n\nWhen interviewing GenAI Application Engineers, I will usually ask about their mastery of AI building blocks and ability to use AI-assisted coding, and sometimes also their product/design instincts. One additional question I've found highly predictive of their skill is, “How do you keep up with the latest developments in AI?” Because AI is evolving so rapidly, someone with good strategies for keeping up — such as reading *The Batch* and taking [short courses](https://www.deeplearning.ai/courses/?courses_date_desc%5BrefinementList%5D%5Bcourse_type%5D%5B0%5D=Short%20Courses&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) 😃, regular hands-on practice building projects, and having a community to talk to — really does stay ahead of the game much better than those who have less-effective strategies (such as if social media were their main source of info about AI, which typically does not provide the depth needed to keep up).\n\nKeep building!\n\nAndrew\n\n## A MESSAGE FROM DEEPLEARNING.AI\n\nThe *Data Analytics Professional Certificate* is fully launched! In the “Data Storytelling” course, you’ll choose the right medium to present your analysis, design effective visuals, and learn techniques for aligning data insights with business goals. You’ll also receive guidance to build your portfolio and land a job in data analysis. [Sign up](https://www.coursera.org/learn/dlai-data-storytelling)\n\n# News\n\n# More Consistent Characters and Styles\n\nSame character, new background, new action. That’s the focus of the latest text-to-image models from Germany’s Black Forest Labs.\n\n**What’s new:** The [FLUX.1 Kontext](https://bfl.ai/announcements/flux-1-kontext?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) family, which comes in versions dubbed max, pro, and dev, is trained to alter images in controlled ways. The company plans to release the weights for FLUX.1 Kontext dev but has not yet specified the licensing terms.\n\n**Input/output:**text, image in; image out**Architecture:**Unspecified text encoders, convolutional neural network image encoder-decoder, transformer. FLUX.1 Kontext dev 12 billion parameters, other parameter counts undisclosed**Features:**Character consistency, local and global alterations**Availability/price:**FLUX.1 Kontext max and FLUX.1 Kontext pro available via[FLUX Playground](https://playground.bfl.ai/image/generate?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp)and various partners, $0.08 per image (FLUX.1 max) and $0.04 per image (FLUX.1 pro) via[Fal](https://blog.fal.ai/flux-kontext-available-on-fal/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp), an image-generation platform.**Undisclosed:**Parameter counts of FLUX.1 Kontext max and FLUX.1 Kontext pro, architecture of text encoders, training data, evaluation protocol, open-weights license\n\n**How it works:** The FLUX.1 Kontext models include encoders that embed input text and/or images, a transformer that processes them, and an image decoder that generates images. The current [technical report](https://cdn.sanity.io/files/gsvmb6gz/production/880b072208997108f87e5d2729d8a8be481310b5.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) doesn’t describe how it trained them for character consistency and image editing.\n\n- The team trained the convolutional neural network encoder-decoder to reproduce images and to fool a discriminator (architecture and training unspecified) into classifying them as real.\n- Having frozen the encoders, they trained the transformer — given a time step, embedding of a text prompt, embedding of a reference image, and noisy image embedding — to remove the noise over a series of steps.\n- They further trained the transformer to encourage it to produce noise-free embeddings that a second discriminator would classify as representing real images. This process, a variant of\n[adversarial diffusion distillation](https://www.deeplearning.ai/the-batch/for-faster-diffusion-think-a-gan/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp), helps reduce the number of steps needed to produce a good image embedding.\n\n**Results:** The team compared the output of FLUX.1 Kontext models with that of five competing models including [OpenAI GPT Image 1](https://openai.com/index/image-generation-api/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) (at three different quality levels) and [Google Gemini 2.0 Flash native image generation](https://developers.googleblog.com/en/experiment-with-gemini-20-flash-native-image-generation?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp). An undisclosed number of people evaluated the models according to a proprietary benchmark that highlights altering local and global aspects of an image, editing generated text within an image, maintaining consistent characters, and generating an image according to a reference style. The dataset included roughly 1,000 crowd-sourced pairs of text prompts and reference images.\n\n- FLUX.1 Kontext max and FLUX.1 Kontext pro outperformed all competing models.\n- FLUX.1 dev outperformed all except other family members and GPT Image 1 set to high or medium quality.\n\n**Behind the news:** Character consistency, also known as personalization, has come a long way since text-to-image generators became popular. In 2022, [Textual Inversion](https://arxiv.org/abs/2208.01618?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) showed how to learn an embedding of a character and use that embedding to produce further images. In 2023, [DreamBooth](https://arxiv.org/abs/2208.12242?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) showed how to get good results by fine-tuning a model on a few images of the character to be portrayed in a new situation. Since then, image-editing models have improved in quality and generality, including [Meta Emu-Edit](https://www.deeplearning.ai/the-batch/metas-emu-edit-improves-text-to-image-generation-with-task-classification/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp), [OmniGen](https://arxiv.org/abs/2409.11340?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp), and OpenAI gpt-image-1.\n\n**Why it matters:** Consistency and precise editing enable artists to craft stories around specific characters. Such models have become better at generating consistent details across images, but they remain finicky, sometimes changing minute details or entire characters and backgrounds. The more faithfully they help users express their ideas, the more firmly embedded in the creative toolkit they’ll become.\n\n**We’re thinking:** Black Forest Labs announced plans to publish its proprietary benchmark. There’s a real need for common benchmarks to evaluate image generation, and we hope other developers will give it due consideration.\n\n# AI Market Trends in Charts and Graphs\n\nRenowned investment analyst Mary Meeker is back with a report on the AI market, six years after publishing her last survey of the internet.\n\n**What’s new:** Meeker, co-founder of the venture capital firm Bond who formerly analyzed technology portfolios for Merrill Lynch, Salomon Brothers, and Morgan Stanley, published “[Trends — Artificial Intelligence (May ‘25)](https://www.bondcap.com/reports/tai?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp).” The report, which spans 340 graph-packed pages, revives and updates a series that chronicled the rise of the internet nearly every year from 1995 through 2019.\n\n**How it works:** The new report focuses on a handful of themes that arise from the unprecedented growth and capabilities of deep learning. As Meeker [told](https://www.axios.com/2025/05/30/mary-meeker-trends-report-openai?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) *Axios*, AI is an arena for “intense competition the likes of which we’ve never seen before,” and that makes the present time “a period for lots of wealth creation and wealth destruction.”\n\n**Rapid growth:**Change in AI is happening faster than ever. Users of ChatGPT reached 1 million in 5 days — compared to the iPhone’s 74 days — and since then have rocketed to 800 million. Total capital expenditures of the six biggest technology companies (largely driven by AI) rose 63 percent to $212 billion between 2023 and 2024. Training datasets are growing 260 percent per year, processing power devoted to training is growing 360 percent per year, effective processing power is growing at 200 percent annually.**Revenues and costs:**The economics of this new world are not straightforward. On one hand, revenue is soaring at giants like Amazon, Google, and Nvidia as well as startups like Scale AI. On the other hand, the cost of computation is rising steadily even as the cost per token of output falls precipitously. Meanwhile, rapid turnover of models and proliferation of open-source alternatives are wild cards for AI-powered businesses.**Rising performance:**AI performance continues to increase. AI’s ability to complete the MMLU benchmark of language understanding outstripped human performance last year. This year, 73 percent of human testers classified responses generated by an LLM as human, according to one study. Synthetic images, video, and speech generation — all are increasingly capable of fooling human testers.**Emerging capabilities:**Today’s AI is capable of writing and editing, tutoring, brainstorming, automating repetitive work, and providing companionship. Within five years, it will generate code as well as humans, create films and games, operate humanlike robots, and drive scientific discovery. Meeker forecasts that within 10 years, AI will conduct scientific research, design advanced technologies, and build immersive digital worlds.**Workforce implications:**Industries most likely to be affected by AI include knowledge work, content creation, legal services, software development, financial services, customer service, drug discovery, and manufacturing. Employers are adopting AI to get a boost in workforce productivity that Stanford researchers estimate is an average 14 percent. Companies like Box, Duolingo, and Shopify are adopting an AI-first orientation, while AI-related job titles have risen 200 percent in the past two years.**AI gets physical:**AI is having a profound impact on the physical world. Lyft’s and Uber’s market share fell around 15 percent while Waymo’s gained 27 percent over the past 18 months. AI-driven mineral exploration is boosting mine efficiency, and AI-powered agriculture is cutting the use of pesticides. And, sadly, AI-equipped attack drones are wreaking destruction upon Ukraine and elsewhere, even as they play a critical role in defense.\n\n**Behind the news:** Meeker published her first “Internet Trends” report in 1995, anticipating the coming online boom, and she issued new editions annually throughout the 2000s and much of the coming decade. Her final internet report arrived in 2019, the year after she founded Bond, when the report highlighted the rise of visual social media like Instagram, wearable technology, and digital payments.\n\n**Why it matters:** “Trends — Artificial Intelligence” offers a wealth of market data culled from analyst reports, consumer surveys, and academic studies. The AI community has a number of excellent annual surveys, including Stanford’s [AI Index](https://hai.stanford.edu/ai-index/2025-ai-index-report?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) and Air Street Capital’s [State of AI](https://www.stateof.ai/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp). Meeker, who has been watching technology markets since the dawning of the web, adds another valuable perspective.\n\n**We’re thinking:** One implication of the report: There has never been a better time to build software applications. For developers, it’s time to hone and update skills. For tech companies, it’s time to cast the net for talent. As Meeker said in her interview with *Axios*, “Companies that get the best developers often win.”\n\n# Benchmarking Costs Climb\n\nAn independent AI test lab detailed the rising cost of benchmarking reasoning models.\n\n**What’s new:** Artificial Analysis, an organization that tracks model performance and cost, revealed its budgets for evaluating a few recent models that improve their output by producing chains of thought, which use extra computation and thus boost the cost of inference. The expense is making it difficult for startups, academic labs, and other organizations that have limited resources to reproduce results reported by model developers, *TechCrunch* [reported](https://techcrunch.com/2025/04/10/the-rise-of-ai-reasoning-models-is-making-benchmarking-more-expensive/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp). (Disclosure: Andrew Ng is an investor in Artificial Analysis.)\n\n**How it works:** Artificial Analysis tested reasoning and non-reasoning models on popular benchmarks that gauge model performance in responding to queries that require specialized knowledge or multi-step reasoning, solving math problems, generating computer programs, and the like.\n\n- Running a group of seven popular benchmarks, OpenAI o1 (which produces chains of thought) produced more than 44 million tokens, while GPT-4o (which doesn’t take explicit reasoning steps) produced around 5.5 million tokens.\n- Benchmarking o1 cost $2,767, while benchmarking Anthropic Claude 3.7 Sonnet (which allows users to allocate a number of reasoning tokens per query;\n*TechCrunch*doesn’t provide the number in this case) cost $1,485. Smaller reasoning models are significantly less expensive: o3-mini (at high effort, which uses the highest number of reasoning tokens per query) cost $345, and o1-mini cost $141. - Non-reasoning models are less expensive to test. Evaluating GPT-4o cost $109, Claude 3.5 Sonnet was $81.\n- Artificial Analysis spent around $5,200 to test 12 reasoning models versus around $2,400 to test more than 80 non-reasoning models.\n\n**Behind the news:** Generally, the cost per token of using AI models has been [falling](https://www.deeplearning.ai/the-batch/ai-price-wars-drive-costs-down-as-competition-heats-up/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) even as their performance has been rising. However, two factors complicate that trend. (i) Reasoning models produce more tokens and thus cost more to run, and (ii) developers are charging higher per-token prices to use their latest models. For example, o1-pro and GPT-4.5 (a non-reasoning model), both released in early 2025, cost $600 per million output tokens, while Claude 3.5 Sonnet (released in July 2024) costs $15 per million tokens of output. Emerging techniques that allow users to allocate numbers of tokens to reasoning (whether “high” or “low” or a specific tally) also make benchmarking more costly and complicated.\n\n**Why it matters:** Benchmarks aren’t entirely sufficient for evaluating models, but they are a critical indicator of relative performance, and independent benchmarking helps to ensure that tests are run in a fair and consistent way. As the cost of benchmarking climbs, fewer labs are likely to confirm or challenge results obtained by the original developer, making it harder to compare models and recognize progress.\n\n**We’re thinking:** Verifying performance claims in independent, open, fair tests is essential to marking progress in general and choosing the right models for particular projects. It's time for the industry to support independent benchmarking organizations.\n\n# Better Video, Fewer Tokens\n\nResearchers reduced the number of tokens needed to represent video frames to be fed to a transformer.\n\n**What’s new:** Jindong Jiang, Xiuyu Li, and collaborators at Nvidia, Rutgers University, UC Berkeley, Massachusetts Institute of Technology, Nanjing University, and Korea Advanced Institute of Science and Technology built [STORM](https://arxiv.org/abs/2503.04130?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp), a text-video system that performs well in tests of video understanding while processing fewer tokens.\n\n**Key insight:** In a multimodal system, a large language model (LLM) that receives video tokens may struggle to process long videos. However, sequences of video frames often contain lots of redundancy, since few pixels may change from one frame to the next. Instead of forcing the LLM to process long sequences of redundant video tokens, [mamba](https://www.deeplearning.ai/the-batch/mamba-a-new-approach-that-may-outperform-transformers/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) layers can enrich the token embeddings that represent one frame with information from other frames in the same clip. That way, the system can average token embeddings across frames without losing crucial information, making it possible to feed fewer tokens to the LLM without compromising performance.\n\n**How it works:** The authors built STORM by training three components: (1) a pretrained [SigLIP](https://arxiv.org/abs/2303.15343?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) vision transformer, (2) untrained mamba layers, and (3) the pretrained large language model (LLM) from [Qwen2-VL](https://arxiv.org/abs/2409.12191?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp). They trained the system to predict the next token in [image](https://arxiv.org/abs/2307.04087?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp)-[text](https://arxiv.org/abs/2405.02246?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) [pairs](https://arxiv.org/abs/2406.16860?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) and video-text pairs with [32-frame](https://arxiv.org/abs/2404.01258?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) [videos](https://arxiv.org/abs/2310.01852?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp), and video-text pairs with [128-frame videos](https://arxiv.org/abs/2410.02713?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp).\n\n- SigLIP learned to turn each video frame into 256 image tokens.\n- Given a sequence of image tokens, mamba layers learned to process them in both directions – left-to-right and right-to-left – so each output token embedding encoded information from the entire video.\n- The system averaged the token embeddings of 4 consecutive frames, reducing by a factor of 4 the number of tokens processed by Qwen2-VL’s LLM.\n- Given the averaged token embeddings, Qwen2-VL LLM learned to predict the next word in the video’s associated text.\n- At inference, the system fed to the LLM the tokens that represented every second frame (a process the authors call temporal sampling), which further halved the input to the LLM.\n\n**Results:** STORM outperformed proprietary and open models on measures of video understanding.\n\n- On\n[MVBench](https://arxiv.org/abs/2311.17005?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp), which asks multiple-choice questions about actions, object interactions, and scene transitions in 16-second videos, STORM achieved 70.6 percent accuracy. That’s better than[GPT-4o](https://openai.com/index/hello-gpt-4o/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp)(64.6 percent accuracy) and Qwen2-VL (67.0 percent accuracy). A baseline system (STORM’s SigLIP and Qwen2-VL LLM without mamba layers, averaging image tokens, and temporal sampling) achieved 69.5 percent. - On\n[MLVU](https://arxiv.org/abs/2406.04264?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp), which asks multiple-choice and open-ended questions about videos that range from 3 minutes to over 2 hours long, STORM reached 72.9 percent accuracy, topping GPT-4o (66.2 percent accuracy). The baseline model achieved 70.2 percent.\n\n**Why it matters:** STORM compresses video at the input to the LLM, so the LLM processes 1/8 as many video tokens and uses 1/8 as much compute to process them. This enables the system to work more than 3 times faster than the baseline while performing better.\n\n**We’re thinking:** Initial work on the mamba architecture positioned it as a replacement for the transformer, but this work, along with [other](https://www.deeplearning.ai/the-batch/ai21-labs-jamba-1-5-outpaces-transformers-in-long-text-processing/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp) [projects](https://arxiv.org/abs/2504.03624?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--L4fNn7TgZ4dfnbjIlq6pRGMNR7s8kwocyGVP0aqBk3eqniHH_Q-Z8_RqY-F-MDDLHgXIp), combines them to get the benefits of both.\n\n## A MESSAGE FROM DEEPLEARNING.AI\n\nIn “Orchestrating Workflows for GenAI Applications” you’ll learn to orchestrate generative AI workflows using Apache Airflow 3.0. You’ll build and schedule RAG pipelines, run tasks in parallel, and add retries and alerts for reliability. No prior Airflow experience is needed! [Enroll for free](https://www.deeplearning.ai/short-courses/orchestrating-workflows-for-genai-applications/)", "content_text": "Dear friends,\nThere’s a new breed of GenAI Application Engineers who can build more-powerful applications faster than was possible before, thanks to generative AI. Individuals who can play this role are highly sought-after by businesses, but the job description is still coming into focus. Let me describe their key skills, as well as the sorts of interview questions I use to identify them.\nSkilled GenAI Application Engineers meet two primary criteria: (i) They are able to use the new AI building blocks to quickly build powerful applications. (ii) They are able to use AI assistance to carry out rapid engineering, building software systems in dramatically less time than was possible before. In addition, good product/design instincts are a significant bonus.\nAI building blocks. If you own a lot of copies of only a single type of Lego brick, you might be able to build some basic structures. But if you own many types of bricks, you can combine them rapidly to form complex, functional structures. Software frameworks, SDKs, and other such tools are like that. If all you know is how to call a large language model (LLM) API, that's a great start. But if you have a broad range of building block types — such as prompting techniques, agentic frameworks, evals, guardrails, RAG, voice stack, async programming, data extraction, embeddings/vectorDBs, model fine tuning, graphDB usage with LLMs, agentic browser/computer use, MCP, reasoning models, and so on — then you can create much richer combinations of building blocks.\nThe number of powerful AI building blocks continues to grow rapidly. But as open-source contributors and businesses make more building blocks available, staying on top of what is available helps you keep on expanding what you can build. Even though new building blocks are created, many building blocks from 1 to 2 years ago (such as eval techniques or frameworks for using vectorDBs) are still very relevant today.\nAI-assisted coding. AI-assisted coding tools enable developers to be far more productive, and such tools are advancing rapidly. Github Copilot, first announced in 2021 (and made widely available in 2022), pioneered modern code autocompletion. But shortly after, a new breed of AI-enabled IDEs such as Cursor and Windsurf offered much better code-QA and code generation. As LLMs improved, these AI-assisted coding tools that were built on them improved as well.\nNow we have highly agentic coding assistants such as OpenAI’s Codex and Anthropic’s Claude Code (which I really enjoy using and find impressive in its ability to write code, test, and debug autonomously for many iterations). In the hands of skilled engineers — who don’t just “vibe code” but deeply understand AI and software architecture fundamentals and can steer a system toward a thoughtfully selected product goal — these tools make it possible to build software with unmatched speed and efficiency.\nI find that AI-assisted coding techniques become obsolete much faster than AI building blocks, and techniques from 1 or 2 years ago are far from today's best practices. Part of the reason for this might be that, while AI builders might use dozens (hundreds?) of different building blocks, they aren’t likely to use dozens of different coding assistance tools at once, and so the forces of Darwinian competition are stronger among tools. Given the massive investments in this space by Anthropic, Google, OpenAI, and other players, I expect the frenetic pace of development to continue, but keeping up with the latest developments in AI-assisted coding tools will pay off, since each generation is much better than the last.\nBonus: Product skills. In some companies, engineers are expected to take pixel-perfect drawings of a product, specified in great detail, and write code to implement it. But if a product manager has to specify even the smallest detail, this slows down the team. The shortage of AI product managers exacerbates this problem. I see teams move much faster if GenAI Engineers also have some user empathy as well at basic skill at designing products, so that, given only high-level guidance on what to build (“a user interface that lets users see their profiles and change their passwords”), they can make a lot of decisions themselves and build at least a prototype to iterate from.\nWhen interviewing GenAI Application Engineers, I will usually ask about their mastery of AI building blocks and ability to use AI-assisted coding, and sometimes also their product/design instincts. One additional question I've found highly predictive of their skill is, “How do you keep up with the latest developments in AI?” Because AI is evolving so rapidly, someone with good strategies for keeping up — such as reading The Batch and taking short courses 😃, regular hands-on practice building projects, and having a community to talk to — really does stay ahead of the game much better than those who have less-effective strategies (such as if social media were their main source of info about AI, which typically does not provide the depth needed to keep up).\nKeep building!\nAndrew\nA MESSAGE FROM DEEPLEARNING.AI\nThe Data Analytics Professional Certificate is fully launched! In the “Data Storytelling” course, you’ll choose the right medium to present your analysis, design effective visuals, and learn techniques for aligning data insights with business goals. You’ll also receive guidance to build your portfolio and land a job in data analysis. Sign up\nNews\nMore Consistent Characters and Styles\nSame character, new background, new action. That’s the focus of the latest text-to-image models from Germany’s Black Forest Labs.\nWhat’s new: The FLUX.1 Kontext family, which comes in versions dubbed max, pro, and dev, is trained to alter images in controlled ways. The company plans to release the weights for FLUX.1 Kontext dev but has not yet specified the licensing terms.\n- Input/output: text, image in; image out\n- Architecture: Unspecified text encoders, convolutional neural network image encoder-decoder, transformer. FLUX.1 Kontext dev 12 billion parameters, other parameter counts undisclosed\n- Features: Character consistency, local and global alterations\n- Availability/price: FLUX.1 Kontext max and FLUX.1 Kontext pro available via FLUX Playground and various partners, $0.08 per image (FLUX.1 max) and $0.04 per image (FLUX.1 pro) via Fal, an image-generation platform.\n- Undisclosed: Parameter counts of FLUX.1 Kontext max and FLUX.1 Kontext pro, architecture of text encoders, training data, evaluation protocol, open-weights license\nHow it works: The FLUX.1 Kontext models include encoders that embed input text and/or images, a transformer that processes them, and an image decoder that generates images. The current technical report doesn’t describe how it trained them for character consistency and image editing.\n- The team trained the convolutional neural network encoder-decoder to reproduce images and to fool a discriminator (architecture and training unspecified) into classifying them as real.\n- Having frozen the encoders, they trained the transformer — given a time step, embedding of a text prompt, embedding of a reference image, and noisy image embedding — to remove the noise over a series of steps.\n- They further trained the transformer to encourage it to produce noise-free embeddings that a second discriminator would classify as representing real images. This process, a variant of adversarial diffusion distillation, helps reduce the number of steps needed to produce a good image embedding.\nResults: The team compared the output of FLUX.1 Kontext models with that of five competing models including OpenAI GPT Image 1 (at three different quality levels) and Google Gemini 2.0 Flash native image generation. An undisclosed number of people evaluated the models according to a proprietary benchmark that highlights altering local and global aspects of an image, editing generated text within an image, maintaining consistent characters, and generating an image according to a reference style. The dataset included roughly 1,000 crowd-sourced pairs of text prompts and reference images.\n- FLUX.1 Kontext max and FLUX.1 Kontext pro outperformed all competing models.\n- FLUX.1 dev outperformed all except other family members and GPT Image 1 set to high or medium quality.\nBehind the news: Character consistency, also known as personalization, has come a long way since text-to-image generators became popular. In 2022, Textual Inversion showed how to learn an embedding of a character and use that embedding to produce further images. In 2023, DreamBooth showed how to get good results by fine-tuning a model on a few images of the character to be portrayed in a new situation. Since then, image-editing models have improved in quality and generality, including Meta Emu-Edit, OmniGen, and OpenAI gpt-image-1.\nWhy it matters: Consistency and precise editing enable artists to craft stories around specific characters. Such models have become better at generating consistent details across images, but they remain finicky, sometimes changing minute details or entire characters and backgrounds. The more faithfully they help users express their ideas, the more firmly embedded in the creative toolkit they’ll become.\nWe’re thinking: Black Forest Labs announced plans to publish its proprietary benchmark. There’s a real need for common benchmarks to evaluate image generation, and we hope other developers will give it due consideration.\nAI Market Trends in Charts and Graphs\nRenowned investment analyst Mary Meeker is back with a report on the AI market, six years after publishing her last survey of the internet.\nWhat’s new: Meeker, co-founder of the venture capital firm Bond who formerly analyzed technology portfolios for Merrill Lynch, Salomon Brothers, and Morgan Stanley, published “Trends — Artificial Intelligence (May ‘25).” The report, which spans 340 graph-packed pages, revives and updates a series that chronicled the rise of the internet nearly every year from 1995 through 2019.\nHow it works: The new report focuses on a handful of themes that arise from the unprecedented growth and capabilities of deep learning. As Meeker told Axios, AI is an arena for “intense competition the likes of which we’ve never seen before,” and that makes the present time “a period for lots of wealth creation and wealth destruction.”\n- Rapid growth: Change in AI is happening faster than ever. Users of ChatGPT reached 1 million in 5 days — compared to the iPhone’s 74 days — and since then have rocketed to 800 million. Total capital expenditures of the six biggest technology companies (largely driven by AI) rose 63 percent to $212 billion between 2023 and 2024. Training datasets are growing 260 percent per year, processing power devoted to training is growing 360 percent per year, effective processing power is growing at 200 percent annually.\n- Revenues and costs: The economics of this new world are not straightforward. On one hand, revenue is soaring at giants like Amazon, Google, and Nvidia as well as startups like Scale AI. On the other hand, the cost of computation is rising steadily even as the cost per token of output falls precipitously. Meanwhile, rapid turnover of models and proliferation of open-source alternatives are wild cards for AI-powered businesses.\n- Rising performance: AI performance continues to increase. AI’s ability to complete the MMLU benchmark of language understanding outstripped human performance last year. This year, 73 percent of human testers classified responses generated by an LLM as human, according to one study. Synthetic images, video, and speech generation — all are increasingly capable of fooling human testers.\n- Emerging capabilities: Today’s AI is capable of writing and editing, tutoring, brainstorming, automating repetitive work, and providing companionship. Within five years, it will generate code as well as humans, create films and games, operate humanlike robots, and drive scientific discovery. Meeker forecasts that within 10 years, AI will conduct scientific research, design advanced technologies, and build immersive digital worlds.\n- Workforce implications: Industries most likely to be affected by AI include knowledge work, content creation, legal services, software development, financial services, customer service, drug discovery, and manufacturing. Employers are adopting AI to get a boost in workforce productivity that Stanford researchers estimate is an average 14 percent. Companies like Box, Duolingo, and Shopify are adopting an AI-first orientation, while AI-related job titles have risen 200 percent in the past two years.\n- AI gets physical: AI is having a profound impact on the physical world. Lyft’s and Uber’s market share fell around 15 percent while Waymo’s gained 27 percent over the past 18 months. AI-driven mineral exploration is boosting mine efficiency, and AI-powered agriculture is cutting the use of pesticides. And, sadly, AI-equipped attack drones are wreaking destruction upon Ukraine and elsewhere, even as they play a critical role in defense.\nBehind the news: Meeker published her first “Internet Trends” report in 1995, anticipating the coming online boom, and she issued new editions annually throughout the 2000s and much of the coming decade. Her final internet report arrived in 2019, the year after she founded Bond, when the report highlighted the rise of visual social media like Instagram, wearable technology, and digital payments.\nWhy it matters: “Trends — Artificial Intelligence” offers a wealth of market data culled from analyst reports, consumer surveys, and academic studies. The AI community has a number of excellent annual surveys, including Stanford’s AI Index and Air Street Capital’s State of AI. Meeker, who has been watching technology markets since the dawning of the web, adds another valuable perspective.\nWe’re thinking: One implication of the report: There has never been a better time to build software applications. For developers, it’s time to hone and update skills. For tech companies, it’s time to cast the net for talent. As Meeker said in her interview with Axios, “Companies that get the best developers often win.”\nBenchmarking Costs Climb\nAn independent AI test lab detailed the rising cost of benchmarking reasoning models.\nWhat’s new: Artificial Analysis, an organization that tracks model performance and cost, revealed its budgets for evaluating a few recent models that improve their output by producing chains of thought, which use extra computation and thus boost the cost of inference. The expense is making it difficult for startups, academic labs, and other organizations that have limited resources to reproduce results reported by model developers, TechCrunch reported. (Disclosure: Andrew Ng is an investor in Artificial Analysis.)\nHow it works: Artificial Analysis tested reasoning and non-reasoning models on popular benchmarks that gauge model performance in responding to queries that require specialized knowledge or multi-step reasoning, solving math problems, generating computer programs, and the like.\n- Running a group of seven popular benchmarks, OpenAI o1 (which produces chains of thought) produced more than 44 million tokens, while GPT-4o (which doesn’t take explicit reasoning steps) produced around 5.5 million tokens.\n- Benchmarking o1 cost $2,767, while benchmarking Anthropic Claude 3.7 Sonnet (which allows users to allocate a number of reasoning tokens per query; TechCrunch doesn’t provide the number in this case) cost $1,485. Smaller reasoning models are significantly less expensive: o3-mini (at high effort, which uses the highest number of reasoning tokens per query) cost $345, and o1-mini cost $141.\n- Non-reasoning models are less expensive to test. Evaluating GPT-4o cost $109, Claude 3.5 Sonnet was $81.\n- Artificial Analysis spent around $5,200 to test 12 reasoning models versus around $2,400 to test more than 80 non-reasoning models.\nBehind the news: Generally, the cost per token of using AI models has been falling even as their performance has been rising. However, two factors complicate that trend. (i) Reasoning models produce more tokens and thus cost more to run, and (ii) developers are charging higher per-token prices to use their latest models. For example, o1-pro and GPT-4.5 (a non-reasoning model), both released in early 2025, cost $600 per million output tokens, while Claude 3.5 Sonnet (released in July 2024) costs $15 per million tokens of output. Emerging techniques that allow users to allocate numbers of tokens to reasoning (whether “high” or “low” or a specific tally) also make benchmarking more costly and complicated.\nWhy it matters: Benchmarks aren’t entirely sufficient for evaluating models, but they are a critical indicator of relative performance, and independent benchmarking helps to ensure that tests are run in a fair and consistent way. As the cost of benchmarking climbs, fewer labs are likely to confirm or challenge results obtained by the original developer, making it harder to compare models and recognize progress.\nWe’re thinking: Verifying performance claims in independent, open, fair tests is essential to marking progress in general and choosing the right models for particular projects. It's time for the industry to support independent benchmarking organizations.\nBetter Video, Fewer Tokens\nResearchers reduced the number of tokens needed to represent video frames to be fed to a transformer.\nWhat’s new: Jindong Jiang, Xiuyu Li, and collaborators at Nvidia, Rutgers University, UC Berkeley, Massachusetts Institute of Technology, Nanjing University, and Korea Advanced Institute of Science and Technology built STORM, a text-video system that performs well in tests of video understanding while processing fewer tokens.\nKey insight: In a multimodal system, a large language model (LLM) that receives video tokens may struggle to process long videos. However, sequences of video frames often contain lots of redundancy, since few pixels may change from one frame to the next. Instead of forcing the LLM to process long sequences of redundant video tokens, mamba layers can enrich the token embeddings that represent one frame with information from other frames in the same clip. That way, the system can average token embeddings across frames without losing crucial information, making it possible to feed fewer tokens to the LLM without compromising performance.\nHow it works: The authors built STORM by training three components: (1) a pretrained SigLIP vision transformer, (2) untrained mamba layers, and (3) the pretrained large language model (LLM) from Qwen2-VL. They trained the system to predict the next token in image-text pairs and video-text pairs with 32-frame videos, and video-text pairs with 128-frame videos.\n- SigLIP learned to turn each video frame into 256 image tokens.\n- Given a sequence of image tokens, mamba layers learned to process them in both directions – left-to-right and right-to-left – so each output token embedding encoded information from the entire video.\n- The system averaged the token embeddings of 4 consecutive frames, reducing by a factor of 4 the number of tokens processed by Qwen2-VL’s LLM.\n- Given the averaged token embeddings, Qwen2-VL LLM learned to predict the next word in the video’s associated text.\n- At inference, the system fed to the LLM the tokens that represented every second frame (a process the authors call temporal sampling), which further halved the input to the LLM.\nResults: STORM outperformed proprietary and open models on measures of video understanding.\n- On MVBench, which asks multiple-choice questions about actions, object interactions, and scene transitions in 16-second videos, STORM achieved 70.6 percent accuracy. That’s better than GPT-4o (64.6 percent accuracy) and Qwen2-VL (67.0 percent accuracy). A baseline system (STORM’s SigLIP and Qwen2-VL LLM without mamba layers, averaging image tokens, and temporal sampling) achieved 69.5 percent.\n- On MLVU, which asks multiple-choice and open-ended questions about videos that range from 3 minutes to over 2 hours long, STORM reached 72.9 percent accuracy, topping GPT-4o (66.2 percent accuracy). The baseline model achieved 70.2 percent.\nWhy it matters: STORM compresses video at the input to the LLM, so the LLM processes 1/8 as many video tokens and uses 1/8 as much compute to process them. This enables the system to work more than 3 times faster than the baseline while performing better.\nWe’re thinking: Initial work on the mamba architecture positioned it as a replacement for the transformer, but this work, along with other projects, combines them to get the benefits of both.\nA MESSAGE FROM DEEPLEARNING.AI\nIn “Orchestrating Workflows for GenAI Applications” you’ll learn to orchestrate generative AI workflows using Apache Airflow 3.0. You’ll build and schedule RAG pipelines, run tasks in parallel, and add retries and alerts for reliability. No prior Airflow experience is needed! Enroll for free", "content_token_count": 4348, "client_status": "queued", "server_status": "embedded", "summary": "The article discusses advancements in generative AI, highlighting the FLUX.1 Kontext models for consistent character generation and image editing. It also covers the rising costs of benchmarking reasoning models and introduces STORM, a system that reduces the number of tokens needed for video processing. Additionally, it mentions the high demand for GenAI Application Engineers and the economic implications of AI development.", "expiry_score": 0.6, "mistral_embedding": "[-0.015731812,0.04888916,0.012008667,0.019378662,0.039611816,0.018814087,0.03201294,-0.00920105,-0.006954193,-0.010040283,-0.054504395,0.07305908,-0.049713135,0.019241333,-0.04296875,0.025848389,0.021209717,0.04043579,0.049713135,-0.0074806213,-0.0256958,-0.03062439,-0.06463623,0.021774292,0.004283905,-0.012145996,-0.00983429,-0.08428955,-0.050567627,0.018112183,-0.0042495728,-0.0345459,-0.0036697388,0.030761719,0.006389618,-0.008148193,-0.012641907,-0.04916382,0.012710571,-0.0146102905,0.008430481,0.0036525726,0.0019397736,-0.004634857,-0.03302002,-0.003019333,0.0024051666,-0.006286621,-0.014961243,-0.0009832382,-0.018539429,0.039611816,-0.006706238,0.004142761,-0.006214142,0.027816772,0.039031982,-0.026397705,-0.028656006,0.025558472,-0.029769897,0.003353119,-0.003124237,-0.017410278,0.008178711,0.015655518,-0.0010318756,0.008529663,0.009407043,-0.027816772,0.037078857,0.017349243,-0.017623901,0.03286743,-0.025848389,-0.061798096,0.0029678345,0.0128479,0.034698486,0.0015621185,-0.030334473,0.023590088,0.07080078,-0.029632568,-0.034118652,0.009056091,-0.009552002,-0.0015888214,-0.005268097,0.013343811,0.03933716,0.041290283,0.02331543,-0.0184021,0.062347412,0.039031982,-0.015235901,0.0058288574,-0.009269714,0.047180176,0.03933716,-0.005722046,0.080322266,-0.0039520264,0.029632568,-0.040161133,0.038482666,-0.012145996,-0.011238098,-0.062072754,-0.08087158,-0.018966675,-0.009269714,-0.06488037,-0.01586914,0.008392334,-0.033843994,0.04437256,-0.05618286,-0.06573486,0.034820557,0.03793335,0.0015010834,0.021636963,-0.030899048,-0.044952393,0.007583618,0.051116943,-0.011871338,-0.030197144,-0.0029144287,-0.0019226074,-0.0017824173,-0.039031982,0.009056091,-0.03173828,0.03427124,-0.036224365,-0.013908386,-0.0008120537,0.031311035,-0.022750854,-0.022750854,0.0051612854,0.0053367615,0.0042304993,0.00033807755,0.02444458,-0.015449524,-0.0054092407,-0.030471802,0.027954102,0.076416016,-0.01727295,-0.021072388,-0.0027561188,-0.040740967,0.011444092,0.05645752,-0.008285522,-0.031463623,0.0007200241,-0.03793335,0.01776123,0.0004940033,-0.018966675,-0.01713562,-0.00085163116,0.0008778572,-0.006778717,0.060668945,-0.06323242,0.026260376,0.05392456,-0.009552002,0.03540039,0.010887146,-0.0096206665,0.0513916,0.033843994,-0.016220093,-0.017562866,0.016860962,0.020370483,0.0054092407,-0.01411438,-0.04156494,0.017410278,-0.0071640015,0.012496948,-0.052246094,-0.00674057,-0.04748535,0.011169434,-0.02444458,0.03314209,0.01826477,-0.05029297,0.030761719,0.0054092407,0.04043579,-0.0014839172,0.005619049,0.048309326,0.0020198822,0.035949707,0.0014839172,0.0031604767,0.01348114,-0.0015974045,-0.024719238,0.010391235,-0.017486572,-0.023452759,-0.014259338,-0.018966675,0.016860962,-0.0256958,0.0007200241,-0.032165527,-0.016860962,-0.030197144,0.014045715,-0.005970001,0.021636963,0.04550171,0.030761719,0.0069885254,-0.028793335,0.0047035217,0.023880005,0.014961243,-0.01158905,-0.025146484,-0.0075149536,-0.005126953,0.033294678,0.027389526,0.0061454773,0.005268097,-0.012779236,0.03302002,-0.030761719,0.0345459,0.028793335,-0.021347046,-0.01776123,-0.013763428,0.011520386,0.046905518,-0.07867432,-0.012573242,-0.051971436,0.0010538101,-0.07696533,-0.014884949,-0.022613525,-0.034973145,0.049438477,-0.061523438,-0.020645142,0.019104004,-0.0155181885,0.005584717,0.0256958,0.026687622,-0.06964111,0.007232666,0.02809143,0.01713562,-0.005760193,-0.031463623,0.04269409,-0.030761719,0.06011963,0.005126953,-0.0012731552,0.019943237,-0.014328003,0.0061798096,-0.013763428,0.0024585724,0.021209717,0.058441162,-0.034820557,-0.012428284,0.0040740967,-0.010673523,-0.003036499,-0.039611816,0.0011940002,0.04663086,0.039031982,-0.00554657,-0.01826477,-0.04888916,0.004951477,0.031463623,0.017623901,0.028366089,0.062072754,-0.04296875,-0.021484375,0.008811951,0.027816772,-0.032165527,-0.06402588,-0.030471802,0.010040283,-0.050842285,-0.0025978088,-0.0036334991,-0.010955811,-0.008110046,-0.0007548332,-0.003107071,-0.060394287,-0.025558472,0.0063552856,-0.0032653809,-0.037353516,-0.014884949,0.066589355,-0.025848389,0.021636963,0.017562866,-0.0064964294,-0.027252197,0.008323669,-0.0060043335,0.03201294,-0.00920105,0.017410278,0.03302002,0.030197144,0.016082764,0.062347412,0.03793335,-0.052246094,0.0019226074,-0.01474762,-0.01474762,0.024856567,-0.019943237,-0.019515991,-0.027816772,-0.031463623,-0.028518677,0.013557434,-0.0020446777,0.038757324,-0.0184021,-0.0014305115,0.059539795,0.0056877136,0.113464355,-0.012359619,-0.057861328,-0.021636963,0.0368042,-0.0073394775,0.017700195,0.021636963,0.026550293,0.037078857,-0.0132751465,-0.012145996,0.011520386,0.006038666,-0.008323669,0.011444092,0.003019333,0.053100586,0.05279541,0.008110046,0.02444458,-0.008880615,-0.021636963,0.027114868,-0.022750854,-0.07080078,0.017562866,0.016159058,0.02696228,0.0031604767,-0.0022830963,-0.050567627,-0.01474762,-0.0064964294,0.06964111,0.070495605,0.036224365,0.018814087,-0.010253906,0.016998291,0.006881714,0.003211975,0.048583984,0.035949707,0.010108948,0.012428284,-0.030471802,0.06463623,0.005584717,0.021209717,-0.02935791,-0.0051612854,0.0012111664,0.026550293,0.0034770966,0.03302002,0.039031982,-0.008743286,0.009056091,-0.014045715,-0.009689331,-0.035247803,0.010955811,0.0045280457,0.010818481,0.03302002,-0.04776001,-0.060668945,0.016357422,0.045776367,-0.06347656,-0.007827759,0.016433716,-0.0005598068,0.027526855,-0.024856567,-0.011306763,0.042144775,-0.023452759,-0.029067993,0.024291992,0.011520386,-0.02218628,-0.024993896,-0.037078857,0.012641907,0.0006146431,0.012077332,0.039886475,-0.0128479,0.007091522,-0.005302429,0.029220581,-0.0028095245,-0.04748535,0.046081543,-0.014465332,-0.0012025833,0.01348114,0.033996582,-0.0010137558,-0.052520752,-0.011657715,0.009552002,-0.0017824173,-0.021774292,0.021072388,-0.058135986,-0.04748535,-0.044677734,0.06378174,0.014816284,0.05279541,-0.021209717,-0.076416016,0.036224365,-0.066589355,-0.01222229,0.003440857,-0.0044059753,-0.013626099,0.045776367,-0.027389526,-0.0048446655,-0.009338379,0.08203125,-0.038757324,-0.0066719055,-0.00983429,0.055908203,0.039886475,-0.043548584,-0.04156494,0.021774292,0.039611816,0.043823242,-0.041015625,-0.037078857,-0.00036859512,-0.049713135,-0.052246094,-0.0107421875,-0.020935059,-0.048034668,0.010536194,0.0053367615,-0.012290955,-0.010322571,0.010040283,-0.03302002,0.0028095245,-0.0061798096,-0.0184021,0.039031982,-0.056732178,-0.034820557,-0.012992859,0.033416748,-0.00038194656,-0.007549286,-0.0005245209,-0.04888916,-0.0018701553,-0.040740967,-0.016357422,-0.015098572,0.00066280365,-0.034973145,-0.03933716,-0.0010137558,0.007827759,0.0018701553,0.0028095245,0.015380859,-0.064331055,0.025283813,0.012573242,-0.010604858,-0.010108948,0.010887146,0.03201294,-0.011375427,0.04888916,-0.022476196,-0.043273926,-0.052246094,0.032318115,-0.0184021,-0.0010318756,-0.050567627,-0.017623901,-0.04550171,-0.012573242,0.010467529,0.0066719055,0.011657715,-0.042144775,0.01966858,-0.01474762,-0.06604004,0.024719238,-0.025283813,0.02696228,-0.011024475,-0.019805908,0.01966858,-0.010887146,0.035949707,0.006954193,0.035125732,-0.04043579,0.008743286,-0.0058288574,-0.030334473,-0.03302002,-0.046081543,0.0065307617,-0.009689331,-0.049438477,0.010253906,0.008392334,0.025146484,0.014259338,-0.029632568,-0.005935669,-0.03652954,0.05984497,-0.024291992,-0.02935791,0.029922485,-0.008987427,0.046905518,0.025283813,0.01222229,-0.05984497,0.00793457,0.02809143,-0.02444458,-0.057861328,-0.036224365,0.024154663,0.0069885254,0.039611816,0.006214142,0.052246094,0.039611816,0.015586853,0.018539429,0.06011963,0.038757324,-0.013908386,-0.07867432,-0.026397705,0.0076904297,-0.060943604,-0.052246094,0.013343811,0.0017471313,-0.076416016,-0.008354187,-0.032440186,-0.016357422,0.008918762,0.004951477,0.015731812,0.04916382,0.013412476,-0.037353516,0.008705139,0.015312195,-0.0065994263,0.046905518,0.031463623,-0.022338867,-0.015167236,0.027114868,-0.0184021,-0.010391235,0.021484375,0.034118652,0.023452759,0.031311035,0.043548584,-0.024719238,-0.0128479,0.004371643,0.035827637,0.044677734,0.060668945,-0.016433716,-0.010391235,0.011795044,-0.0013256073,-0.029220581,0.005126953,-0.016647339,-0.018539429,-0.024856567,0.012359619,-0.037353516,0.02696228,0.053375244,-0.00705719,-0.009552002,0.026397705,-0.02458191,-0.042419434,0.028793335,-0.020645142,-0.0051612854,-0.009269714,-0.044952393,0.032318115,-0.02949524,-0.06573486,0.013557434,0.06604004,-0.007129669,-0.031311035,-0.0041618347,-0.010467529,0.030761719,-0.022476196,-0.050842285,-0.02696228,0.013343811,-0.05758667,0.013977051,-0.0045661926,-0.02809143,-0.021636963,-0.028656006,-0.05505371,0.026260376,-0.024017334,0.03933716,0.024856567,-0.023452759,0.042144775,0.04296875,-0.008460999,-0.013694763,-0.011726379,-0.019515991,-0.016647339,0.0061454773,0.016082764,-0.041290283,-0.00642395,0.05871582,-0.076416016,0.010185242,0.014328003,0.013557434,-0.027816772,0.02696228,-0.025848389,0.010185242,0.040161133,-0.0012464523,-0.0345459,0.012992859,-0.04156494,0.05899048,0.0050201416,0.03427124,-0.05731201,0.003211975,0.004020691,-0.023880005,-0.010391235,-0.066833496,0.005897522,0.027664185,-0.01348114,-0.027664185,-0.0018787384,0.007549286,0.025146484,-0.06378174,0.024154663,0.0013427734,-0.019104004,-0.036224365,-0.032318115,0.0045280457,-0.06488037,-0.023742676,-0.049987793,0.037628174,0.024017334,-0.045776367,0.018966675,0.022750854,0.028656006,0.003791809,0.0440979,-0.021072388,-0.0018167496,0.00054216385,0.021347046,0.025421143,0.0132751465,-0.023040771,0.011795044,0.0027389526,-0.0026512146,0.02218628,0.03286743,0.01651001,-0.049987793,0.016708374,-0.024154663,0.048583984,0.0012111664,0.04269409,-0.018539429,0.025283813,-0.07696533,0.07305908,-0.0256958,-0.0074806213,-0.0045280457,-0.0231781,-0.048309326,-0.06488037,-0.04776001,-0.012992859,-0.032440186,0.036224365,0.027114868,0.009971619,0.0007724762,0.020507812,-0.024291992,0.018539429,0.0345459,0.011375427,0.07470703,0.0031433105,0.0017204285,-0.015235901,0.014816284,0.0051956177,-0.0019140244,-0.029922485,0.005054474,0.036224365,0.020507812,-0.051116943,-0.026260376,0.012641907,-0.026397705,-0.021484375,-0.0074806213,-0.023590088,-0.024291992,-0.013130188,-0.003211975,-0.03793335,-0.05505371,-0.00030064583,-0.014961243,-0.025421143,-0.027389526,-0.02218628,-0.0056877136,-0.021072388,0.030471802,0.020645142,-0.008079529,-0.022476196,0.0054779053,-0.046905518,0.010818481,-0.018814087,-0.051116943,0.023742676,0.00024461746,-0.038208008,-0.033996582,0.034423828,-0.002439499,0.033569336,-0.024017334,-0.017410278,-0.008636475,-0.0024929047,0.014045715,-0.00086450577,0.019241333,-0.00793457,-0.055908203,0.0027561188,0.007408142,-0.040740967,1.1503696e-05,0.015449524,-0.027389526,0.0028972626,0.0256958,0.015380859,0.06011963,-0.011375427,0.0027923584,0.021209717,-0.013908386,0.025558472,0.0031776428,0.06347656,0.012779236,0.010818481,0.0016851425,-0.0713501,0.043548584,-0.012496948,-0.060394287,0.00674057,0.05731201,0.044677734,0.008430481,0.04437256,0.024719238,0.070495605,0.03161621,-0.015586853,-0.061798096,-0.023040771,0.021347046,-0.018539429,-0.08148193,0.05758667,0.060943604,-0.0035648346,-0.0128479,0.021636963,-0.0060043335,0.052246094,0.037628174,-0.0027923584,-0.06744385,0.015731812,0.057861328,-0.01222229,-0.0053367615,-0.0048103333,-0.01222229,0.020935059,-0.038482666,-0.0044937134,-0.00028300285,0.07751465,-0.007724762,-0.017974854,0.014045715,0.032440186,0.015098572,-0.0440979,-0.0066719055,-0.009902954,-0.027664185,0.0055122375,0.018539429,-0.0024929047,0.099975586,0.021347046,-0.01966858,0.053100586,0.0040740967,-0.013763428,0.012077332,-0.024017334,0.02458191,-0.039611816,0.05618286,0.015098572,0.02218628,-0.016784668,-0.0440979,-0.005897522,0.0024051666,0.040740967,0.01586914,0.019104004,0.012924194,-0.005584717,0.07104492,0.043823242,-0.00062322617,0.026550293,-0.034820557,0.0054779053,-0.046905518,0.009056091,0.019104004,-0.031036377,0.024856567,0.017486572,0.02218628,-0.031036377,-0.018814087,-0.034820557,0.046905518,-0.025421143,0.030899048,-0.004020691,-0.019241333,0.039031982,-0.051971436,0.054504395,0.015808105,0.04522705,-0.014183044,0.032440186,0.049713135,-0.017349243,0.026123047,0.0463562,0.015098572,0.04156494,0.02935791,0.0050201416,-0.01966858,0.01600647,0.0030555725,-0.052520752,0.012779236,0.016159058,0.0020198822,0.006778717,-0.01651001,-0.005794525,-0.032318115,0.070495605,0.02696228,-0.026550293,0.020645142,-0.038757324,0.057861328,-0.0015182495,0.051696777,-0.0231781,0.025421143,0.0030021667,-0.0048446655,0.0018787384,-0.0023345947,0.0022125244,0.043823242,-0.043273926,0.060668945,0.018676758,0.006778717,-0.071899414,-0.015098572,-0.0019664764,-0.0065994263,0.030334473,-0.0013341904,-0.04550171,0.021209717,0.015808105,0.021774292,0.037078857,0.058441162,0.008460999,-0.02218628,0.006778717,-0.026123047]", "client_status_at": "2025-10-02T09:52:48.211722Z", "server_status_at": "2025-10-02T09:52:48.211705Z", "created_at": "2025-10-02T08:52:42.551985Z"}, {"id": "fbf749b3-88f5-437b-b682-d077cd975706", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://www.anthropic.com/engineering/claude-code-best-practices", "canonical_url": null, "title": "Claude Code Best Practices", "source_site": "AnthropicAI", "publication_date": "2025-04-18T00:00:00Z", "favicon_url": "https://www.anthropic.com/favicon.ico", "content_markdown": "We recently [released Claude Code](https://www.anthropic.com/news/claude-3-7-sonnet), a command line tool for agentic coding. Developed as a research project, Claude Code gives Anthropic engineers and researchers a more native way to integrate Claude into their coding workflows.\n\nClaude Code is intentionally low-level and unopinionated, providing close to raw model access without forcing specific workflows. This design philosophy creates a flexible, customizable, scriptable, and safe power tool. While powerful, this flexibility presents a learning curve for engineers new to agentic coding tools—at least until they develop their own best practices.\n\nThis post outlines general patterns that have proven effective, both for Anthropic's internal teams and for external engineers using Claude Code across various codebases, languages, and environments. Nothing in this list is set in stone nor universally applicable; consider these suggestions as starting points. We encourage you to experiment and find what works best for you!\n\n*Looking for more detailed information? Our comprehensive documentation at claude.ai/code*\n\n*covers all the features mentioned in this post and provides additional examples, implementation details, and advanced techniques.*\n\n## 1. Customize your setup\n\nClaude Code is an agentic coding assistant that automatically pulls context into prompts. This context gathering consumes time and tokens, but you can optimize it through environment tuning.\n\n### a. Create `CLAUDE.md`\n\nfiles\n\n`CLAUDE.md`\n\nis a special file that Claude automatically pulls into context when starting a conversation. This makes it an ideal place for documenting:\n\n- Common bash commands\n- Core files and utility functions\n- Code style guidelines\n- Testing instructions\n- Repository etiquette (e.g., branch naming, merge vs. rebase, etc.)\n- Developer environment setup (e.g., pyenv use, which compilers work)\n- Any unexpected behaviors or warnings particular to the project\n- Other information you want Claude to remember\n\nThere’s no required format for `CLAUDE.md`\n\nfiles. We recommend keeping them concise and human-readable. For example:\n\n```\n# Bash commands\n- npm run build: Build the project\n- npm run typecheck: Run the typechecker\n# Code style\n- Use ES modules (import/export) syntax, not CommonJS (require)\n- Destructure imports when possible (eg. import { foo } from 'bar')\n# Workflow\n- Be sure to typecheck when you’re done making a series of code changes\n- Prefer running single tests, and not the whole test suite, for performance\n```\n\n\nYou can place `CLAUDE.md`\n\nfiles in several locations:\n\n**The root of your repo**, or wherever you run`claude`\n\nfrom (the most common usage). Name it`CLAUDE.md`\n\nand check it into git so that you can share it across sessions and with your team (recommended), or name it`CLAUDE.local.md`\n\nand`.gitignore`\n\nit**Any parent of the directory**where you run`claude`\n\n. This is most useful for monorepos, where you might run`claude`\n\nfrom`root/foo`\n\n, and have`CLAUDE.md`\n\nfiles in both`root/CLAUDE.md`\n\nand`root/foo/CLAUDE.md`\n\n. Both of these will be pulled into context automatically**Any child of the directory**where you run`claude`\n\n. This is the inverse of the above, and in this case, Claude will pull in`CLAUDE.md`\n\nfiles on demand when you work with files in child directories**Your home folder**(`~/.claude/CLAUDE.md`\n\n), which applies it to all your*claude*sessions\n\nWhen you run the `/init`\n\ncommand, Claude will automatically generate a `CLAUDE.md`\n\nfor you.\n\n### b. Tune your `CLAUDE.md`\n\nfiles\n\nYour `CLAUDE.md`\n\nfiles become part of Claude’s prompts, so they should be refined like any frequently used prompt. A common mistake is adding extensive content without iterating on its effectiveness. Take time to experiment and determine what produces the best instruction following from the model.\n\nYou can add content to your `CLAUDE.md`\n\nmanually or press the `#`\n\nkey to give Claude an instruction that it will automatically incorporate into the relevant `CLAUDE.md`\n\n. Many engineers use `#`\n\nfrequently to document commands, files, and style guidelines while coding, then include `CLAUDE.md`\n\nchanges in commits so team members benefit as well.\n\nAt Anthropic, we occasionally run `CLAUDE.md`\n\nfiles through the [prompt improver](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-improver) and often tune instructions (e.g. adding emphasis with \"IMPORTANT\" or \"YOU MUST\") to improve adherence.\n\n### c. Curate Claude's list of allowed tools\n\nBy default, Claude Code requests permission for any action that might modify your system: file writes, many bash commands, MCP tools, etc. We designed Claude Code with this deliberately conservative approach to prioritize safety. You can customize the allowlist to permit additional tools that you know are safe, or to allow potentially unsafe tools that are easy to undo (e.g., file editing, `git commit`\n\n).\n\nThere are four ways to manage allowed tools:\n\n**Select \"Always allow\"**when prompted during a session.**Use the**after starting Claude Code to add or remove tools from the allowlist. For example, you can add`/permissions`\n\ncommand`Edit`\n\nto always allow file edits,`Bash(git commit:*)`\n\nto allow git commits, or`mcp__puppeteer__puppeteer_navigate`\n\nto allow navigating with the Puppeteer MCP server.**Manually edit**your`.claude/settings.json`\n\nor`~/.claude.json`\n\n(we recommend checking the former into source control to share with your team)*.***Use the**for session-specific permissions.`--allowedTools`\n\nCLI flag\n\n### d. If using GitHub, install the gh CLI\n\nClaude knows how to use the `gh`\n\nCLI to interact with GitHub for creating issues, opening pull requests, reading comments, and more. Without `gh`\n\ninstalled, Claude can still use the GitHub API or MCP server (if you have it installed).\n\n## 2. Give Claude more tools\n\nClaude has access to your shell environment, where you can build up sets of convenience scripts and functions for it just like you would for yourself. It can also leverage more complex tools through MCP and REST APIs.\n\n### a. Use Claude with bash tools\n\nClaude Code inherits your bash environment, giving it access to all your tools. While Claude knows common utilities like unix tools and `gh`\n\n, it won't know about your custom bash tools without instructions:\n\n- Tell Claude the tool name with usage examples\n- Tell Claude to run\n`--help`\n\nto see tool documentation - Document frequently used tools in\n`CLAUDE.md`\n\n\n### b. Use Claude with MCP\n\nClaude Code functions as both an MCP server and client. As a client, it can connect to any number of MCP servers to access their tools in three ways:\n\n**In project config**(available when running Claude Code in that directory)**In global config**(available in all projects)**In a checked-in**(available to anyone working in your codebase). For example, you can add Puppeteer and Sentry servers to your`.mcp.json`\n\nfile`.mcp.json`\n\n, so that every engineer working on your repo can use these out of the box.\n\nWhen working with MCP, it can also be helpful to launch Claude with the `--mcp-debug`\n\nflag to help identify configuration issues.\n\n### c. Use custom slash commands\n\nFor repeated workflows—debugging loops, log analysis, etc.—store prompt templates in Markdown files within the `.claude/commands`\n\nfolder. These become available through the slash commands menu when you type `/`\n\n. You can check these commands into git to make them available for the rest of your team.\n\nCustom slash commands can include the special keyword `$ARGUMENTS`\n\nto pass parameters from command invocation.\n\nFor example, here’s a slash command that you could use to automatically pull and fix a Github issue:\n\n```\nPlease analyze and fix the GitHub issue: $ARGUMENTS.\nFollow these steps:\n1. Use `gh issue view` to get the issue details\n2. Understand the problem described in the issue\n3. Search the codebase for relevant files\n4. Implement the necessary changes to fix the issue\n5. Write and run tests to verify the fix\n6. Ensure code passes linting and type checking\n7. Create a descriptive commit message\n8. Push and create a PR\nRemember to use the GitHub CLI (`gh`) for all GitHub-related tasks.\n```\n\n\nPutting the above content into `.claude/commands/fix-github-issue.md`\n\nmakes it available as the `/project:fix-github-issue`\n\ncommand in Claude Code. You could then for example use `/project:fix-github-issue 1234`\n\nto have Claude fix issue #1234. Similarly, you can add your own personal commands to the* *`~/.claude/commands`\n\nfolder for commands you want available in all of your sessions.\n\n## 3. Try common workflows\n\nClaude Code doesn’t impose a specific workflow, giving you the flexibility to use it how you want. Within the space this flexibility affords, several successful patterns for effectively using Claude Code have emerged across our community of users:\n\n### a. Explore, plan, code, commit\n\nThis versatile workflow suits many problems:\n\n**Ask Claude to read relevant files, images, or URLs**, providing either general pointers (\"read the file that handles logging\") or specific filenames (\"read logging.py\"), but explicitly tell it not to write any code just yet.- This is the part of the workflow where you should consider strong use of subagents, especially for complex problems. Telling Claude to use subagents to verify details or investigate particular questions it might have, especially early on in a conversation or task, tends to preserve context availability without much downside in terms of lost efficiency.\n\n**Ask Claude to make a plan for how to approach a specific problem**. We recommend using the word \"think\" to trigger extended thinking mode, which gives Claude additional computation time to evaluate alternatives more thoroughly. These specific phrases are mapped directly to increasing levels of thinking budget in the system: \"think\" < \"think hard\" < \"think harder\" < \"ultrathink.\" Each level allocates progressively more thinking budget for Claude to use.- If the results of this step seem reasonable, you can have Claude create a document or a GitHub issue with its plan so that you can reset to this spot if the implementation (step 3) isn’t what you want.\n\n**Ask Claude to implement its solution in code**. This is also a good place to ask it to explicitly verify the reasonableness of its solution as it implements pieces of the solution.**Ask Claude to commit the result and create a pull request**. If relevant, this is also a good time to have Claude update any READMEs or changelogs with an explanation of what it just did.\n\nSteps #1-#2 are crucial—without them, Claude tends to jump straight to coding a solution. While sometimes that's what you want, asking Claude to research and plan first significantly improves performance for problems requiring deeper thinking upfront.\n\n### b. Write tests, commit; code, iterate, commit\n\nThis is an Anthropic-favorite workflow for changes that are easily verifiable with unit, integration, or end-to-end tests. Test-driven development (TDD) becomes even more powerful with agentic coding:\n\n**Ask Claude to write tests based on expected input/output pairs**. Be explicit about the fact that you’re doing test-driven development so that it avoids creating mock implementations, even for functionality that doesn’t exist yet in the codebase.**Tell Claude to run the tests and confirm they fail**. Explicitly telling it not to write any implementation code at this stage is often helpful.**Ask Claude to commit the tests**when you’re satisfied with them.**Ask Claude to write code that passes the tests**, instructing it not to modify the tests. Tell Claude to keep going until all tests pass. It will usually take a few iterations for Claude to write code, run the tests, adjust the code, and run the tests again.- At this stage, it can help to ask it to verify with independent subagents that the implementation isn’t overfitting to the tests\n\n**Ask Claude to commit the code**once you’re satisfied with the changes.\n\nClaude performs best when it has a clear target to iterate against—a visual mock, a test case, or another kind of output. By providing expected outputs like tests, Claude can make changes, evaluate results, and incrementally improve until it succeeds.\n\n### c. Write code, screenshot result, iterate\n\nSimilar to the testing workflow, you can provide Claude with visual targets:\n\n**Give Claude a way to take browser screenshots**(e.g., with the[Puppeteer MCP server](https://github.com/modelcontextprotocol/servers/tree/c19925b8f0f2815ad72b08d2368f0007c86eb8e6/src/puppeteer), an[iOS simulator MCP server](https://github.com/joshuayoes/ios-simulator-mcp), or manually copy / paste screenshots into Claude).**Give Claude a visual mock**by copying / pasting or drag-dropping an image, or giving Claude the image file path.**Ask Claude to implement the design**in code, take screenshots of the result, and iterate until its result matches the mock.**Ask Claude to commit**when you're satisfied.\n\nLike humans, Claude's outputs tend to improve significantly with iteration. While the first version might be good, after 2-3 iterations it will typically look much better. Give Claude the tools to see its outputs for best results.\n\n### d. Safe YOLO mode\n\nInstead of supervising Claude, you can use `claude --dangerously-skip-permissions`\n\nto bypass all permission checks and let Claude work uninterrupted until completion. This works well for workflows like fixing lint errors or generating boilerplate code.\n\nLetting Claude run arbitrary commands is risky and can result in data loss, system corruption, or even data exfiltration (e.g., via prompt injection attacks). To minimize these risks, use `--dangerously-skip-permissions`\n\nin a container without internet access. You can follow this [reference implementation](https://github.com/anthropics/claude-code/tree/main/.devcontainer) using Docker Dev Containers.\n\n### e. Codebase Q&A\n\nWhen onboarding to a new codebase, use Claude Code for learning and exploration. You can ask Claude the same sorts of questions you would ask another engineer on the project when pair programming. Claude can agentically search the codebase to answer general questions like:\n\n- How does logging work?\n- How do I make a new API endpoint?\n- What does\n`async move { ... }`\n\ndo on line 134 of`foo.rs`\n\n? - What edge cases does\n`CustomerOnboardingFlowImpl`\n\nhandle? - Why are we calling\n`foo()`\n\ninstead of`bar()`\n\non line 333? - What’s the equivalent of line 334 of\n`baz.py`\n\nin Java?\n\nAt Anthropic, using Claude Code in this way has become our core onboarding workflow, significantly improving ramp-up time and reducing load on other engineers. No special prompting is required! Simply ask questions, and Claude will explore the code to find answers.\n\n### f. Use Claude to interact with git\n\nClaude can effectively handle many git operations. Many Anthropic engineers use Claude for 90%+ of our *git* interactions:\n\n**Searching**to answer questions like \"What changes made it into v1.2.3?\", \"Who owns this particular feature?\", or \"Why was this API designed this way?\" It helps to explicitly prompt Claude to look through git history to answer queries like these.*git*history**Writing commit messages**.Claude will look at your changes and recent history automatically to compose a message taking all the relevant context into account**Handling complex git operations**like reverting files, resolving rebase conflicts, and comparing and grafting patches\n\n### g. Use Claude to interact with GitHub\n\nClaude Code can manage many GitHub interactions:\n\n**Creating pull requests**: Claude understands the shorthand \"pr\" and will generate appropriate commit messages based on the diff and surrounding context.**Implementing one-shot resolutions**for simple code review comments: just tell it to fix comments on your PR (optionally, give it more specific instructions) and push back to the PR branch when it's done.**Fixing failing builds**or linter warnings**Categorizing and triaging open issues**by asking Claude to loop over open GitHub issues\n\nThis eliminates the need to remember `gh`\n\ncommand line syntax while automating routine tasks.\n\n### h. Use Claude to work with Jupyter notebooks\n\nResearchers and data scientists at Anthropic use Claude Code to read and write Jupyter notebooks. Claude can interpret outputs, including images, providing a fast way to explore and interact with data. There are no required prompts or workflows, but a workflow we recommend is to have Claude Code and a `.ipynb`\n\nfile open side-by-side in VS Code.\n\nYou can also ask Claude to clean up or make aesthetic improvements to your Jupyter notebook before you show it to colleagues. Specifically telling it to make the notebook or its data visualizations “aesthetically pleasing” tends to help remind it that it’s optimizing for a human viewing experience.\n\n## 4. Optimize your workflow\n\nThe suggestions below apply across all workflows:\n\n### a. Be specific in your instructions\n\nClaude Code’s success rate improves significantly with more specific instructions, especially on first attempts. Giving clear directions upfront reduces the need for course corrections later.\n\nFor example:\n\n| Poor | Good |\n|---|---|\n| add tests for foo.py | write a new test case for foo.py, covering the edge case where the user is logged out. avoid mocks |\n| why does ExecutionFactory have such a weird api? | look through ExecutionFactory's git history and summarize how its api came to be |\n| add a calendar widget | look at how existing widgets are implemented on the home page to understand the patterns and specifically how code and interfaces are separated out. HotDogWidget.php is a good example to start with. then, follow the pattern to implement a new calendar widget that lets the user select a month and paginate forwards/backwards to pick a year. Build from scratch without libraries other than the ones already used in the rest of the codebase. |\n\nClaude can infer intent, but it can't read minds. Specificity leads to better alignment with expectations.\n\n### b. Give Claude images\n\nClaude excels with images and diagrams through several methods:\n\n**Paste screenshots**(pro tip: hit*cmd+ctrl+shift+4*in macOS to screenshot to clipboard and*ctrl+v*to paste. Note that this is not cmd+v like you would usually use to paste on mac and does not work remotely.)**Drag and drop**images directly into the prompt input**Provide file paths**for images\n\nThis is particularly useful when working with design mocks as reference points for UI development, and visual charts for analysis and debugging. If you are not adding visuals to context, it can still be helpful to be clear with Claude about how important it is for the result to be visually appealing.\n\n### c. Mention files you want Claude to look at or work on\n\nUse tab-completion to quickly reference files or folders anywhere in your repository, helping Claude find or update the right resources.\n\n### d. Give Claude URLs\n\nPaste specific URLs alongside your prompts for Claude to fetch and read. To avoid permission prompts for the same domains (e.g., docs.foo.com), use* *`/permissions`\n\nto add domains to your allowlist.\n\n### e. Course correct early and often\n\nWhile auto-accept mode (shift+tab to toggle) lets Claude work autonomously, you'll typically get better results by being an active collaborator and guiding Claude's approach. You can get the best results by thoroughly explaining the task to Claude at the beginning, but you can also course correct Claude at any time.\n\nThese four tools help with course correction:\n\n**Ask Claude to make a plan**before coding. Explicitly tell it not to code until you’ve confirmed its plan looks good.**Press Escape to interrupt**Claude during any phase (thinking, tool calls, file edits), preserving context so you can redirect or expand instructions.**Double-tap Escape to jump back in history**, edit a previous prompt, and explore a different direction. You can edit the prompt and repeat until you get the result you're looking for.**Ask Claude to undo changes**, often in conjunction with option #2 to take a different approach.\n\nThough Claude Code occasionally solves problems perfectly on the first attempt, using these correction tools generally produces better solutions faster.\n\n### f. Use `/clear`\n\nto keep context focused\n\nDuring long sessions, Claude's context window can fill with irrelevant conversation, file contents, and commands. This can reduce performance and sometimes distract Claude. Use the `/clear`\n\ncommand frequently between tasks to reset the context window.\n\n### g. Use checklists and scratchpads for complex workflows\n\nFor large tasks with multiple steps or requiring exhaustive solutions—like code migrations, fixing numerous lint errors, or running complex build scripts—improve performance by having Claude use a Markdown file (or even a GitHub issue!) as a checklist and working scratchpad:\n\nFor example, to fix a large number of lint issues, you can do the following:\n\n**Tell Claude to run the lint command**and write all resulting errors (with filenames and line numbers) to a Markdown checklist**Instruct Claude to address each issue one by one**, fixing and verifying before checking it off and moving to the next\n\n### h. Pass data into Claude\n\nSeveral methods exist for providing data to Claude:\n\n**Copy and paste**directly into your prompt (most common approach)**Pipe into Claude Code**(e.g.,`cat foo.txt | claude`\n\n), particularly useful for logs, CSVs, and large data**Tell Claude to pull data**via bash commands, MCP tools, or custom slash commands**Ask Claude to read files**or fetch URLs (works for images too)\n\nMost sessions involve a combination of these approaches. For example, you can pipe in a log file, then tell Claude to use a tool to pull in additional context to debug the logs.\n\n## 5. Use headless mode to automate your infra\n\nClaude Code includes [headless mode](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview#automate-ci-and-infra-workflows) for non-interactive contexts like CI, pre-commit hooks, build scripts, and automation. Use the `-p`\n\nflag with a prompt to enable headless mode, and `--output-format stream-json`\n\nfor streaming JSON output.\n\nNote that headless mode does not persist between sessions. You have to trigger it each session.\n\n### a. Use Claude for issue triage\n\nHeadless mode can power automations triggered by GitHub events, such as when a new issue is created in your repository. For example, the public [Claude Code repository](https://github.com/anthropics/claude-code/blob/main/.github/actions/claude-issue-triage-action/action.yml) uses Claude to inspect new issues as they come in and assign appropriate labels.\n\n### b. Use Claude as a linter\n\nClaude Code can provide [subjective code reviews](https://github.com/anthropics/claude-code/blob/main/.github/actions/claude-code-action/action.yml) beyond what traditional linting tools detect, identifying issues like typos, stale comments, misleading function or variable names, and more.\n\n## 6. Uplevel with multi-Claude workflows\n\nBeyond standalone usage, some of the most powerful applications involve running multiple Claude instances in parallel:\n\n### a. Have one Claude write code; use another Claude to verify\n\nA simple but effective approach is to have one Claude write code while another reviews or tests it. Similar to working with multiple engineers, sometimes having separate context is beneficial:\n\n- Use Claude to write code\n- Run\n`/clear`\n\nor start a second Claude in another terminal - Have the second Claude review the first Claude's work\n- Start another Claude (or\n`/clear`\n\nagain) to read both the code and review feedback - Have this Claude edit the code based on the feedback\n\nYou can do something similar with tests: have one Claude write tests, then have another Claude write code to make the tests pass. You can even have your Claude instances communicate with each other by giving them separate working scratchpads and telling them which one to write to and which one to read from.\n\nThis separation often yields better results than having a single Claude handle everything.\n\n### b. Have multiple checkouts of your repo\n\nRather than waiting for Claude to complete each step, something many engineers at Anthropic do is:\n\n**Create 3-4 git checkouts**in separate folders**Open each folder**in separate terminal tabs**Start Claude in each folder**with different tasks**Cycle through**to check progress and approve/deny permission requests\n\n### c. Use git worktrees\n\nThis approach shines for multiple independent tasks, offering a lighter-weight alternative to multiple checkouts. Git worktrees allow you to check out multiple branches from the same repository into separate directories. Each worktree has its own working directory with isolated files, while sharing the same Git history and reflog.\n\nUsing git worktrees enables you to run multiple Claude sessions simultaneously on different parts of your project, each focused on its own independent task. For instance, you might have one Claude refactoring your authentication system while another builds a completely unrelated data visualization component. Since the tasks don't overlap, each Claude can work at full speed without waiting for the other's changes or dealing with merge conflicts:\n\n**Create worktrees**:`git worktree add ../project-feature-a feature-a`\n\n**Launch Claude in each worktree**:`cd ../project-feature-a && claude`\n\n**Create additional worktrees**as needed (repeat steps 1-2 in new terminal tabs)\n\nSome tips:\n\n- Use consistent naming conventions\n- Maintain one terminal tab per worktree\n- If you’re using iTerm2 on Mac,\n[set up notifications](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview#notification-setup)for when Claude needs attention - Use separate IDE windows for different worktrees\n- Clean up when finished:\n`git worktree remove ../project-feature-a`\n\n\n### d. Use headless mode with a custom harness\n\n`claude -p`\n\n(headless mode) integrates Claude Code programmatically into larger workflows while leveraging its built-in tools and system prompt. There are two primary patterns for using headless mode:\n\n1. **Fanning out** handles large migrations or analyses (e.g., analyzing sentiment in hundreds of logs or analyzing thousands of CSVs):\n\n- Have Claude write a script to generate a task list. For example, generate a list of 2k files that need to be migrated from framework A to framework B.\n- Loop through tasks, calling Claude programmatically for each and giving it a task and a set of tools it can use. For example:\n`claude -p “migrate foo.py from React to Vue. When you are done, you MUST return the string OK if you succeeded, or FAIL if the task failed.” --allowedTools Edit Bash(git commit:*)`\n\n- Run the script several times and refine your prompt to get the desired outcome.\n\n2. **Pipelining** integrates Claude into existing data/processing pipelines:\n\n- Call\n`claude -p “<your prompt>” --json | your_command`\n\n, where`your_command`\n\nis the next step of your processing pipeline - That’s it! JSON output (optional) can help provide structure for easier automated processing.\n\nFor both of these use cases, it can be helpful to use the `--verbose`\n\nflag for debugging the Claude invocation. We generally recommend turning verbose mode off in production for cleaner output.\n\nWhat are your tips and best practices for working with Claude Code? Tag @AnthropicAI so we can see what you're building!\n\n## Acknowledgements\n\nWritten by Boris Cherny. This work draws upon best practices from across the broader Claude Code user community, whose creative approaches and workflows continue to inspire us. Special thanks also to Daisy Hollman, Ashwin Bhat, Cat Wu, Sid Bidasaria, Cal Rueb, Nodir Turakulov, Barry Zhang, Drew Hodun and many other Anthropic engineers whose valuable insights and practical experience with Claude Code helped shape these recommendations.", "content_text": "We recently released Claude Code, a command line tool for agentic coding. Developed as a research project, Claude Code gives Anthropic engineers and researchers a more native way to integrate Claude into their coding workflows.\nClaude Code is intentionally low-level and unopinionated, providing close to raw model access without forcing specific workflows. This design philosophy creates a flexible, customizable, scriptable, and safe power tool. While powerful, this flexibility presents a learning curve for engineers new to agentic coding tools—at least until they develop their own best practices.\nThis post outlines general patterns that have proven effective, both for Anthropic's internal teams and for external engineers using Claude Code across various codebases, languages, and environments. Nothing in this list is set in stone nor universally applicable; consider these suggestions as starting points. We encourage you to experiment and find what works best for you!\nLooking for more detailed information? Our comprehensive documentation at claude.ai/code covers all the features mentioned in this post and provides additional examples, implementation details, and advanced techniques.\n1. Customize your setup\nClaude Code is an agentic coding assistant that automatically pulls context into prompts. This context gathering consumes time and tokens, but you can optimize it through environment tuning.\na. Create CLAUDE.md\nfiles\nCLAUDE.md\nis a special file that Claude automatically pulls into context when starting a conversation. This makes it an ideal place for documenting:\n- Common bash commands\n- Core files and utility functions\n- Code style guidelines\n- Testing instructions\n- Repository etiquette (e.g., branch naming, merge vs. rebase, etc.)\n- Developer environment setup (e.g., pyenv use, which compilers work)\n- Any unexpected behaviors or warnings particular to the project\n- Other information you want Claude to remember\nThere’s no required format for CLAUDE.md\nfiles. We recommend keeping them concise and human-readable. For example:\n# Bash commands\n- npm run build: Build the project\n- npm run typecheck: Run the typechecker\n# Code style\n- Use ES modules (import/export) syntax, not CommonJS (require)\n- Destructure imports when possible (eg. import { foo } from 'bar')\n# Workflow\n- Be sure to typecheck when you’re done making a series of code changes\n- Prefer running single tests, and not the whole test suite, for performance\nYou can place CLAUDE.md\nfiles in several locations:\n- The root of your repo, or wherever you run\nclaude\nfrom (the most common usage). Name itCLAUDE.md\nand check it into git so that you can share it across sessions and with your team (recommended), or name itCLAUDE.local.md\nand.gitignore\nit - Any parent of the directory where you run\nclaude\n. This is most useful for monorepos, where you might runclaude\nfromroot/foo\n, and haveCLAUDE.md\nfiles in bothroot/CLAUDE.md\nandroot/foo/CLAUDE.md\n. Both of these will be pulled into context automatically - Any child of the directory where you run\nclaude\n. This is the inverse of the above, and in this case, Claude will pull inCLAUDE.md\nfiles on demand when you work with files in child directories - Your home folder (\n~/.claude/CLAUDE.md\n), which applies it to all your claude sessions\nWhen you run the /init\ncommand, Claude will automatically generate a CLAUDE.md\nfor you.\nb. Tune your CLAUDE.md\nfiles\nYour CLAUDE.md\nfiles become part of Claude’s prompts, so they should be refined like any frequently used prompt. A common mistake is adding extensive content without iterating on its effectiveness. Take time to experiment and determine what produces the best instruction following from the model.\nYou can add content to your CLAUDE.md\nmanually or press the #\nkey to give Claude an instruction that it will automatically incorporate into the relevant CLAUDE.md\n. Many engineers use #\nfrequently to document commands, files, and style guidelines while coding, then include CLAUDE.md\nchanges in commits so team members benefit as well.\nAt Anthropic, we occasionally run CLAUDE.md\nfiles through the prompt improver and often tune instructions (e.g. adding emphasis with \"IMPORTANT\" or \"YOU MUST\") to improve adherence.\nc. Curate Claude's list of allowed tools\nBy default, Claude Code requests permission for any action that might modify your system: file writes, many bash commands, MCP tools, etc. We designed Claude Code with this deliberately conservative approach to prioritize safety. You can customize the allowlist to permit additional tools that you know are safe, or to allow potentially unsafe tools that are easy to undo (e.g., file editing, git commit\n).\nThere are four ways to manage allowed tools:\n- Select \"Always allow\" when prompted during a session.\n- Use the\n/permissions\ncommand after starting Claude Code to add or remove tools from the allowlist. For example, you can addEdit\nto always allow file edits,Bash(git commit:*)\nto allow git commits, ormcp__puppeteer__puppeteer_navigate\nto allow navigating with the Puppeteer MCP server. - Manually edit your\n.claude/settings.json\nor~/.claude.json\n(we recommend checking the former into source control to share with your team). - Use the\n--allowedTools\nCLI flag for session-specific permissions.\nd. If using GitHub, install the gh CLI\nClaude knows how to use the gh\nCLI to interact with GitHub for creating issues, opening pull requests, reading comments, and more. Without gh\ninstalled, Claude can still use the GitHub API or MCP server (if you have it installed).\n2. Give Claude more tools\nClaude has access to your shell environment, where you can build up sets of convenience scripts and functions for it just like you would for yourself. It can also leverage more complex tools through MCP and REST APIs.\na. Use Claude with bash tools\nClaude Code inherits your bash environment, giving it access to all your tools. While Claude knows common utilities like unix tools and gh\n, it won't know about your custom bash tools without instructions:\n- Tell Claude the tool name with usage examples\n- Tell Claude to run\n--help\nto see tool documentation - Document frequently used tools in\nCLAUDE.md\nb. Use Claude with MCP\nClaude Code functions as both an MCP server and client. As a client, it can connect to any number of MCP servers to access their tools in three ways:\n- In project config (available when running Claude Code in that directory)\n- In global config (available in all projects)\n- In a checked-in\n.mcp.json\nfile (available to anyone working in your codebase). For example, you can add Puppeteer and Sentry servers to your.mcp.json\n, so that every engineer working on your repo can use these out of the box.\nWhen working with MCP, it can also be helpful to launch Claude with the --mcp-debug\nflag to help identify configuration issues.\nc. Use custom slash commands\nFor repeated workflows—debugging loops, log analysis, etc.—store prompt templates in Markdown files within the .claude/commands\nfolder. These become available through the slash commands menu when you type /\n. You can check these commands into git to make them available for the rest of your team.\nCustom slash commands can include the special keyword $ARGUMENTS\nto pass parameters from command invocation.\nFor example, here’s a slash command that you could use to automatically pull and fix a Github issue:\nPlease analyze and fix the GitHub issue: $ARGUMENTS.\nFollow these steps:\n1. Use `gh issue view` to get the issue details\n2. Understand the problem described in the issue\n3. Search the codebase for relevant files\n4. Implement the necessary changes to fix the issue\n5. Write and run tests to verify the fix\n6. Ensure code passes linting and type checking\n7. Create a descriptive commit message\n8. Push and create a PR\nRemember to use the GitHub CLI (`gh`) for all GitHub-related tasks.\nPutting the above content into .claude/commands/fix-github-issue.md\nmakes it available as the /project:fix-github-issue\ncommand in Claude Code. You could then for example use /project:fix-github-issue 1234\nto have Claude fix issue #1234. Similarly, you can add your own personal commands to the ~/.claude/commands\nfolder for commands you want available in all of your sessions.\n3. Try common workflows\nClaude Code doesn’t impose a specific workflow, giving you the flexibility to use it how you want. Within the space this flexibility affords, several successful patterns for effectively using Claude Code have emerged across our community of users:\na. Explore, plan, code, commit\nThis versatile workflow suits many problems:\n- Ask Claude to read relevant files, images, or URLs, providing either general pointers (\"read the file that handles logging\") or specific filenames (\"read logging.py\"), but explicitly tell it not to write any code just yet.\n- This is the part of the workflow where you should consider strong use of subagents, especially for complex problems. Telling Claude to use subagents to verify details or investigate particular questions it might have, especially early on in a conversation or task, tends to preserve context availability without much downside in terms of lost efficiency.\n- Ask Claude to make a plan for how to approach a specific problem. We recommend using the word \"think\" to trigger extended thinking mode, which gives Claude additional computation time to evaluate alternatives more thoroughly. These specific phrases are mapped directly to increasing levels of thinking budget in the system: \"think\" < \"think hard\" < \"think harder\" < \"ultrathink.\" Each level allocates progressively more thinking budget for Claude to use.\n- If the results of this step seem reasonable, you can have Claude create a document or a GitHub issue with its plan so that you can reset to this spot if the implementation (step 3) isn’t what you want.\n- Ask Claude to implement its solution in code. This is also a good place to ask it to explicitly verify the reasonableness of its solution as it implements pieces of the solution.\n- Ask Claude to commit the result and create a pull request. If relevant, this is also a good time to have Claude update any READMEs or changelogs with an explanation of what it just did.\nSteps #1-#2 are crucial—without them, Claude tends to jump straight to coding a solution. While sometimes that's what you want, asking Claude to research and plan first significantly improves performance for problems requiring deeper thinking upfront.\nb. Write tests, commit; code, iterate, commit\nThis is an Anthropic-favorite workflow for changes that are easily verifiable with unit, integration, or end-to-end tests. Test-driven development (TDD) becomes even more powerful with agentic coding:\n- Ask Claude to write tests based on expected input/output pairs. Be explicit about the fact that you’re doing test-driven development so that it avoids creating mock implementations, even for functionality that doesn’t exist yet in the codebase.\n- Tell Claude to run the tests and confirm they fail. Explicitly telling it not to write any implementation code at this stage is often helpful.\n- Ask Claude to commit the tests when you’re satisfied with them.\n- Ask Claude to write code that passes the tests, instructing it not to modify the tests. Tell Claude to keep going until all tests pass. It will usually take a few iterations for Claude to write code, run the tests, adjust the code, and run the tests again.\n- At this stage, it can help to ask it to verify with independent subagents that the implementation isn’t overfitting to the tests\n- Ask Claude to commit the code once you’re satisfied with the changes.\nClaude performs best when it has a clear target to iterate against—a visual mock, a test case, or another kind of output. By providing expected outputs like tests, Claude can make changes, evaluate results, and incrementally improve until it succeeds.\nc. Write code, screenshot result, iterate\nSimilar to the testing workflow, you can provide Claude with visual targets:\n- Give Claude a way to take browser screenshots (e.g., with the Puppeteer MCP server, an iOS simulator MCP server, or manually copy / paste screenshots into Claude).\n- Give Claude a visual mock by copying / pasting or drag-dropping an image, or giving Claude the image file path.\n- Ask Claude to implement the design in code, take screenshots of the result, and iterate until its result matches the mock.\n- Ask Claude to commit when you're satisfied.\nLike humans, Claude's outputs tend to improve significantly with iteration. While the first version might be good, after 2-3 iterations it will typically look much better. Give Claude the tools to see its outputs for best results.\nd. Safe YOLO mode\nInstead of supervising Claude, you can use claude --dangerously-skip-permissions\nto bypass all permission checks and let Claude work uninterrupted until completion. This works well for workflows like fixing lint errors or generating boilerplate code.\nLetting Claude run arbitrary commands is risky and can result in data loss, system corruption, or even data exfiltration (e.g., via prompt injection attacks). To minimize these risks, use --dangerously-skip-permissions\nin a container without internet access. You can follow this reference implementation using Docker Dev Containers.\ne. Codebase Q&A\nWhen onboarding to a new codebase, use Claude Code for learning and exploration. You can ask Claude the same sorts of questions you would ask another engineer on the project when pair programming. Claude can agentically search the codebase to answer general questions like:\n- How does logging work?\n- How do I make a new API endpoint?\n- What does\nasync move { ... }\ndo on line 134 offoo.rs\n? - What edge cases does\nCustomerOnboardingFlowImpl\nhandle? - Why are we calling\nfoo()\ninstead ofbar()\non line 333? - What’s the equivalent of line 334 of\nbaz.py\nin Java?\nAt Anthropic, using Claude Code in this way has become our core onboarding workflow, significantly improving ramp-up time and reducing load on other engineers. No special prompting is required! Simply ask questions, and Claude will explore the code to find answers.\nf. Use Claude to interact with git\nClaude can effectively handle many git operations. Many Anthropic engineers use Claude for 90%+ of our git interactions:\n- Searching git history to answer questions like \"What changes made it into v1.2.3?\", \"Who owns this particular feature?\", or \"Why was this API designed this way?\" It helps to explicitly prompt Claude to look through git history to answer queries like these.\n- Writing commit messages. Claude will look at your changes and recent history automatically to compose a message taking all the relevant context into account\n- Handling complex git operations like reverting files, resolving rebase conflicts, and comparing and grafting patches\ng. Use Claude to interact with GitHub\nClaude Code can manage many GitHub interactions:\n- Creating pull requests: Claude understands the shorthand \"pr\" and will generate appropriate commit messages based on the diff and surrounding context.\n- Implementing one-shot resolutions for simple code review comments: just tell it to fix comments on your PR (optionally, give it more specific instructions) and push back to the PR branch when it's done.\n- Fixing failing builds or linter warnings\n- Categorizing and triaging open issues by asking Claude to loop over open GitHub issues\nThis eliminates the need to remember gh\ncommand line syntax while automating routine tasks.\nh. Use Claude to work with Jupyter notebooks\nResearchers and data scientists at Anthropic use Claude Code to read and write Jupyter notebooks. Claude can interpret outputs, including images, providing a fast way to explore and interact with data. There are no required prompts or workflows, but a workflow we recommend is to have Claude Code and a .ipynb\nfile open side-by-side in VS Code.\nYou can also ask Claude to clean up or make aesthetic improvements to your Jupyter notebook before you show it to colleagues. Specifically telling it to make the notebook or its data visualizations “aesthetically pleasing” tends to help remind it that it’s optimizing for a human viewing experience.\n4. Optimize your workflow\nThe suggestions below apply across all workflows:\na. Be specific in your instructions\nClaude Code’s success rate improves significantly with more specific instructions, especially on first attempts. Giving clear directions upfront reduces the need for course corrections later.\nFor example:\n| Poor | Good |\n|---|---|\n| add tests for foo.py | write a new test case for foo.py, covering the edge case where the user is logged out. avoid mocks |\n| why does ExecutionFactory have such a weird api? | look through ExecutionFactory's git history and summarize how its api came to be |\n| add a calendar widget | look at how existing widgets are implemented on the home page to understand the patterns and specifically how code and interfaces are separated out. HotDogWidget.php is a good example to start with. then, follow the pattern to implement a new calendar widget that lets the user select a month and paginate forwards/backwards to pick a year. Build from scratch without libraries other than the ones already used in the rest of the codebase. |\nClaude can infer intent, but it can't read minds. Specificity leads to better alignment with expectations.\nb. Give Claude images\nClaude excels with images and diagrams through several methods:\n- Paste screenshots (pro tip: hit cmd+ctrl+shift+4 in macOS to screenshot to clipboard and ctrl+v to paste. Note that this is not cmd+v like you would usually use to paste on mac and does not work remotely.)\n- Drag and drop images directly into the prompt input\n- Provide file paths for images\nThis is particularly useful when working with design mocks as reference points for UI development, and visual charts for analysis and debugging. If you are not adding visuals to context, it can still be helpful to be clear with Claude about how important it is for the result to be visually appealing.\nc. Mention files you want Claude to look at or work on\nUse tab-completion to quickly reference files or folders anywhere in your repository, helping Claude find or update the right resources.\nd. Give Claude URLs\nPaste specific URLs alongside your prompts for Claude to fetch and read. To avoid permission prompts for the same domains (e.g., docs.foo.com), use /permissions\nto add domains to your allowlist.\ne. Course correct early and often\nWhile auto-accept mode (shift+tab to toggle) lets Claude work autonomously, you'll typically get better results by being an active collaborator and guiding Claude's approach. You can get the best results by thoroughly explaining the task to Claude at the beginning, but you can also course correct Claude at any time.\nThese four tools help with course correction:\n- Ask Claude to make a plan before coding. Explicitly tell it not to code until you’ve confirmed its plan looks good.\n- Press Escape to interrupt Claude during any phase (thinking, tool calls, file edits), preserving context so you can redirect or expand instructions.\n- Double-tap Escape to jump back in history, edit a previous prompt, and explore a different direction. You can edit the prompt and repeat until you get the result you're looking for.\n- Ask Claude to undo changes, often in conjunction with option #2 to take a different approach.\nThough Claude Code occasionally solves problems perfectly on the first attempt, using these correction tools generally produces better solutions faster.\nf. Use /clear\nto keep context focused\nDuring long sessions, Claude's context window can fill with irrelevant conversation, file contents, and commands. This can reduce performance and sometimes distract Claude. Use the /clear\ncommand frequently between tasks to reset the context window.\ng. Use checklists and scratchpads for complex workflows\nFor large tasks with multiple steps or requiring exhaustive solutions—like code migrations, fixing numerous lint errors, or running complex build scripts—improve performance by having Claude use a Markdown file (or even a GitHub issue!) as a checklist and working scratchpad:\nFor example, to fix a large number of lint issues, you can do the following:\n- Tell Claude to run the lint command and write all resulting errors (with filenames and line numbers) to a Markdown checklist\n- Instruct Claude to address each issue one by one, fixing and verifying before checking it off and moving to the next\nh. Pass data into Claude\nSeveral methods exist for providing data to Claude:\n- Copy and paste directly into your prompt (most common approach)\n- Pipe into Claude Code (e.g.,\ncat foo.txt | claude\n), particularly useful for logs, CSVs, and large data - Tell Claude to pull data via bash commands, MCP tools, or custom slash commands\n- Ask Claude to read files or fetch URLs (works for images too)\nMost sessions involve a combination of these approaches. For example, you can pipe in a log file, then tell Claude to use a tool to pull in additional context to debug the logs.\n5. Use headless mode to automate your infra\nClaude Code includes headless mode for non-interactive contexts like CI, pre-commit hooks, build scripts, and automation. Use the -p\nflag with a prompt to enable headless mode, and --output-format stream-json\nfor streaming JSON output.\nNote that headless mode does not persist between sessions. You have to trigger it each session.\na. Use Claude for issue triage\nHeadless mode can power automations triggered by GitHub events, such as when a new issue is created in your repository. For example, the public Claude Code repository uses Claude to inspect new issues as they come in and assign appropriate labels.\nb. Use Claude as a linter\nClaude Code can provide subjective code reviews beyond what traditional linting tools detect, identifying issues like typos, stale comments, misleading function or variable names, and more.\n6. Uplevel with multi-Claude workflows\nBeyond standalone usage, some of the most powerful applications involve running multiple Claude instances in parallel:\na. Have one Claude write code; use another Claude to verify\nA simple but effective approach is to have one Claude write code while another reviews or tests it. Similar to working with multiple engineers, sometimes having separate context is beneficial:\n- Use Claude to write code\n- Run\n/clear\nor start a second Claude in another terminal - Have the second Claude review the first Claude's work\n- Start another Claude (or\n/clear\nagain) to read both the code and review feedback - Have this Claude edit the code based on the feedback\nYou can do something similar with tests: have one Claude write tests, then have another Claude write code to make the tests pass. You can even have your Claude instances communicate with each other by giving them separate working scratchpads and telling them which one to write to and which one to read from.\nThis separation often yields better results than having a single Claude handle everything.\nb. Have multiple checkouts of your repo\nRather than waiting for Claude to complete each step, something many engineers at Anthropic do is:\n- Create 3-4 git checkouts in separate folders\n- Open each folder in separate terminal tabs\n- Start Claude in each folder with different tasks\n- Cycle through to check progress and approve/deny permission requests\nc. Use git worktrees\nThis approach shines for multiple independent tasks, offering a lighter-weight alternative to multiple checkouts. Git worktrees allow you to check out multiple branches from the same repository into separate directories. Each worktree has its own working directory with isolated files, while sharing the same Git history and reflog.\nUsing git worktrees enables you to run multiple Claude sessions simultaneously on different parts of your project, each focused on its own independent task. For instance, you might have one Claude refactoring your authentication system while another builds a completely unrelated data visualization component. Since the tasks don't overlap, each Claude can work at full speed without waiting for the other's changes or dealing with merge conflicts:\n- Create worktrees:\ngit worktree add ../project-feature-a feature-a\n- Launch Claude in each worktree:\ncd ../project-feature-a && claude\n- Create additional worktrees as needed (repeat steps 1-2 in new terminal tabs)\nSome tips:\n- Use consistent naming conventions\n- Maintain one terminal tab per worktree\n- If you’re using iTerm2 on Mac, set up notifications for when Claude needs attention\n- Use separate IDE windows for different worktrees\n- Clean up when finished:\ngit worktree remove ../project-feature-a\nd. Use headless mode with a custom harness\nclaude -p\n(headless mode) integrates Claude Code programmatically into larger workflows while leveraging its built-in tools and system prompt. There are two primary patterns for using headless mode:\n1. Fanning out handles large migrations or analyses (e.g., analyzing sentiment in hundreds of logs or analyzing thousands of CSVs):\n- Have Claude write a script to generate a task list. For example, generate a list of 2k files that need to be migrated from framework A to framework B.\n- Loop through tasks, calling Claude programmatically for each and giving it a task and a set of tools it can use. For example:\nclaude -p “migrate foo.py from React to Vue. When you are done, you MUST return the string OK if you succeeded, or FAIL if the task failed.” --allowedTools Edit Bash(git commit:*)\n- Run the script several times and refine your prompt to get the desired outcome.\n2. Pipelining integrates Claude into existing data/processing pipelines:\n- Call\nclaude -p “<your prompt>” --json | your_command\n, whereyour_command\nis the next step of your processing pipeline - That’s it! JSON output (optional) can help provide structure for easier automated processing.\nFor both of these use cases, it can be helpful to use the --verbose\nflag for debugging the Claude invocation. We generally recommend turning verbose mode off in production for cleaner output.\nWhat are your tips and best practices for working with Claude Code? Tag @AnthropicAI so we can see what you're building!\nAcknowledgements\nWritten by Boris Cherny. This work draws upon best practices from across the broader Claude Code user community, whose creative approaches and workflows continue to inspire us. Special thanks also to Daisy Hollman, Ashwin Bhat, Cat Wu, Sid Bidasaria, Cal Rueb, Nodir Turakulov, Barry Zhang, Drew Hodun and many other Anthropic engineers whose valuable insights and practical experience with Claude Code helped shape these recommendations.", "content_token_count": 5552, "client_status": "queued", "server_status": "embedded", "summary": "The document outlines best practices for using Claude Code, an agentic coding tool developed by Anthropic. It covers various aspects such as customizing the setup, giving Claude more tools, trying common workflows, optimizing workflows, using headless mode for automation, and leveraging multi-Claude workflows for enhanced productivity.", "expiry_score": 0.3, "mistral_embedding": "[-0.05581665,0.038085938,0.022705078,0.012184143,0.0345459,-0.0027942657,0.05770874,0.0009908676,-0.021408081,0.010055542,-0.029693604,0.07714844,-0.05227661,0.016677856,-0.066711426,0.06384277,0.004524231,0.028030396,0.062438965,0.044708252,-0.024963379,-0.031219482,-0.05038452,-0.033599854,-0.031219482,-0.020706177,0.009223938,-0.078063965,-0.050628662,-0.02128601,0.0018854141,-0.02708435,0.03265381,0.005794525,0.016677856,-0.0013380051,-0.00081682205,-0.053466797,-0.012420654,0.0064468384,-0.005973816,-0.024719238,0.029800415,0.018218994,0.004436493,0.0069503784,0.023773193,-0.012359619,-0.0009832382,-0.0087509155,-0.028503418,0.041870117,-0.037841797,0.005382538,-0.004257202,0.01939392,0.037384033,0.019866943,-0.050628662,0.036895752,-0.0619812,0.009407043,0.015609741,-0.0078048706,0.036895752,0.030517578,0.0023651123,0.0064468384,-0.005115509,-0.07287598,-0.0011091232,0.0039901733,-0.006416321,0.03479004,-0.019866943,-0.017623901,-0.00014328957,0.017745972,-0.0032978058,-0.0018777847,-0.020584106,0.03857422,0.046142578,-0.0056495667,-0.025787354,0.00015616417,-0.012832642,0.01928711,-0.020233154,0.011352539,0.039733887,0.003030777,0.022125244,-0.028625488,0.06341553,0.03857422,0.00056552887,0.024246216,0.011238098,0.06384277,0.024246216,-0.006416321,0.085632324,-0.017868042,0.017501831,-0.011352539,0.051818848,0.0072135925,0.0087509155,-0.050628662,-0.09509277,-0.030166626,0.0069503784,-0.088012695,-0.013542175,0.025314331,0.0024700165,0.075683594,-0.046142578,-0.033355713,-0.009346008,0.047790527,-0.047790527,-0.0041389465,-0.0619812,-0.024124146,0.026611328,0.036193848,-0.011474609,-0.024017334,-0.044006348,-0.024124146,-0.021881104,-0.04446411,0.0065956116,-0.02330017,0.040924072,0.009170532,-0.033843994,-0.056762695,0.016555786,-0.025543213,-0.020111084,-0.018569946,-0.005794525,0.011474609,0.0071258545,0.034057617,-0.0068588257,-0.008041382,0.009407043,0.027679443,0.05630493,-0.017623901,0.0043182373,-0.022598267,-0.011062622,0.004585266,0.05770874,-0.0078048706,-0.02519226,0.010292053,-0.014549255,0.016326904,-0.0065956116,-0.025909424,-0.011238098,-0.012535095,-0.05203247,0.007980347,0.046844482,-0.052978516,0.040924072,0.0309906,-0.019866943,0.017745972,0.021652222,0.0014638901,0.06958008,0.028503418,-0.0031490326,-0.0309906,0.020233154,0.021408081,-0.0022029877,-0.023895264,-0.07287598,0.00774765,-0.013664246,0.044952393,-0.021759033,-0.024963379,-0.044708252,0.01360321,0.010940552,0.024963379,0.019638062,-0.012123108,0.039733887,-0.022827148,0.014015198,0.036193848,-0.008811951,0.05795288,0.029800415,0.029449463,0.006801605,0.022598267,0.05038452,-0.03074646,-0.007095337,0.008636475,-0.010643005,-0.0039634705,-0.057495117,-0.022827148,0.009170532,-0.017868042,-0.012535095,-0.008987427,-0.004585266,-0.048034668,-0.00674057,0.0039024353,0.017868042,0.045410156,0.028381348,-0.013191223,-0.025909424,-0.0029716492,0.026489258,0.030517578,-0.016555786,-0.011772156,-0.0149002075,-0.023544312,0.028274536,0.04849243,0.0154953,0.026489258,-0.007068634,0.014961243,0.00072431564,0.0095825195,0.009284973,-0.03643799,0.0042304993,-0.0035934448,-0.013900757,0.057250977,-0.07287598,-0.00021994114,-0.03857422,0.017150879,-0.089904785,-0.018218994,-0.017501831,-0.032409668,0.04046631,-0.045898438,-0.032165527,0.008277893,0.012359619,0.005531311,0.028274536,0.016433716,-0.04425049,0.016433716,0.0463562,-0.004760742,-0.016677856,-0.010116577,0.022598267,0.0060920715,0.068115234,-0.044006348,-0.0067710876,0.021759033,-0.0049972534,-0.0008945465,-0.0126571655,0.033843994,-0.002986908,0.062927246,-0.03857422,0.0046424866,-0.00041389465,0.0024547577,-0.0052948,-0.04046631,-0.00090551376,0.029220581,0.066223145,0.0053215027,-0.003873825,-0.043762207,0.007511139,0.0031642914,0.037384033,0.05606079,0.035736084,-0.02885437,-0.011116028,-0.018447876,0.05441284,-0.017028809,-0.022354126,-0.0095825195,-0.033355713,-0.037841797,0.0035934448,0.00025129318,0.0050849915,-0.008277893,-0.0073928833,0.0019664764,-0.035003662,-0.028381348,-0.024841309,-0.021408081,-0.0075416565,-0.0034160614,0.051818848,-0.026138306,-0.013427734,0.016326904,-0.015022278,-0.04046631,0.005203247,-0.010116577,0.009521484,-0.0005912781,0.037139893,0.058898926,0.030517578,0.012954712,0.041412354,0.0037403107,-0.035491943,0.023544312,-0.03074646,-0.0118255615,0.045898438,-0.016677856,-0.015960693,-0.023895264,-0.0022621155,-0.00774765,0.02720642,0.0043754578,0.035736084,0.024841309,-0.014251709,0.055603027,0.0039901733,0.06384277,-0.0055007935,-0.028152466,-0.0037403107,0.047088623,-0.0039024353,0.0057678223,0.053222656,0.0062408447,0.002986908,-7.390976e-05,0.0115356445,-0.022125244,-0.02897644,0.009521484,0.020935059,0.014190674,0.03878784,0.035736084,0.006504059,0.007926941,0.025314331,-0.03857422,-0.005470276,-0.012359619,-0.047088623,0.037841797,0.041870117,0.06768799,-0.018447876,-0.0056495667,-0.033355713,-0.024490356,0.0027503967,0.074279785,0.046844482,0.016921997,-0.008460999,-0.0076293945,0.027328491,-0.02909851,0.030166626,0.04849243,0.034301758,0.028503418,0.001168251,-0.048736572,0.031463623,0.043060303,0.030517578,-0.015609741,0.020233154,0.03286743,0.0463562,0.008934021,0.033599854,0.0023212433,-0.024017334,0.044006348,-0.008987427,-0.019866943,-0.037384033,0.015258789,-0.026138306,-0.013191223,0.0075683594,-0.009933472,-0.058441162,0.029571533,0.06384277,-0.018569946,-0.012954712,0.039520264,-0.024246216,0.016677856,-0.0309906,-0.048736572,0.051818848,-0.051818848,-0.024368286,0.053466797,0.006652832,-0.018096924,-0.017868042,-0.047546387,0.036193848,-0.009643555,0.0024986267,0.020706177,-0.010940552,-0.008636475,0.014785767,0.019515991,-0.0154953,-0.014251709,0.047088623,-0.020233154,-0.02128601,0.041870117,0.03074646,-0.008163452,-0.033355713,-0.01928711,0.03668213,0.0009908676,-0.0619812,0.02696228,-0.066711426,-0.074279785,-0.051818848,0.08282471,0.026733398,0.028503418,-0.005558014,-0.070007324,0.049682617,-0.04046631,-0.013900757,-0.004760742,-0.013244629,-0.025436401,0.031707764,0.022232056,0.0069503784,-0.0038433075,0.020584106,-0.021881104,-0.0036067963,-0.029327393,0.029922485,0.011886597,-0.04849243,-0.0032520294,0.023773193,0.054870605,0.083740234,-0.036193848,-0.016921997,-0.022354126,-0.048034668,-0.03643799,0.051086426,-0.005382538,-0.05038452,0.013954163,0.018692017,0.012535095,-0.013427734,0.0055885315,0.007423401,0.053466797,0.0056495667,-0.049682617,0.044708252,-0.05441284,-0.026260376,-0.008811951,0.04660034,-0.0075683594,-0.0021438599,-0.029449463,-0.06149292,-0.0064468384,-0.05227661,0.00022912025,0.011238098,-0.0211792,-0.03074646,-0.030517578,-0.007095337,0.022476196,0.021759033,0.0008687973,0.022476196,-0.031707764,0.0071258545,0.037384033,-0.031463623,-0.0052337646,0.007926941,0.024841309,-0.01928711,0.010826111,-0.035247803,-0.0309906,-0.045898438,0.015960693,-0.012184143,-0.010292053,-0.015022278,-0.021530151,-0.05038452,-0.005794525,0.022598267,0.007926941,0.02720642,-0.032409668,0.020111084,-0.0118255615,-0.048034668,0.05392456,-0.016921997,0.044952393,0.0053215027,0.024597168,0.009819031,0.009757996,0.04257202,0.010467529,0.034301758,-0.037628174,0.028625488,-0.038330078,-0.047302246,-0.04824829,-0.041412354,0.016799927,-0.013664246,-0.032409668,0.024017334,0.0423584,0.02897644,-0.0028095245,-0.056549072,-0.009048462,-0.0013895035,0.052520752,-0.0036373138,-0.043518066,0.022705078,-0.012481689,0.031219482,0.0118255615,-0.009170532,-0.023422241,0.033599854,0.045898438,-0.034057617,-0.037841797,-0.016921997,0.027801514,-0.01939392,0.035736084,-0.0068588257,0.04849243,0.015731812,0.0019369125,0.01259613,0.06530762,0.037841797,-0.034057617,-0.016555786,0.012420654,-0.016921997,-0.03479004,-0.041168213,0.033355713,0.048980713,-0.0619812,-0.01928711,-0.028152466,-0.013839722,0.020462036,0.039733887,0.006832123,0.049194336,0.0154953,-0.035491943,-0.0011529922,0.02720642,-0.0076293945,0.055358887,0.057250977,-0.009284973,0.028030396,0.0034294128,-0.0022182465,-0.025436401,0.016799927,0.028503418,0.035491943,0.029220581,0.060333252,-0.0053520203,-0.042114258,-0.003238678,0.008277893,0.038085938,0.03265381,0.017501831,0.011116028,0.00869751,-0.0025730133,0.009223938,-0.009109497,-0.0345459,-0.00020051003,-0.051574707,0.011177063,0.0018920898,0.0031642914,0.037628174,-0.01360321,-0.01158905,0.022476196,0.007686615,-0.03479004,0.030517578,-0.05203247,-0.049194336,-0.026855469,-0.036895752,0.03074646,-0.0031929016,-0.068115234,0.012245178,0.044006348,0.0075683594,-0.033843994,-0.035949707,0.024963379,0.009521484,-0.024017334,-0.029800415,-0.0075416565,0.015960693,-0.061035156,-0.0006580353,4.7147274e-05,-0.006149292,0.0105896,-0.026733398,-0.047790527,0.02885437,-0.022476196,0.041168213,0.029922485,-0.005558014,0.030044556,0.0690918,-0.016677856,-0.0032234192,-0.005027771,-0.064819336,0.0107040405,0.0005173683,0.0052948,-0.05984497,-0.031707764,0.051574707,-0.036895752,0.025543213,0.027328491,-0.005973816,-0.009109497,0.031921387,-0.014846802,0.011238098,0.056549072,0.0072135925,-0.028747559,-0.008163452,-0.033843994,0.06573486,-0.0015525818,0.04849243,-0.042114258,0.026138306,-0.009994507,0.0020694733,-0.009223938,-0.097961426,-0.0031929016,0.01348114,0.016677856,-0.019515991,0.0028533936,0.009407043,-0.018692017,-0.062927246,0.021652222,0.0014781952,0.01348114,-0.01537323,-0.022125244,-0.021057129,-0.03265381,-0.024841309,-0.045410156,0.026489258,0.016921997,-0.018447876,0.016204834,-0.01727295,0.0036964417,0.004108429,0.047302246,-0.0018033981,0.035003662,-0.010940552,0.016204834,0.012123108,0.0309906,-0.02897644,-0.0048217773,0.019760132,0.0211792,0.023895264,0.035247803,0.004940033,-0.02519226,0.021881104,-0.0541687,0.019042969,-0.026016235,0.015609741,-0.0047035217,0.012420654,-0.10595703,0.056549072,-0.009170532,-0.0013456345,-0.0017671585,-0.026855469,-0.06530762,-0.039031982,-0.057250977,-0.0031929016,-0.008041382,0.021057129,-0.011711121,-0.0020256042,0.022125244,0.013954163,-0.01939392,0.0057678223,0.006122589,-0.013542175,0.05203247,0.0056762695,0.01939392,-0.023895264,0.04046631,0.015609741,-0.03265381,-0.018096924,-0.017868042,0.019989014,0.021408081,-0.0137786865,0.017745972,0.0032672882,-0.04046631,-0.033599854,-0.002986908,0.005264282,-0.010528564,-0.010353088,-0.022827148,-0.035491943,-0.06384277,0.010528564,-0.013305664,-0.043060303,-0.0309906,0.005886078,0.017028809,-0.035491943,0.027328491,0.025787354,-0.00053596497,-0.021408081,-0.0076293945,-0.044952393,0.013130188,0.012954712,-0.035736084,0.037384033,0.0031929016,0.009880066,-0.0042877197,0.00674057,-0.025314331,0.043060303,-0.033355713,-0.011001587,-0.020935059,-0.008811951,0.019866943,0.0021743774,0.038330078,-0.022125244,-0.05441284,0.0061798096,0.0002901554,-0.07476807,0.007156372,0.013900757,-0.012184143,-0.021408081,0.007926941,0.021759033,0.051818848,-0.009933472,-0.015731812,0.024490356,-0.0231781,0.0044670105,-0.0071258545,0.074279785,0.006122589,0.030517578,0.004020691,-0.0847168,-0.008636475,0.013191223,-0.055603027,0.010292053,0.06719971,0.024368286,0.02897644,0.016082764,0.050872803,0.07098389,0.0015230179,-0.061035156,-0.04046631,-0.020111084,0.0345459,-0.0033111572,-0.064331055,0.037384033,0.062438965,-0.044006348,-0.006061554,-0.009407043,-0.0017595291,0.052978516,0.021408081,0.0035934448,-0.043762207,0.011947632,0.010467529,0.00484848,-0.019989014,0.008277893,0.0023212433,0.039276123,-0.04446411,0.033599854,0.008041382,0.0847168,-0.004020691,-0.062438965,-0.014190674,0.047790527,0.010406494,-0.015022278,-0.006477356,-0.0309906,-0.0071868896,0.009109497,0.022949219,0.00024211407,0.086120605,0.002632141,-0.0057678223,0.0154953,-0.012832642,0.016677856,-0.015853882,-0.016204834,-0.0025577545,-0.05606079,0.043060303,-0.008811951,0.019760132,-0.015853882,-0.024963379,0.017150879,-0.009880066,0.041168213,-0.010292053,0.013717651,0.024719238,0.0072746277,0.026733398,0.045410156,0.0006175041,0.023895264,-0.037628174,0.0054397583,-0.03668213,0.010643005,0.007423401,0.0018854141,0.007686615,-0.006477356,0.016204834,-0.032409668,-0.022125244,-0.018692017,0.05203247,-0.044006348,0.029693604,-0.0003142357,-0.008872986,0.014015198,-0.0463562,0.046844482,0.014373779,0.014663696,-0.006652832,0.026855469,0.046142578,-0.015022278,0.0028533936,0.055114746,-0.041168213,0.04849243,0.049438477,0.053222656,-0.013427734,0.013900757,-0.006210327,-0.041412354,0.005706787,0.0063591003,0.022705078,-0.003578186,-0.026733398,0.00969696,-0.0032081604,0.080444336,0.023422241,-0.012893677,0.03668213,-0.029220581,0.051574707,-0.0011463165,0.07476807,-0.02897644,0.04425049,0.009460449,-0.026138306,-0.0146102905,-0.007095337,-0.00040841103,0.016082764,-0.03286743,0.04257202,0.02720642,0.012008667,-0.066711426,-0.022125244,-0.014373779,0.0024108887,0.019165039,-0.0118255615,-0.046142578,0.035247803,0.0231781,0.01928711,0.019760132,0.042816162,0.009407043,-0.019515991,-0.008277893,-0.0069770813]", "client_status_at": "2025-10-02T09:52:43.443071Z", "server_status_at": "2025-10-02T09:52:43.443056Z", "created_at": "2025-10-02T08:52:39.015741Z"}, {"id": "e23f5217-427c-4f5a-95a8-b1654525b345", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://machinelearning.apple.com/research/fast-vision-language-models", "canonical_url": "https://machinelearning.apple.com/research/fast-vision-language-models", "title": "FastVLM: Efficient Vision Encoding for Vision Language Models", "source_site": "Apple Machine Learning Research", "publication_date": "2022-06-07T00:00:00Z", "favicon_url": "https://machinelearning.apple.com/favicon.ico", "content_markdown": "Vision Language Models (VLMs) enable visual understanding alongside textual inputs. They are typically built by passing visual tokens from a pretrained vision encoder to a pretrained Large Language Model (LLM) through a projection layer. By leveraging the rich visual representations of the vision encoder and the world knowledge and reasoning capabilities of the LLM, VLMs can be useful for a wide range of applications, including accessibility assistants, UI navigation, robotics, and gaming.\n\n**Update*** - September 22, 2025:*\n\n*FastVLM models, including checkpoints for MLX and CoreML, are available on HuggingFace*[here](https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e).*HuggingFace has also shared a FastVLM demo that works in real-time directly in the browser, powered by transformers.js and WebGPU, available*[here](https://huggingface.co/spaces/apple/fastvlm-webgpu). Videos of the demo in action are available in[this thread](https://x.com/xenovacom/status/1961454543503344036).*MobileCLIP2 is now also available on HuggingFace,*[here](https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47). MobileCLIP2 models are a family of image-text models that can be used as the image encoder in fast VLM models such as FastVLM. For more detail, please see the paper:[MobileCLIP2: Improving Multi-Modal Reinforced Training](/research/mobileclip2), which has been accepted to[Transactions on Machine Learning Research](https://jmlr.org/tmlr/papers/#)with Featured certification.\n\nVLM accuracy generally improves with higher input image resolution, creating a tradeoff between accuracy and efficiency. For many production use-cases, VLMs need to be both accurate and efficient to meet the low-latency demands of real-time applications and run on-device for privacy-preserving AI experiences.\n\nIn a [paper](https://machinelearning.apple.com/research/fastvlm-efficient-vision-encoding) accepted to CVPR 2025, Apple ML researchers recently shared a new technique to address this challenge: FastVLM, a new type of VLM that significantly improves accuracy-latency trade-offs with a simple design. Leveraging a hybrid architecture visual encoder designed for high-resolution images, FastVLM delivers accurate, fast, and efficient visual query processing, making it suitable for powering real-time applications on-device. The inference code, model checkpoints, and an iOS/macOS demo app based on [MLX](https://opensource.apple.com/projects/mlx/) are available [here](https://github.com/apple/ml-fastvlm/).\n\n## Image Resolution and the Accuracy-Latency Tradeoff\n\nGenerally, VLM accuracy improves with higher image resolution, especially for tasks needing detailed understanding, such as document analysis, UI recognition, or answering natural language queries about images. For example, in [Figure 1](#figure1) below, we ask our VLM about the street sign visible in the image. On the left, the model receives a low-resolution image and cannot respond correctly. On the right, the VLM receives a high-resolution image and correctly identifies the traffic sign which is a “Do Not Enter”.\n\nHigh resolution significantly increases time-to-first-token in VLMs. While using high-resolution images improves accuracy, it also reduces efficiency in two ways: 1) higher resolution images take longer for the vision encoder to process, and 2) the encoder creates more visual tokens, which increases the pre-filling time for the LLM. Both factors increase the time-to-first-token (TTFT), which is the sum of vision encoding time and LLM pre-filling time. As shown in [Figure 2](#figure2) below, both vision encoding and LLM pre-filling times grow as image resolution increases, and at high resolutions, vision encoder latency becomes the dominant bottleneck. To address this, our research introduces FastVLM, a new vision language model that significantly improves efficiency without sacrificing accuracy.\n\n## Hybrid Vision Encoders Deliver the Best Accuracy-Latency Tradeoff\n\nTo identify which architecture delivers the best accuracy-latency tradeoff, we systematically compared existing pre-trained vision encoders with an experiment in which everything (training data, recipe, LLM, etc.) was kept the same, and only the vision encoder was changed. In [Figure 3](#figure3) below, the x-axis shows TTFT, and the y-axis shows the average accuracy across different VLM tasks. We show two points for popular transformer-based encoders, [ViT-L/14](https://arxiv.org/abs/2103.00020)and [SigLIP-SO400](https://arxiv.org/abs/2303.15343), pre-trained on image-text data at their native resolutions. We also show curves for [ConvNeXT](https://arxiv.org/abs/2405.15738)(fully convolutional encoder) and FastViT (a hybrid encoder combining convolutional and transformer blocks) at various resolutions. FastViT, which is based on two of our previous works ([FastViT](https://machinelearning.apple.com/research/fastvit), ICCV 2023; and [MobileCLIP](https://machinelearning.apple.com/research/mobileclip), CVPR 2024), achieves the best accuracy-latency trade-off compared to other vision encoders—about 8 times smaller and 20 times faster than ViT-L/14.\n\n## FastViTHD: An Optimal Vision Encoder for VLMs\n\nWhile the FastViT hybrid backbone is a great choice for efficient VLMs, larger vision encoders are needed for improved accuracy on challenging tasks. Initially, we simply increased the size of each FastViT layer. However, this naive scaling made FastViT even less efficient than fully convolutional encoders at higher resolutions. To address this, we designed a new backbone, FastViTHD, specifically for high-resolution images. FastViTHD includes an extra stage compared to FastViT and is pre-trained using the MobileCLIP recipe to produce fewer but higher-quality visual tokens.\n\nFastViTHD has better latency at high resolution images compared to FastViT, but to evaluate which is best in a VLM, we compared their performance when combined with LLMs of various sizes. We evaluated different pairs of (image resolution, LLM size), and three LLMs with 0.5B, 1.5B, and 7B parameters (corresponding to each curve in [Figure 4](#figure4) below) and pair it with vision backbone running at different resolutions.\n\nAs shown in [Figure 4](#figure4), using very high resolution images with a small LLM is not always the optimal choice; sometimes it is better to switch the LLM to a larger one instead of increasing the resolution. For each case, we show the Pareto-optimal curve with dashed lines, which shows the optimal (image resolution, LLM size) for a given runtime budget (TTFT here). Comparing Pareto-optimal curves, FastVLM (based on FastViTHD) offers a much better accuracy-latency trade-off than the FastViT-based model. It can be up to 3x faster for the same accuracy. Note that we had already shown that FastViT is significantly better than purely transformer-based or convolutional-based encoders.\n\n## FastVLM: a New VLM Based on FastViTHD\n\nFastViTHD is a hybrid convolutional-transformer architecture comprising a convolutional stem, three convolutional stages, and two subsequent stages of transformer blocks. Each stage is preceded by a patch embedding layer that reduces the spatial dimensions of the input tensor by a factor of two. Using FastViTHD as the vision encoder, we built FastVLM, with a simple Multi-Layer Perceptron (MLP) module to project visual tokens to the embedding space of LLM, as shown in [Figure 5](#figure5).\n\n## FastVLM Outperforms Token Pruning and Merging Methods\n\nPrior research works in accelerating VLMs have employed complex merging or pruning techniques to reduce visual token counts to speed up LLM prefilling (and thus reduce the time to first token). As shown in [Figure 6](#figure6) below, FastVLM achieves higher overall accuracy across different visual token counts (corresponding to different input resolutions) compared to these approaches. This is due to the high-quality visual tokens from its FastViTHD encoder, and because FastVLM does not require complicated token pruning or merging, it is simpler to deploy.\n\n## FastVLM and Dynamic Tiling\n\nAs noted earlier, VLM accuracy increases with input resolution, particularly for tasks requiring understanding fine-grain details. Dynamic tiling (for example, in [AnyRes](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.html)) is a popular way to handle very high-resolution images. This approach divides an image into smaller tiles, processes each tile separately through the vision encoder, and then sends all tokens to the LLM, as in [Figure 7](#figure7) shown below.\n\nSince FastVLM naturally handles high-resolution images, we explored if combining FastVLM with dynamic tiling improves its accuracy-latency tradeoff. [Figure 8](#figure8) below shows that FastVLM without tiling (blue curve) achieves a better accuracy-latency trade-off compared to dynamic tiling (pink points), up to very high image resolutions, at which point combing FastVLM and AnyRes can be beneficial.\n\n## FastVLM is Faster and More Accurate Than Popular VLMs of the Same Size\n\nFinally, we compared FastVLM with other popular VLMs. In [Figure 9](#figure9) below, we show two curves for FastVLM: one with AnyRes (to achieve the highest accuracy) and one without tiling (for the best accuracy-latency tradeoff), each tested with three different LLM sizes. FastVLM is significantly faster and more accurate than popular models of the same size as indicated by the arrows: it is 85x faster than [LLava-OneVision](https://arxiv.org/abs/2408.03326)(0.5B LLM), 5.2x faster than [SmolVLM](https://arxiv.org/abs/2504.05299v1)(~0.5B LLM), and 21x faster than [Cambrian-1](https://arxiv.org/abs/2406.16860)(7B LLM).\n\nTo further show the on-device efficiency of FastVLM, we released an [iOS/macOS demo app](https://github.com/apple/ml-fastvlm/tree/main/app) based on MLX. [Figure 10](#figure10) shows examples of FastVLM running locally on an iPhone GPU. FastVLM’s near real-time performance can enable new on-device features and experiences.\n\n## Conclusion\n\nBy combining visual and textual understanding, VLMs can power a range of useful applications. Because the accuracy of these models generally corresponds to the resolution of input images, there has often been a performance tradeoff between accuracy and efficiency, which has limited the value of VLMs for applications that require both high accuracy and great efficiency.\n\nFastVLM addresses this tradeoff by leveraging a hybrid-architecture vision encoder built for high-resolution images, FastViTHD. With a simple design, FastVLM outperforms prior approaches in both accuracy and efficiency, enabling on-device visual query processing suitable for real-time on-device applications.\n\n## Related readings and updates.\n\n\nFastVLM: Efficient Vision encoding for Vision Language Models\n\nApril 18, 2025[research area Computer Vision](/research/?domain=Computer%20Vision)[conference CVPR](/research/?event=CVPR)\n\nScaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing…\n\n[Read more](/research/fastvlm-efficient-vision-encoding)\n\n\nA Multi-Task Neural Architecture for On-Device Scene Analysis\n\nJune 7, 2022[research area Computer Vision](/highlights?domain=Computer%20Vision), [research area Methods and Algorithms](/highlights?domain=Methods%20and%20Algorithms)\n\nScene analysis is an integral core technology that powers many features and experiences in the Apple ecosystem. From visual content search to powerful memories marking special occasions in one’s life, outputs (or “signals”) produced by scene analysis are critical to how users interface with the photos on their devices. Deploying dedicated models for each of these individual features is inefficient as many of these models can benefit from sharing resources. We present how we developed Apple Neural Scene Analyzer (ANSA), a unified backbone to build and maintain scene analysis workflows in production. This was an important step towards enabling Apple to be among the first in the industry to deploy fully client-side scene analysis in 2016.\n\n[Read more](/research/on-device-scene-analysis)", "content_text": "Vision Language Models (VLMs) enable visual understanding alongside textual inputs. They are typically built by passing visual tokens from a pretrained vision encoder to a pretrained Large Language Model (LLM) through a projection layer. By leveraging the rich visual representations of the vision encoder and the world knowledge and reasoning capabilities of the LLM, VLMs can be useful for a wide range of applications, including accessibility assistants, UI navigation, robotics, and gaming.\nUpdate - September 22, 2025:\n- FastVLM models, including checkpoints for MLX and CoreML, are available on HuggingFace here.\n- HuggingFace has also shared a FastVLM demo that works in real-time directly in the browser, powered by transformers.js and WebGPU, available here. Videos of the demo in action are available in this thread.\n- MobileCLIP2 is now also available on HuggingFace, here. MobileCLIP2 models are a family of image-text models that can be used as the image encoder in fast VLM models such as FastVLM. For more detail, please see the paper: MobileCLIP2: Improving Multi-Modal Reinforced Training, which has been accepted to Transactions on Machine Learning Research with Featured certification.\nVLM accuracy generally improves with higher input image resolution, creating a tradeoff between accuracy and efficiency. For many production use-cases, VLMs need to be both accurate and efficient to meet the low-latency demands of real-time applications and run on-device for privacy-preserving AI experiences.\nIn a paper accepted to CVPR 2025, Apple ML researchers recently shared a new technique to address this challenge: FastVLM, a new type of VLM that significantly improves accuracy-latency trade-offs with a simple design. Leveraging a hybrid architecture visual encoder designed for high-resolution images, FastVLM delivers accurate, fast, and efficient visual query processing, making it suitable for powering real-time applications on-device. The inference code, model checkpoints, and an iOS/macOS demo app based on MLX are available here.\nImage Resolution and the Accuracy-Latency Tradeoff\nGenerally, VLM accuracy improves with higher image resolution, especially for tasks needing detailed understanding, such as document analysis, UI recognition, or answering natural language queries about images. For example, in Figure 1 below, we ask our VLM about the street sign visible in the image. On the left, the model receives a low-resolution image and cannot respond correctly. On the right, the VLM receives a high-resolution image and correctly identifies the traffic sign which is a “Do Not Enter”.\nHigh resolution significantly increases time-to-first-token in VLMs. While using high-resolution images improves accuracy, it also reduces efficiency in two ways: 1) higher resolution images take longer for the vision encoder to process, and 2) the encoder creates more visual tokens, which increases the pre-filling time for the LLM. Both factors increase the time-to-first-token (TTFT), which is the sum of vision encoding time and LLM pre-filling time. As shown in Figure 2 below, both vision encoding and LLM pre-filling times grow as image resolution increases, and at high resolutions, vision encoder latency becomes the dominant bottleneck. To address this, our research introduces FastVLM, a new vision language model that significantly improves efficiency without sacrificing accuracy.\nHybrid Vision Encoders Deliver the Best Accuracy-Latency Tradeoff\nTo identify which architecture delivers the best accuracy-latency tradeoff, we systematically compared existing pre-trained vision encoders with an experiment in which everything (training data, recipe, LLM, etc.) was kept the same, and only the vision encoder was changed. In Figure 3 below, the x-axis shows TTFT, and the y-axis shows the average accuracy across different VLM tasks. We show two points for popular transformer-based encoders, ViT-L/14and SigLIP-SO400, pre-trained on image-text data at their native resolutions. We also show curves for ConvNeXT(fully convolutional encoder) and FastViT (a hybrid encoder combining convolutional and transformer blocks) at various resolutions. FastViT, which is based on two of our previous works (FastViT, ICCV 2023; and MobileCLIP, CVPR 2024), achieves the best accuracy-latency trade-off compared to other vision encoders—about 8 times smaller and 20 times faster than ViT-L/14.\nFastViTHD: An Optimal Vision Encoder for VLMs\nWhile the FastViT hybrid backbone is a great choice for efficient VLMs, larger vision encoders are needed for improved accuracy on challenging tasks. Initially, we simply increased the size of each FastViT layer. However, this naive scaling made FastViT even less efficient than fully convolutional encoders at higher resolutions. To address this, we designed a new backbone, FastViTHD, specifically for high-resolution images. FastViTHD includes an extra stage compared to FastViT and is pre-trained using the MobileCLIP recipe to produce fewer but higher-quality visual tokens.\nFastViTHD has better latency at high resolution images compared to FastViT, but to evaluate which is best in a VLM, we compared their performance when combined with LLMs of various sizes. We evaluated different pairs of (image resolution, LLM size), and three LLMs with 0.5B, 1.5B, and 7B parameters (corresponding to each curve in Figure 4 below) and pair it with vision backbone running at different resolutions.\nAs shown in Figure 4, using very high resolution images with a small LLM is not always the optimal choice; sometimes it is better to switch the LLM to a larger one instead of increasing the resolution. For each case, we show the Pareto-optimal curve with dashed lines, which shows the optimal (image resolution, LLM size) for a given runtime budget (TTFT here). Comparing Pareto-optimal curves, FastVLM (based on FastViTHD) offers a much better accuracy-latency trade-off than the FastViT-based model. It can be up to 3x faster for the same accuracy. Note that we had already shown that FastViT is significantly better than purely transformer-based or convolutional-based encoders.\nFastVLM: a New VLM Based on FastViTHD\nFastViTHD is a hybrid convolutional-transformer architecture comprising a convolutional stem, three convolutional stages, and two subsequent stages of transformer blocks. Each stage is preceded by a patch embedding layer that reduces the spatial dimensions of the input tensor by a factor of two. Using FastViTHD as the vision encoder, we built FastVLM, with a simple Multi-Layer Perceptron (MLP) module to project visual tokens to the embedding space of LLM, as shown in Figure 5.\nFastVLM Outperforms Token Pruning and Merging Methods\nPrior research works in accelerating VLMs have employed complex merging or pruning techniques to reduce visual token counts to speed up LLM prefilling (and thus reduce the time to first token). As shown in Figure 6 below, FastVLM achieves higher overall accuracy across different visual token counts (corresponding to different input resolutions) compared to these approaches. This is due to the high-quality visual tokens from its FastViTHD encoder, and because FastVLM does not require complicated token pruning or merging, it is simpler to deploy.\nFastVLM and Dynamic Tiling\nAs noted earlier, VLM accuracy increases with input resolution, particularly for tasks requiring understanding fine-grain details. Dynamic tiling (for example, in AnyRes) is a popular way to handle very high-resolution images. This approach divides an image into smaller tiles, processes each tile separately through the vision encoder, and then sends all tokens to the LLM, as in Figure 7 shown below.\nSince FastVLM naturally handles high-resolution images, we explored if combining FastVLM with dynamic tiling improves its accuracy-latency tradeoff. Figure 8 below shows that FastVLM without tiling (blue curve) achieves a better accuracy-latency trade-off compared to dynamic tiling (pink points), up to very high image resolutions, at which point combing FastVLM and AnyRes can be beneficial.\nFastVLM is Faster and More Accurate Than Popular VLMs of the Same Size\nFinally, we compared FastVLM with other popular VLMs. In Figure 9 below, we show two curves for FastVLM: one with AnyRes (to achieve the highest accuracy) and one without tiling (for the best accuracy-latency tradeoff), each tested with three different LLM sizes. FastVLM is significantly faster and more accurate than popular models of the same size as indicated by the arrows: it is 85x faster than LLava-OneVision(0.5B LLM), 5.2x faster than SmolVLM(~0.5B LLM), and 21x faster than Cambrian-1(7B LLM).\nTo further show the on-device efficiency of FastVLM, we released an iOS/macOS demo app based on MLX. Figure 10 shows examples of FastVLM running locally on an iPhone GPU. FastVLM’s near real-time performance can enable new on-device features and experiences.\nConclusion\nBy combining visual and textual understanding, VLMs can power a range of useful applications. Because the accuracy of these models generally corresponds to the resolution of input images, there has often been a performance tradeoff between accuracy and efficiency, which has limited the value of VLMs for applications that require both high accuracy and great efficiency.\nFastVLM addresses this tradeoff by leveraging a hybrid-architecture vision encoder built for high-resolution images, FastViTHD. With a simple design, FastVLM outperforms prior approaches in both accuracy and efficiency, enabling on-device visual query processing suitable for real-time on-device applications.\nRelated readings and updates.\nFastVLM: Efficient Vision encoding for Vision Language Models\nApril 18, 2025research area Computer Visionconference CVPR\nScaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing…\nA Multi-Task Neural Architecture for On-Device Scene Analysis\nJune 7, 2022research area Computer Vision, research area Methods and Algorithms\nScene analysis is an integral core technology that powers many features and experiences in the Apple ecosystem. From visual content search to powerful memories marking special occasions in one’s life, outputs (or “signals”) produced by scene analysis are critical to how users interface with the photos on their devices. Deploying dedicated models for each of these individual features is inefficient as many of these models can benefit from sharing resources. We present how we developed Apple Neural Scene Analyzer (ANSA), a unified backbone to build and maintain scene analysis workflows in production. This was an important step towards enabling Apple to be among the first in the industry to deploy fully client-side scene analysis in 2016.", "content_token_count": 2339, "client_status": "queued", "server_status": "embedded", "summary": "Apple Machine Learning Research introduces FastVLM, a new Vision Language Model that improves accuracy-latency trade-offs using a hybrid architecture visual encoder designed for high-resolution images. This model is efficient and suitable for real-time, on-device applications, with inference code and demo apps available on GitHub.", "expiry_score": 0.3, "mistral_embedding": "[-0.020751953,0.036590576,0.029724121,-0.018295288,0.05456543,0.014923096,0.026107788,-0.011947632,-0.0044288635,-0.011833191,-0.032165527,0.059448242,-0.05130005,0.008331299,-0.053375244,0.04849243,-0.008628845,0.04336548,0.04522705,-0.030776978,-0.024124146,-0.03894043,-0.07366943,0.01725769,0.03778076,-0.01713562,-0.026351929,-0.07879639,-0.04360962,-0.0010633469,-0.00019669533,-0.031707764,0.006526947,0.02180481,0.03451538,-0.036376953,0.015975952,-0.045684814,0.01876831,-0.008857727,-0.00055742264,0.031234741,0.020874023,-0.02319336,-0.022491455,-0.015731812,9.8347664e-05,0.02168274,0.024475098,0.012939453,-0.034729004,0.03427124,-0.00466156,0.038238525,-0.015151978,0.023544312,0.030776978,-0.017837524,-0.034729004,0.02331543,-0.03543091,-0.023086548,0.012184143,-0.011306763,0.033111572,0.034729004,0.040100098,-0.020401001,0.016204834,-0.0012969971,0.004283905,0.0107803345,-0.01247406,0.032409668,-0.0075187683,-0.036376953,-0.007637024,0.025878906,0.01713562,0.017364502,-0.007461548,0.009498596,0.056884766,-0.011131287,-0.008682251,0.019470215,-0.006412506,0.019348145,0.0011587143,0.018417358,0.055725098,0.040802002,0.022964478,-0.024719238,0.04220581,0.02949524,0.012763977,0.007865906,-0.008102417,0.041046143,0.028671265,-0.019577026,0.08441162,-0.00040245056,0.0076942444,-0.039642334,0.018066406,-0.032409668,-0.0048675537,-0.0592041,-0.095581055,-0.035217285,-0.0015296936,-0.07318115,0.009147644,0.015731812,-0.022964478,0.026229858,-0.05456543,-0.05783081,0.014923096,0.044525146,-0.0078086853,0.030548096,-0.026931763,-0.03286743,-0.003540039,0.04498291,-0.0076942444,0.010375977,-0.01159668,-0.045928955,-0.020629883,-0.030303955,0.023666382,-0.014045715,0.05734253,-0.02482605,-0.028793335,0.0008049011,0.03729248,-0.035217285,-0.022491455,-0.013343811,-0.03894043,-0.05432129,-0.021331787,0.020629883,0.0028705597,-0.01725769,-0.03567505,0.039398193,0.0496521,0.014045715,-0.016433716,-0.011192322,-0.05432129,-0.03543091,0.047546387,-0.012763977,-0.029373169,-0.0074005127,0.011077881,0.03869629,-0.02180481,-0.020980835,-0.00856781,0.012237549,-0.021453857,-0.005302429,0.016662598,-0.05130005,0.05130005,0.019348145,0.018997192,0.0340271,0.010604858,-0.01864624,0.056427002,0.046173096,-0.0031032562,-0.020980835,0.03286743,0.0038471222,0.007461548,-0.014923096,-0.0060920715,-0.016662598,-0.006790161,0.016555786,-0.045684814,-0.020751953,-0.046173096,0.03591919,-0.015975952,0.07318115,0.008163452,-0.02331543,0.015975952,0.01550293,0.04989624,0.012702942,-0.027862549,0.06530762,0.024475098,0.03286743,-0.01725769,0.01399231,0.0017557144,0.0034828186,0.01399231,0.015853882,-0.0053634644,-0.023895264,-0.025405884,-0.044311523,0.020050049,-0.0012454987,0.018188477,-0.013931274,-0.006526947,-0.022155762,-0.0027694702,0.0034675598,0.031707764,0.05456543,0.019577026,0.0037002563,-0.028564453,0.0067329407,-0.012878418,0.0033073425,-0.008743286,-0.039154053,-0.0037879944,-0.018417358,0.028564453,0.05105591,0.015853882,0.00932312,-0.011947632,0.017959595,-0.01247406,0.023895264,0.020629883,-0.013404846,-0.0340271,-0.03286743,-0.009674072,0.03778076,-0.061553955,-0.03729248,-0.08721924,-0.013580322,-0.041503906,-0.019699097,-0.0062942505,-0.07366943,0.041259766,-0.06390381,-0.04336548,0.008506775,-0.0052452087,0.01322937,0.020629883,-0.002521515,-0.050354004,0.000351429,0.074157715,0.010551453,0.0031757355,-0.041748047,0.036834717,-0.019470215,0.05596924,-0.019577026,0.003030777,0.00040626526,0.008041382,-0.008628845,-0.03265381,0.007637024,0.019927979,0.039154053,-0.022155762,-0.007865906,0.00737381,-0.0016098022,0.01474762,-0.040100098,0.0141067505,0.05432129,0.017959595,0.01550293,-0.028671265,-0.028320312,0.04034424,0.036834717,0.043823242,0.03567505,0.062469482,-0.039855957,-0.02798462,-0.00064468384,0.047332764,-0.03451538,-0.03778076,-0.031951904,-0.029373169,-0.053863525,0.0022010803,-0.016082764,-0.01159668,-0.007926941,-0.020751953,-0.025177002,-0.04220581,-0.038238525,0.00856781,-0.012123108,-0.034973145,-0.002609253,0.08532715,-0.012878418,0.039642334,-0.019577026,-0.009498596,-0.033569336,0.005302429,-0.016555786,0.024719238,-0.004108429,0.04336548,0.06201172,-0.01247406,0.013465881,0.043823242,0.031707764,-0.047088623,-0.005683899,0.0049552917,-0.015731812,0.014511108,-0.02798462,-0.0052452087,-0.029144287,-0.026229858,-0.028442383,0.005302429,0.013053894,0.043823242,-0.01474762,0.031234741,0.044769287,0.004108429,0.08068848,0.009269714,-0.056640625,0.005596161,0.030303955,-0.022262573,-0.008216858,0.021102905,0.013755798,0.014801025,-0.017959595,-0.01084137,0.03778076,0.0074310303,0.0033359528,0.009963989,0.033325195,0.039154053,0.04055786,0.02482605,0.008163452,0.03265381,-0.025177002,0.018066406,-0.028915405,-0.045928955,0.016433716,0.021102905,0.042663574,0.03729248,-0.01322937,-0.047546387,-0.011245728,-0.003162384,0.07879639,0.07879639,0.036590576,0.01550293,-0.013931274,0.0076942444,0.02798462,0.017715454,0.036834717,0.013931274,0.05014038,0.0055389404,-0.030303955,0.062927246,0.025650024,0.023773193,-0.039855957,0.02658081,0.031707764,0.03894043,0.002462387,0.015388489,0.013755798,-0.006439209,0.010902405,-0.00289917,0.025878906,-0.038482666,0.019699097,-0.006614685,-0.008857727,0.023086548,0.00856781,-0.044769287,0.01084137,0.036834717,-0.04055786,-0.010955811,-0.016555786,-0.0031471252,0.021453857,-0.036132812,-0.027038574,0.026931763,-0.030548096,-0.010437012,0.03427124,0.006587982,-0.021560669,-0.025177002,-0.010665894,0.009498596,-0.025405884,-0.01701355,0.036590576,-0.02809143,0.010314941,0.0051879883,0.03451538,-0.00504303,-0.028915405,0.03729248,-0.009963989,-0.031234741,0.062927246,0.046173096,-0.019119263,-0.048950195,-0.006877899,0.0078086853,0.002286911,0.003250122,0.0138168335,-0.055267334,-0.06109619,-0.072265625,0.08300781,0.012588501,0.036376953,-0.021102905,-0.0680542,0.0234375,-0.04055786,-0.030776978,-0.005302429,-0.009208679,0.0066719055,0.0680542,-0.01713562,-0.016662598,-0.012413025,0.06854248,-0.010437012,0.017608643,-0.0073432922,0.035217285,0.059692383,-0.055725098,-0.02645874,0.048034668,0.016555786,0.051513672,-0.039398193,-0.0099105835,0.00737381,-0.056427002,-0.036376953,-0.0043144226,-0.024719238,-0.041503906,0.007255554,0.010726929,-0.008041382,0.012184143,-0.002374649,-0.006439209,0.024719238,-0.005596161,-0.041503906,0.039398193,-0.07647705,-0.0015077591,0.010375977,0.011245728,-0.0060043335,0.0009689331,-0.010375977,-0.06341553,-0.016555786,-0.015037537,-0.027282715,-0.017608643,-0.019699097,-0.018188477,-0.009674072,0.03753662,0.033813477,-0.0031757355,-0.011077881,-0.0138168335,-0.046173096,-0.012237549,0.033569336,-0.027282715,-0.012649536,0.005771637,0.02482605,-0.008682251,0.041259766,-0.013290405,-0.0138168335,-0.06341553,0.04220581,-0.009208679,-0.02331543,-0.031463623,-0.0141067505,-0.033569336,-0.0071411133,0.0340271,0.014457703,0.03778076,-0.036132812,0.009849548,-0.0029582977,-0.05407715,0.012062073,-0.008216858,0.02949524,-0.016906738,-0.0031757355,0.019348145,-0.0008234978,0.04220581,0.00233078,0.034729004,-0.03567505,-0.0096206665,-0.021453857,-0.027633667,-0.023773193,-0.01713562,0.014801025,-0.01171875,-0.029022217,-0.0070228577,0.017959595,0.030776978,-0.010604858,-0.012649536,0.0070228577,-0.013465881,0.04336548,-0.0025501251,-0.022491455,0.043121338,0.01550293,0.034729004,0.012359619,0.008628845,-0.032409668,0.013053894,0.008453369,-0.026687622,-0.064819336,-0.019119263,0.01171875,0.006996155,0.058746338,0.02331543,0.048736572,-0.0061187744,0.013694763,0.027038574,0.04055786,0.059692383,-0.01864624,-0.060150146,-0.019241333,0.00737381,-0.060150146,-0.033325195,0.013000488,0.030303955,-0.060150146,-0.018539429,-0.033325195,0.005302429,0.03100586,0.026229858,0.047546387,0.047790527,0.025177002,-0.026809692,0.04522705,0.008743286,-0.025878906,0.03451538,0.033325195,0.011367798,-0.01701355,0.060150146,0.004371643,-0.032409668,0.0234375,-0.00012385845,0.015274048,0.04498291,0.057128906,-0.012939453,-0.007255554,-0.022964478,0.031707764,0.03286743,0.058288574,-0.013580322,0.004722595,0.0051879883,-0.044067383,-0.0024909973,0.04547119,-0.007751465,-0.021102905,-0.023086548,0.0060920715,-0.042663574,0.024017334,0.036132812,-0.003932953,-0.02180481,0.07086182,0.019470215,-0.045684814,0.024017334,-0.05596924,-0.016555786,-0.015853882,-0.0592041,0.04849243,-0.031707764,-0.054779053,-0.009384155,0.048950195,-0.0074310303,-0.050598145,0.0099105835,-0.02960205,0.027160645,-0.016555786,-0.06341553,-0.024719238,0.022155762,-0.045684814,0.00932312,-0.019119263,-0.04824829,0.006996155,-0.030548096,-0.04220581,0.028671265,-0.030548096,0.040802002,0.005332947,-0.015731812,0.049438477,0.047332764,0.008628845,-0.012062073,-0.021209717,-0.033569336,-0.0054512024,0.03894043,0.05407715,-0.051757812,-0.004924774,0.031707764,-0.047790527,0.033325195,0.01084137,0.008743286,-0.016082764,0.021453857,-0.009033203,0.0066719055,0.04638672,-0.002374649,-0.00944519,0.014633179,-0.072753906,0.10119629,-0.015731812,0.052001953,-0.089538574,0.01889038,0.01171875,-0.0003387928,-0.005302429,-0.061553955,-0.011306763,0.037994385,-0.00033521652,-0.006587982,0.03265381,0.0037593842,-0.0102005005,-0.07183838,-0.0016679764,-0.021209717,-0.018539429,3.8683414e-05,-0.0010566711,0.014045715,-0.014045715,-0.020050049,-0.055480957,0.015853882,0.015274048,-0.01701355,0.031463623,0.0043411255,0.030548096,0.0062675476,0.03543091,0.016906738,-0.018997192,0.0069084167,0.002418518,0.023544312,0.013519287,-0.029022217,0.022964478,0.021560669,-0.013114929,0.021209717,0.013290405,-0.012359619,-0.012062073,0.0022144318,-0.042663574,0.03265381,0.0012969971,0.03894043,0.026809692,0.027511597,-0.07879639,0.06854248,-0.014923096,0.025650024,0.020050049,0.002506256,-0.037994385,-0.04220581,-0.05291748,-0.006500244,-0.013343811,0.008216858,-0.021102905,0.029846191,-0.0023174286,0.0028705597,0.0023174286,0.038482666,0.014457703,-0.013931274,0.034729004,-0.027160645,-0.00088500977,-0.045928955,0.03567505,-0.010665894,-0.016555786,-0.0021705627,-0.029724121,0.024246216,0.015731812,-0.074157715,-0.013755798,0.005596161,-0.033569336,0.0032787323,0.009033203,-0.022155762,0.008918762,-0.008392334,-0.030776978,-0.042419434,-0.06903076,-0.03100586,-0.018188477,-0.029373169,-0.022842407,0.017959595,0.015731812,-0.03100586,0.041503906,0.00034070015,0.008506775,0.0008559227,0.020050049,-0.014335632,0.027633667,0.007575989,-0.02494812,0.043823242,0.0011434555,-0.01889038,-0.014862061,0.009147644,8.6545944e-05,0.026351929,-0.03451538,0.0016393661,0.003686905,0.013053894,0.024719238,0.023544312,0.021209717,-0.028213501,-0.039154053,0.01889038,0.00233078,-0.043121338,0.005332947,0.026229858,-0.013641357,0.0030021667,0.0054779053,0.011245728,0.022842407,-0.014862061,-0.013175964,0.01725769,-0.012878418,0.03100586,0.011245728,0.05783081,0.010902405,0.014335632,-0.006996155,-0.05899048,-0.0037155151,-0.015151978,-0.066223145,0.007255554,0.058532715,0.022964478,0.010261536,0.027160645,0.02507019,0.07598877,0.025527954,-0.035217285,-0.06573486,-0.0015153885,0.047790527,-0.0102005005,-0.08721924,0.022842407,0.04849243,-0.0018358231,-0.05899048,0.034973145,-0.016433716,0.04055786,0.036132812,0.009094238,-0.044067383,0.011245728,0.062927246,-0.01725769,-0.0066719055,-0.022613525,-0.008331299,0.0340271,-0.03451538,-0.004749298,0.02645874,0.08532715,0.03286743,-0.009674072,-0.008804321,0.03543091,-0.007637024,-0.04360962,-0.030776978,0.0049819946,-0.024719238,-0.023544312,0.023666382,0.0011148453,0.044769287,0.008857727,-0.01889038,0.022491455,-0.013519287,0.00097608566,0.020523071,-0.00021398067,-0.0065574646,-0.04034424,0.056427002,-0.011833191,0.031707764,0.010375977,-0.07366943,0.0029582977,0.0102005005,0.019927979,0.02798462,0.02180481,0.016555786,0.02494812,0.03451538,0.045928955,-0.026809692,0.021453857,-0.043823242,-0.0018291473,-0.042663574,0.01876831,0.015853882,3.7789345e-05,0.016326904,0.03543091,0.013641357,-0.02949524,-0.02949524,-0.036376953,0.042907715,-0.022262573,0.019241333,-0.0027694702,-0.019577026,0.017715454,-0.052459717,0.072265625,-0.00737381,0.040802002,-0.004722595,0.015731812,0.020751953,-0.000238657,-0.005683899,0.029846191,0.02949524,0.037078857,0.030548096,-0.009384155,-0.0496521,0.04360962,-0.02017212,-0.074157715,0.01171875,0.028915405,0.028442383,0.014633179,-0.0069351196,-0.0026378632,-0.029144287,0.06201172,-0.007255554,-0.042663574,-0.00087451935,-0.05316162,0.06341553,-0.0078086853,0.07318115,-0.013000488,0.011077881,0.011192322,-0.043823242,-0.021911621,0.006149292,-0.017837524,0.02017212,-0.03778076,0.05758667,0.0023174286,0.003583908,-0.062927246,-0.016204834,-0.011131287,0.014862061,0.0009288788,-0.02798462,-0.04034424,0.046173096,-0.001865387,0.017486572,0.008804321,0.044769287,0.004020691,-0.03286743,0.0037593842,0.0021705627]", "client_status_at": "2025-10-02T09:52:38.077234Z", "server_status_at": "2025-10-02T09:52:38.077213Z", "created_at": "2025-10-02T08:52:33.896164Z"}, {"id": "d7915f1b-d0de-4b94-b3ee-b786204e8aff", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://mistral.ai/news/codestral-embed", "canonical_url": null, "title": "Codestral Embed | Mistral AI", "source_site": "mistral.ai", "publication_date": "2025-01-17T00:00:00Z", "favicon_url": "https://mistral.ai/favicon.ico", "content_markdown": "# Codestral Embed\n\nThe new state-of-the-art embedding model for code.\n\nWe are excited to release Codestral Embed, our first embedding model specialized for code. It performs especially well for retrieval use cases on real-world code data.\n\nCodestral Embed significantly outperforms leading code embedders in the market today: Voyage Code 3, Cohere Embed v4.0 and OpenAI’s large embedding model.\n\nCodestral Embed can output embeddings with different dimensions and precisions, and the figure below illustrates the trade-offs between retrieval quality and storage costs. Codestral Embed with dimension 256 and int8 precision still performs better than any model from our competitors. The dimensions of our embeddings are ordered by relevance. For any integer target dimension n, you can choose to keep the first n dimensions for a smooth trade-off between quality and cost.\n\n### Results\n\nBelow we show the performance of Codestral Embed for several categories. The details of the benchmarks corresponding to each category can be found in the table in the “Benchmarks details” section.\n\nSWE-Bench is based on a dataset of real-world GitHub issues and corresponding fixes, and is especially relevant for retrieval-augmented generation for coding agents. Text2Code (GitHub) contains benchmarks relevant for giving context for code completion or edition. We believe that these two categories are especially relevant to code assistants.\n\n## Use cases\n\nCodestral Embed is optimized for high-performance code retrieval and semantic understanding. It enables a range of practical applications across development workflows, especially when working with large-scale code corpora.\n\n### 1. Retrieval-augmented generation\n\nCodestral Embed facilitates rapid and efficient context retrieval for code completion, editing, or explanation tasks. It is ideal for AI-powered software engineering in copilots or coding agent frameworks.\n\n### 2. Semantic code search\n\nEmbed enables accurate search of relevant code snippets from natural language or code queries. It is suitable for use within developer tools, documentation systems, and copilots.\n\n### 3. Similarity search and duplicate detection\n\nThe model’s embeddings can be used to identify near-duplicate or functionally similar code segments, even with significant lexical variation. This supports use cases such as identifying reusable code to avoid duplicates, or detecting copy-paste reuse to enforce licensing policies.\n\n### 4. Semantic clustering and code analytics\n\nCodestral Embed supports unsupervised grouping of code based on functionality or structure. This is useful for analyzing repository composition, identifying emergent architecture patterns, or feeding into automated documentation and categorization systems.\n\n## Availability\n\nCodestral Embed is available on our API under the name `codestral-embed-2505` at a price of $0.15 per million tokens. It is also available on our [batch API](https://docs.mistral.ai/capabilities/batch/) at a 50% discount. For on-prem deployments, please [contact us](https://mistral.ai/contact) to connect with our applied AI team.\n\nPlease check our [docs](https://docs.mistral.ai/capabilities/embeddings/code_embeddings/) to get started and our [cookbook ](https://colab.research.google.com/github/mistralai/cookbook/blob/main/mistral/embeddings/code_embedding.ipynb)for examples of how to use Codestral Embed for code agent retrieval.\n\n### Chunking parameters\n\nFor retrieval use cases, while you can use the full context size of 8192 tokens, it is often more efficient to chunk your dataset. We recommend using chunks of 3000 characters with 1000 characters overlap. Larger chunks tend to adversely affect the performance of the retrieval system. Refer to [our ](https://docs.mistral.ai/guides/rag/#split-document-into-chunks)[cookbook](https://colab.research.google.com/github/mistralai/cookbook/blob/main/mistral/embeddings/code_embedding.ipynb) for more information about chunking.\n\n## Benchmark details\n\nYou can find the details of the benchmarks that we used to evaluate our model in the table below. We report the average score per category, and the macro average (average of the scores of each category).\n\n| Benchmark | Description | Category |\n|---|---|---|\n| SWE-Bench lite | Examples from SWE-Bench lite: given real github issues, retrieve the files that should be modified to fix the issue from the given state of the repository. Most relevant for code agent RAG. |\nswebench_lite |\n|\nCodeSearchNet Code -> Code |\nGiven real-world code from GitHub, retrieve the code that appears in the same context |\ncode2code |\n|\nCodeSearchNet doc2code |\nGiven a docstring from real-world GitHub code, retrieve the corresponding code |\nText2code (github) |\n|\nCommitPack |\nGiven a commit message from real-world GitHub code, retrieve the corresponding modified files |\nText2code (github) |\n|\nSpider |\nRetrieve SQL code given a query |\nText2SQL |\n|\nWikiSQL |\nRetrieve SQL code given a query |\nText2SQL |\n|\nSynthetic Text2SQL |\nRetrieve SQL code given a query |\nText2SQL |\n|\nDM code contests |\nMatch problem descriptions to correct solutions for programming competition websites (corpus is correct + incorrect solutions for each problem). |\nText2Code (Algorithms) |\n|\nAPPS |\nMatch problem descriptions to solutions for programming competition websites. |\nText2Code (Algorithms) |\n|\nCodeChef |\nMatch problem descriptions to solutions for programming competition websites. |\nText2Code (Algorithms) |\n|\nMBPP+ |\nMatch algorithmic questions to solutions for mostly basic python programs |\nText2Code (Algorithms) |\n|\nDS 1000 |\nMatch data science questions to implementations |\nText2Code (Data Science) |", "content_text": "Codestral Embed\nThe new state-of-the-art embedding model for code.\nWe are excited to release Codestral Embed, our first embedding model specialized for code. It performs especially well for retrieval use cases on real-world code data.\nCodestral Embed significantly outperforms leading code embedders in the market today: Voyage Code 3, Cohere Embed v4.0 and OpenAI’s large embedding model.\nCodestral Embed can output embeddings with different dimensions and precisions, and the figure below illustrates the trade-offs between retrieval quality and storage costs. Codestral Embed with dimension 256 and int8 precision still performs better than any model from our competitors. The dimensions of our embeddings are ordered by relevance. For any integer target dimension n, you can choose to keep the first n dimensions for a smooth trade-off between quality and cost.\nResults\nBelow we show the performance of Codestral Embed for several categories. The details of the benchmarks corresponding to each category can be found in the table in the “Benchmarks details” section.\nSWE-Bench is based on a dataset of real-world GitHub issues and corresponding fixes, and is especially relevant for retrieval-augmented generation for coding agents. Text2Code (GitHub) contains benchmarks relevant for giving context for code completion or edition. We believe that these two categories are especially relevant to code assistants.\nUse cases\nCodestral Embed is optimized for high-performance code retrieval and semantic understanding. It enables a range of practical applications across development workflows, especially when working with large-scale code corpora.\n1. Retrieval-augmented generation\nCodestral Embed facilitates rapid and efficient context retrieval for code completion, editing, or explanation tasks. It is ideal for AI-powered software engineering in copilots or coding agent frameworks.\n2. Semantic code search\nEmbed enables accurate search of relevant code snippets from natural language or code queries. It is suitable for use within developer tools, documentation systems, and copilots.\n3. Similarity search and duplicate detection\nThe model’s embeddings can be used to identify near-duplicate or functionally similar code segments, even with significant lexical variation. This supports use cases such as identifying reusable code to avoid duplicates, or detecting copy-paste reuse to enforce licensing policies.\n4. Semantic clustering and code analytics\nCodestral Embed supports unsupervised grouping of code based on functionality or structure. This is useful for analyzing repository composition, identifying emergent architecture patterns, or feeding into automated documentation and categorization systems.\nAvailability\nCodestral Embed is available on our API under the name `codestral-embed-2505` at a price of $0.15 per million tokens. It is also available on our batch API at a 50% discount. For on-prem deployments, please contact us to connect with our applied AI team.\nPlease check our docs to get started and our cookbook for examples of how to use Codestral Embed for code agent retrieval.\nChunking parameters\nFor retrieval use cases, while you can use the full context size of 8192 tokens, it is often more efficient to chunk your dataset. We recommend using chunks of 3000 characters with 1000 characters overlap. Larger chunks tend to adversely affect the performance of the retrieval system. Refer to our cookbook for more information about chunking.\nBenchmark details\nYou can find the details of the benchmarks that we used to evaluate our model in the table below. We report the average score per category, and the macro average (average of the scores of each category).\n| Benchmark | Description | Category |\n|---|---|---|\n| SWE-Bench lite | Examples from SWE-Bench lite: given real github issues, retrieve the files that should be modified to fix the issue from the given state of the repository. Most relevant for code agent RAG. |\nswebench_lite |\n|\nCodeSearchNet Code -> Code |\nGiven real-world code from GitHub, retrieve the code that appears in the same context |\ncode2code |\n|\nCodeSearchNet doc2code |\nGiven a docstring from real-world GitHub code, retrieve the corresponding code |\nText2code (github) |\n|\nCommitPack |\nGiven a commit message from real-world GitHub code, retrieve the corresponding modified files |\nText2code (github) |\n|\nSpider |\nRetrieve SQL code given a query |\nText2SQL |\n|\nWikiSQL |\nRetrieve SQL code given a query |\nText2SQL |\n|\nSynthetic Text2SQL |\nRetrieve SQL code given a query |\nText2SQL |\n|\nDM code contests |\nMatch problem descriptions to correct solutions for programming competition websites (corpus is correct + incorrect solutions for each problem). |\nText2Code (Algorithms) |\n|\nAPPS |\nMatch problem descriptions to solutions for programming competition websites. |\nText2Code (Algorithms) |\n|\nCodeChef |\nMatch problem descriptions to solutions for programming competition websites. |\nText2Code (Algorithms) |\n|\nMBPP+ |\nMatch algorithmic questions to solutions for mostly basic python programs |\nText2Code (Algorithms) |\n|\nDS 1000 |\nMatch data science questions to implementations |\nText2Code (Data Science) |", "content_token_count": 1044, "client_status": "queued", "server_status": "embedded", "summary": "Mistral AI has released Codestral Embed, a state-of-the-art embedding model specialized for code that outperforms leading competitors in code retrieval and semantic understanding. It offers various use cases such as retrieval-augmented generation, semantic code search, and similarity search, and is available via API and batch API.", "expiry_score": 0.3, "mistral_embedding": "[-0.034973145,0.04208374,0.010322571,0.016708374,0.050720215,-0.016616821,0.009506226,-0.014411926,-0.025558472,0.0023536682,-0.018630981,0.066101074,-0.03515625,-0.018157959,-0.05493164,0.05380249,0.0395813,0.04840088,0.0395813,0.011383057,-0.029769897,-0.0061950684,-0.04321289,-0.0054512024,-0.016906738,-0.024307251,-0.021713257,-0.06185913,-0.032073975,-0.018249512,0.025741577,-0.027862549,0.00166893,0.010040283,-0.015274048,-0.018737793,0.007156372,-0.07183838,0.0064353943,-0.004081726,-0.023727417,0.042663574,-0.013160706,-0.033996582,-0.014793396,-0.032470703,-0.018157959,-0.012680054,0.01109314,-0.03475952,-0.012580872,0.045532227,9.679794e-05,-0.010902405,-0.019699097,0.0064849854,0.037261963,0.0077819824,-0.049194336,0.038604736,-0.049560547,0.0011463165,-0.025161743,-0.0032176971,0.030929565,0.02218628,-0.016998291,-0.0007505417,0.022094727,-0.018920898,0.01373291,-0.014503479,-0.02305603,0.05609131,-0.0064353943,-0.048797607,-0.013931274,0.016143799,0.01071167,-0.02267456,0.004875183,0.017196655,0.051879883,-0.014312744,-0.017196655,0.00040531158,0.024978638,0.016418457,0.008888245,0.030731201,0.027282715,0.015945435,0.00504303,-0.033996582,0.038604736,0.015464783,-0.030548096,0.0041542053,-0.011619568,0.037078857,0.05340576,-0.016418457,0.09069824,-0.051116943,0.0055236816,-0.03475952,0.018157959,-0.024017334,-0.009651184,-0.025558472,-0.07763672,-0.032287598,-0.0025691986,-0.058013916,-0.005859375,-0.0022563934,-0.012870789,0.06225586,-0.052246094,-0.056854248,0.0072517395,-0.0077819824,-0.014602661,-0.012779236,-0.05456543,-0.030548096,0.019012451,0.034576416,-0.013832092,-0.004753113,-0.023345947,-0.025161743,-0.019699097,-0.049194336,0.012779236,-0.02104187,0.022766113,-0.011238098,-0.0395813,-0.01737976,0.013252258,-0.029968262,-0.021224976,0.01171875,-0.009605408,0.0017528534,0.007106781,0.03765869,-0.04647827,-0.0061950684,0.007926941,0.036895752,0.07110596,0.0008945465,-0.007926941,-0.037078857,-0.01777649,0.0112838745,0.07067871,-0.02104187,-0.0030975342,-0.004299164,-0.015945435,0.014694214,-0.007347107,-0.025360107,-0.016036987,0.032073975,-0.017486572,0.003145218,0.049560547,-0.030349731,0.042266846,0.042266846,-0.04034424,0.029968262,0.0023899078,-0.030166626,0.041107178,0.0041542053,-0.0060043335,-0.0052604675,0.012870789,-0.00667572,0.039764404,-0.014694214,-0.044555664,0.0052337646,-0.006290436,0.013061523,-0.037475586,-0.0055236816,-0.036499023,0.046691895,-0.013832092,0.059936523,0.037475586,-0.021896362,0.028427124,-0.016326904,0.01335144,0.040527344,-0.019592285,0.0703125,0.01737976,0.007396698,-0.007926941,0.028244019,0.025161743,-0.020263672,-0.016525269,0.025939941,-0.003025055,-0.017196655,-0.026321411,-0.010276794,0.019119263,0.018630981,0.0002040863,-0.028244019,-0.0045394897,-0.049957275,0.0024852753,-0.010803223,0.016708374,0.053009033,0.040740967,-0.024017334,-0.024108887,-0.012779236,0.008453369,0.03842163,-0.010803223,-0.04208374,0.014312744,-0.025939941,0.015274048,0.029006958,0.012054443,0.012580872,-0.0010385513,0.04360962,-0.0042495728,0.045532227,0.017578125,-0.006340027,0.0018014908,-0.032287598,0.018737793,0.047058105,-0.04611206,-0.044006348,-0.059173584,0.0043945312,-0.08532715,-0.04360962,0.007396698,-0.0541687,0.044189453,-0.044769287,-0.035736084,0.01335144,0.0054512024,0.025360107,0.061462402,0.020462036,-0.041503906,0.011764526,0.057647705,-0.00091838837,-0.019592285,0.0024375916,0.029769897,-0.011528015,0.038024902,-0.0024967194,0.019302368,0.025360107,0.011909485,0.0059547424,-0.0060043335,0.036315918,0.012779236,0.061462402,-0.040924072,-0.018341064,-0.010276794,0.007587433,0.010375977,-0.03439331,0.0016565323,0.05609131,0.066467285,-0.013542175,-0.005355835,-0.06842041,0.029205322,0.0068206787,0.01777649,0.07147217,0.058410645,-0.0395813,-0.007156372,-0.01940918,0.046691895,-0.038604736,-0.009941101,-0.025741577,-0.01940918,-0.06726074,-0.021896362,0.013061523,-0.016906738,0.004081726,0.008354187,0.0015611649,-0.024017334,-0.036499023,0.00094270706,-0.022277832,-0.034210205,-0.029205322,0.07220459,-0.011817932,0.018341064,0.0002206564,0.017105103,-0.029388428,0.0064353943,-0.037841797,0.0060768127,0.0062446594,0.028427124,0.037475586,0.011138916,0.015365601,0.039764404,0.024780273,-0.032073975,-0.0032901764,-0.038238525,-0.029968262,0.01737976,-0.05380249,-0.012680054,-0.030166626,-0.03475952,-0.006149292,0.002149582,0.027664185,0.012870789,-0.010040283,-0.021606445,0.064575195,0.017288208,0.08874512,0.02218628,-0.023727417,-0.0007352829,0.043029785,-0.041870117,-0.0019330978,0.05340576,0.011955261,0.023239136,0.007827759,0.010612488,0.008071899,-0.0087890625,0.042663574,0.014411926,-0.0012245178,0.0021972656,0.0018491745,0.005569458,0.028823853,0.02267456,-0.024597168,0.0025806427,-0.015655518,-0.08453369,0.024490356,0.027862549,0.030548096,0.018341064,-0.023147583,-0.067993164,-0.018829346,0.00019061565,0.07336426,0.0848999,0.014694214,-0.015083313,0.0022449493,0.01940918,0.003578186,0.019973755,0.053009033,0.019973755,0.0039863586,0.021133423,-0.048797607,0.06298828,0.041503906,0.021331787,-0.0012311935,0.036499023,0.023635864,0.037261963,-0.008071899,0.053009033,0.03668213,-0.00085258484,0.018051147,-0.029205322,0.018829346,-0.040527344,-0.00055217743,-0.011482239,-0.0023174286,0.029968262,-0.018447876,-0.07147217,0.0031223297,0.03881836,-0.025939941,-0.009315491,0.036499023,-0.016418457,0.005859375,-0.025558472,0.0012483597,0.036499023,-0.021896362,-0.023910522,0.050323486,0.024978638,-0.044952393,-0.0016450882,-0.048980713,0.020080566,-0.0014104843,0.011047363,0.032073975,-0.024490356,0.025741577,0.015945435,0.05493164,0.0033130646,-0.00333786,0.028625488,-0.022003174,-0.015563965,0.04437256,0.038238525,-0.0023765564,-0.037475586,-0.029769897,-0.019973755,0.0052604675,-0.0027980804,0.019210815,-0.027664185,-0.07879639,-0.081848145,0.088012695,0.013641357,0.056854248,-0.0036735535,-0.06414795,0.040527344,-0.05340576,-0.011482239,-0.011817932,-0.01133728,-0.015174866,0.025558472,0.016326904,0.017578125,-0.010803223,0.028823853,-0.005569458,-0.009170532,-0.03668213,0.04763794,0.040924072,-0.058410645,-0.027282715,0.0042266846,0.0395813,0.03918457,-0.046875,-0.029769897,0.00065755844,-0.030731201,-0.0395813,0.0023651123,0.007587433,-0.041107178,0.026321411,0.024780273,0.0013504028,0.008453369,0.021713257,-0.0075416565,0.03112793,0.018447876,-0.03842163,0.05532837,-0.040527344,0.001572609,-0.0008225441,-0.00026106834,0.019302368,0.00064229965,-0.00166893,-0.025360107,-0.00091838837,-0.014602661,-0.008071899,-0.006099701,-0.024398804,-0.037261963,-0.01979065,0.0067253113,0.026901245,0.018341064,-0.021713257,0.04168701,-0.027862549,0.0055007935,-0.011138916,-0.005619049,0.046875,0.0057144165,0.042663574,-0.044555664,-0.0036258698,-0.033416748,-0.0061950684,-0.053009033,0.03189087,-0.03439331,-0.0001860857,-0.012199402,-0.023529053,-0.051483154,-0.0060272217,0.029205322,0.010520935,-0.011909485,-0.053009033,-0.004009247,-0.029769897,-0.029006958,0.016418457,-0.022964478,0.01737976,-0.014122009,0.036499023,0.012199402,-0.007637024,0.020462036,0.014122009,0.031707764,-0.046691895,0.01008606,-0.022476196,-0.017959595,-0.02670288,-0.0115737915,0.034210205,-0.036499023,-0.01109314,0.010902405,0.039398193,0.01737976,-0.0007805824,-0.017578125,-0.011909485,0.016235352,0.049194336,-0.0074920654,-0.05456543,0.01737976,-0.009025574,0.07879639,0.024307251,-0.009849548,-0.013160706,-0.0037708282,0.036895752,-0.024597168,-0.049194336,-0.013160706,0.047454834,-0.0074424744,0.03149414,-0.015464783,0.041290283,0.02708435,0.0020656586,0.02017212,0.04727173,0.058776855,-0.017868042,-0.056488037,-0.007637024,0.0087890625,-0.024398804,-0.036895752,0.026901245,0.066467285,-0.07952881,-0.034973145,-0.020751953,-0.009796143,0.033233643,0.019882202,0.0036735535,0.031707764,0.01777649,-0.012870789,0.026504517,-0.014312744,0.016815186,0.037078857,0.041107178,0.00026726723,0.019699097,0.02017212,-0.029205322,-0.02746582,0.015174866,0.014602661,0.012390137,0.03363037,0.07531738,-0.013832092,-0.014503479,-0.023910522,0.027664185,0.03994751,0.06298828,-0.0046844482,0.010231018,-0.0033855438,-0.020751953,-0.00667572,0.014022827,0.010665894,-0.005596161,-0.03515625,-0.008018494,-0.001572609,-0.0039138794,-0.0011167526,-0.010856628,-0.018920898,0.045532227,-0.0035057068,-0.040527344,0.04611206,-0.018829346,-0.016616821,-0.021331787,-0.07720947,0.07147217,-0.026504517,-0.08148193,-0.01979065,0.08453369,0.00566864,-0.03918457,-0.013160706,0.018051147,0.005138397,-0.0084991455,-0.044769287,0.0009784698,0.020263672,-0.06414795,0.009223938,0.0020179749,-0.007106781,-0.0042762756,-0.0014410019,-0.049560547,0.025558472,-0.022384644,0.033813477,-0.007205963,-0.02420044,0.026123047,0.029968262,-0.0093688965,0.0042037964,-0.004875183,-0.041107178,-0.030731201,0.011764526,0.02104187,-0.06262207,0.0042266846,0.053009033,-0.08105469,-0.005306244,0.022277832,-0.0054740906,-0.028427124,0.0044441223,0.0013751984,0.012779236,0.049560547,0.0062446594,-0.033233643,0.0012426376,-0.04763794,0.058013916,-0.001909256,0.044006348,-0.060699463,0.016235352,-5.77569e-05,-0.004634857,0.007873535,-0.09338379,0.0024261475,0.019210815,0.029205322,-0.04647827,0.047821045,-0.0070610046,-0.016998291,-0.02708435,0.030548096,-0.0035057068,0.002462387,-0.020080566,-0.02218628,-0.00092458725,-0.02746582,-0.011428833,-0.052246094,-0.0010328293,0.028625488,-0.009414673,0.0003466606,-0.032287598,0.014503479,0.0012664795,0.05493164,-0.013832092,0.0034828186,0.0019454956,0.011817932,0.013641357,0.010803223,-0.017486572,0.008308411,-0.012680054,-0.0115737915,0.03765869,0.03265381,-0.014884949,-0.017868042,0.030548096,-0.03881836,0.029006958,-0.0057411194,0.018737793,-0.02142334,0.0049476624,-0.08721924,0.06225586,0.009605408,0.014122009,-8.404255e-05,-0.052642822,-0.07800293,-0.037261963,-0.045928955,0.008743286,-0.019882202,0.016708374,0.002521515,-0.0090789795,0.015563965,-0.02305603,-0.018630981,0.042266846,0.013931274,-0.010231018,0.05340576,-0.034576416,-0.006580353,-0.007205963,0.03439331,-0.0011281967,-0.015853882,-0.0004954338,-0.015853882,0.018920898,0.016616821,-0.0541687,-0.02218628,0.0044441223,-0.024978638,0.013252258,0.014213562,0.013542175,0.0016326904,-0.0060768127,-0.0017652512,-0.028045654,-0.083740234,-0.022476196,-0.007396698,-0.03668213,-0.0064849854,-0.023727417,-0.011238098,-0.040527344,0.045715332,0.01777649,0.015174866,0.00035429,0.00667572,-0.015174866,0.02670288,-0.016525269,-0.0067253113,0.0042495728,0.0022449493,-0.030548096,-0.049957275,0.020080566,-0.008552551,0.029388428,-0.046875,-0.036102295,-0.0023536682,0.026123047,-0.00033473969,0.022766113,0.029006958,0.012969971,-0.053009033,0.014022827,-0.030731201,-0.03918457,0.0030975342,0.003145218,-0.013450623,-0.00045919418,0.034576416,0.017105103,0.07720947,0.012680054,-0.0024738312,0.009986877,-0.0028705597,0.005428314,4.1484833e-05,0.040161133,-0.002161026,0.030929565,-0.015365601,-0.07183838,0.015174866,-0.026321411,-0.064575195,0.017578125,0.07147217,0.022766113,0.012680054,0.023635864,0.033416748,0.08416748,0.012390137,-0.04321289,-0.05532837,-0.012199402,0.026123047,0.014213562,-0.07989502,0.0093688965,0.045715332,-0.01777649,-0.024490356,0.015174866,0.021606445,0.032287598,0.02017212,0.0076828003,-0.06994629,0.004058838,0.027664185,-0.0061950684,-0.022094727,-0.015945435,0.044769287,0.049560547,-0.049560547,-0.00040221214,0.0029773712,0.044952393,0.005355835,-0.03668213,-0.022766113,0.024780273,0.008407593,-0.037261963,-0.033233643,0.020935059,-0.027664185,-0.0049934387,0.0118637085,0.037841797,0.032470703,0.016815186,0.0016326904,0.014503479,0.011764526,0.00970459,0.013832092,-0.006340027,0.010902405,-0.047454834,0.07110596,-0.008308411,0.034576416,0.011428833,-0.033996582,0.009796143,-0.026321411,0.02670288,0.0015850067,0.032836914,0.028823853,-0.010131836,0.019699097,0.019592285,-0.020843506,0.035339355,-0.023147583,-0.01109314,-0.05380249,-0.0027618408,-0.0011167526,-0.029968262,0.023727417,0.013252258,0.02746582,-0.057647705,-0.02142334,-0.040527344,0.052246094,-0.07720947,0.033813477,-0.0017290115,-0.014213562,-0.015365601,-0.044769287,0.07684326,0.015274048,0.005882263,-0.011001587,-0.009895325,0.03842163,-0.016998291,-0.021896362,0.022857666,-0.014022827,0.02104187,0.02670288,0.047454834,-0.03918457,0.025161743,-0.005836487,-0.030166626,-0.012680054,0.047058105,0.01335144,0.03555298,0.0006093979,0.042266846,-0.052642822,0.052246094,0.038604736,-0.036315918,-0.0008945465,-0.0018491745,0.040527344,-0.014984131,0.06530762,-0.019592285,0.019699097,0.0074424744,0.023147583,-0.0062446594,0.0087890625,0.004322052,0.031311035,-0.022476196,0.08642578,0.011619568,0.009849548,-0.07647705,-0.020843506,0.0019330978,0.008018494,0.016525269,-0.050720215,-0.04360962,0.034576416,0.016143799,0.0042266846,-0.0008764267,0.08300781,0.030349731,-0.032836914,-0.019699097,-0.0012903214]", "client_status_at": "2025-10-02T09:52:32.967348Z", "server_status_at": "2025-10-02T09:52:32.967334Z", "created_at": "2025-10-02T08:52:29.379846Z"}, {"id": "ec2c40d8-ea1d-4280-9fb9-e8771424083f", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://arxiv.org/abs/2505.10468v4", "canonical_url": "https://arxiv.org/abs/2505.10468v4", "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges", "source_site": "arXiv.org", "publication_date": "2025-05-15T00:00:00Z", "favicon_url": "https://arxiv.org/favicon.ico", "content_markdown": "# Computer Science > Artificial Intelligence\n\n[v1](https://arxiv.org/abs/2505.10468v1)), revised 28 May 2025 (this version, v4),\n\n*latest version 30 Sep 2025*(\n\n[v5](https://arxiv.org/abs/2505.10468v5))]\n\n# Title:AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges\n\n[View PDF](/pdf/2505.10468v4)\n\n[HTML (experimental)](https://arxiv.org/html/2505.10468v4)\n\nAbstract:This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven by Large Language Models (LLMs) and Large Image Models (LIMs) for narrow, task-specific automation. Generative AI is positioned as a precursor, with AI Agents advancing through tool integration, prompt engineering, and reasoning enhancements. In contrast, Agentic AI systems represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Through a sequential evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both paradigms. Application domains such as customer support, scheduling, and data summarization are contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions such as ReAct loops, RAG, orchestration layers, and causal modeling. This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision Support System, Agentic-AI Applications\n\n## Submission history\n\nFrom: Ranjan Sapkota [[view email](/show-email/33b1662e/2505.10468)]\n\n**Thu, 15 May 2025 16:21:33 UTC (13,055 KB)**\n\n[[v1]](/abs/2505.10468v1)**Fri, 16 May 2025 23:31:18 UTC (13,058 KB)**\n\n[[v2]](/abs/2505.10468v2)**Tue, 20 May 2025 04:49:56 UTC (13,059 KB)**\n\n[[v3]](/abs/2505.10468v3)**[v4]**Wed, 28 May 2025 01:28:08 UTC (13,076 KB)\n\n**Tue, 30 Sep 2025 04:21:32 UTC (5,973 KB)**\n\n[[v5]](/abs/2505.10468v5)# Bibliographic and Citation Tools\n\n# Code, Data and Media Associated with this Article\n\n# Demos\n\n# Recommenders and Search Tools\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [ Learn more about arXivLabs](https://info.arxiv.org/labs/index.html).", "content_text": "Computer Science > Artificial Intelligence\nTitle:AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges\nView PDF HTML (experimental)Abstract:This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven by Large Language Models (LLMs) and Large Image Models (LIMs) for narrow, task-specific automation. Generative AI is positioned as a precursor, with AI Agents advancing through tool integration, prompt engineering, and reasoning enhancements. In contrast, Agentic AI systems represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Through a sequential evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both paradigms. Application domains such as customer support, scheduling, and data summarization are contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions such as ReAct loops, RAG, orchestration layers, and causal modeling. This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision Support System, Agentic-AI Applications\nSubmission history\nFrom: Ranjan Sapkota [view email][v1] Thu, 15 May 2025 16:21:33 UTC (13,055 KB)\n[v2] Fri, 16 May 2025 23:31:18 UTC (13,058 KB)\n[v3] Tue, 20 May 2025 04:49:56 UTC (13,059 KB)\n[v4] Wed, 28 May 2025 01:28:08 UTC (13,076 KB)\n[v5] Tue, 30 Sep 2025 04:21:32 UTC (5,973 KB)\nBibliographic and Citation Tools\nCode, Data and Media Associated with this Article\nDemos\nRecommenders and Search Tools\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.", "content_token_count": 595, "client_status": "queued", "server_status": "embedded", "summary": "This study distinguishes between AI Agents and Agentic AI, providing a conceptual taxonomy, application mapping, and challenge analysis. It outlines the architectural evolution, operational mechanisms, and unique challenges of both paradigms, aiming to offer a roadmap for developing robust and scalable AI systems.", "expiry_score": 0.6, "mistral_embedding": "[-0.015808105,0.033721924,0.028549194,0.021072388,0.0034008026,0.010726929,0.049438477,-0.01398468,-0.0054359436,0.0007157326,-0.032958984,0.08203125,-0.035827637,0.009628296,-0.052886963,0.01878357,-0.030654907,0.018112183,0.053253174,-0.020690918,-0.013793945,0.012741089,-0.025299072,-0.013511658,0.007904053,-0.026245117,0.00010329485,-0.09082031,-0.040802002,-0.006515503,0.012649536,-0.03717041,-0.011833191,0.001616478,-0.0019874573,0.0017366409,-0.024520874,-0.041778564,0.040802002,-0.03314209,0.005126953,-0.0056037903,-0.020309448,-0.009674072,-0.0104904175,-0.0035209656,0.0040016174,-0.017532349,0.029693604,0.014175415,-0.0017242432,0.06707764,-0.041381836,-0.033325195,-0.049041748,0.025299072,0.0018796921,0.012023926,-0.03488159,-8.684397e-05,-0.021942139,-0.011543274,0.009147644,-0.002538681,0.028930664,0.025680542,-0.019165039,-0.024429321,0.030471802,-0.0368042,0.006034851,0.021743774,0.017440796,0.03353882,-0.014274597,-0.025863647,0.0063705444,0.030075073,0.02796936,0.006801605,-0.027404785,0.012931824,0.0793457,-0.04559326,-0.021453857,0.014465332,0.03161621,0.019454956,-0.013221741,-0.005531311,0.044830322,0.029129028,0.032196045,-0.05709839,0.04598999,0.017440796,-0.015808105,0.010391235,0.00920105,0.050201416,0.045410156,-0.006755829,0.06744385,-0.049438477,0.037750244,-0.032958984,0.030654907,-0.008865356,0.0095825195,-0.021270752,-0.086242676,-0.030654907,-0.017822266,-0.049438477,-0.022994995,0.0023479462,-0.035064697,0.036987305,-0.0758667,-0.045410156,0.046569824,0.036590576,-0.029129028,0.029129028,-0.049438477,-0.048858643,0.004047394,0.03756714,-0.018966675,-0.005268097,0.005580902,-0.015327454,-0.02720642,-0.026443481,0.026824951,-0.028549194,0.05557251,-0.013893127,-0.03161621,-0.0034008026,0.008911133,-0.008384705,-0.027404785,0.0035438538,-0.016189575,-0.039855957,-0.012069702,0.03314209,-0.011253357,-0.030471802,0.034301758,0.047698975,0.051727295,-0.0030536652,-0.009338379,-0.021270752,-0.025100708,-0.011688232,0.04598999,-0.022033691,-0.0046691895,-0.00033521652,-0.024337769,0.031799316,-0.04751587,-0.014755249,-0.04119873,0.0038089752,-0.00028443336,0.010299683,0.036224365,-0.026443481,0.02835083,0.04559326,0.0015029907,-0.0041923523,0.015426636,-0.011398315,0.087768555,0.02758789,-0.0056037903,-0.050598145,0.038330078,0.003736496,0.013122559,-0.021743774,-0.07244873,0.010108948,0.003112793,0.012550354,-0.030075073,-0.018493652,-0.04272461,0.0011739731,8.046627e-05,0.021560669,0.03488159,-0.039093018,0.0051727295,-0.018875122,0.028167725,0.009338379,-0.0044555664,0.04714966,0.026245117,0.026443481,0.005748749,-0.001209259,0.022415161,0.013122559,0.026824951,0.026245117,-0.0023231506,-0.023376465,-0.014083862,0.017150879,0.038116455,0.00093984604,-0.0071372986,0.023849487,-0.027786255,-0.015617371,-0.009147644,-0.017532349,0.040618896,0.04196167,0.020507812,-0.009819031,0.0023593903,0.025680542,0.002981186,0.04156494,-0.019836426,0.020980835,-0.020217896,-0.021942139,0.060943604,0.009674072,0.009864807,0.002538681,0.01676941,0.034484863,-0.059020996,0.057495117,0.023666382,0.0008740425,-0.023468018,-0.006515503,0.0024662018,0.06707764,-0.07891846,-0.044067383,-0.050964355,0.013702393,-0.06628418,-0.01878357,-0.010635376,-0.032562256,0.017822266,-0.053649902,-0.03488159,0.013412476,0.026062012,0.045410156,0.02758789,0.010871887,-0.038330078,0.011207581,0.07550049,-0.016952515,0.0020008087,-0.037750244,0.03353882,-0.0056533813,0.050964355,-0.018585205,0.0044784546,0.029891968,-0.0045280457,0.0060577393,-0.0005297661,0.009101868,-0.020690918,0.020889282,-0.03237915,-0.020019531,0.008003235,-0.044067383,-0.009292603,-0.049438477,0.0003786087,0.03945923,0.06976318,0.004863739,0.006084442,0.0049095154,0.016952515,0.0121154785,-0.01638794,0.035461426,0.060150146,-0.031433105,-0.012023926,-0.013221741,0.023284912,-0.016571045,-0.042907715,-0.03640747,-0.025299072,-0.052124023,-0.016098022,0.030471802,-0.0047912598,-0.016281128,-0.0061798096,-0.007663727,-0.061706543,-0.037353516,0.018005371,-0.0026474,-0.021652222,-0.008140564,0.070495605,-0.015235901,0.019546509,0.0368042,-0.028549194,-0.03640747,0.03945923,0.014564514,0.03967285,-0.016662598,0.022033691,0.047332764,0.019546509,-0.0107803345,0.03756714,0.05557251,-0.024902344,-0.0073280334,-0.012550354,-0.012214661,0.048461914,-0.013893127,0.008003235,-0.016098022,-0.013313293,-0.020019531,0.008094788,0.017913818,-0.015327454,-0.0034008026,-0.0044555664,0.07092285,-0.017440796,0.08508301,-0.021362305,-0.07092285,-0.008956909,0.039276123,-0.010154724,-0.004550934,0.013122559,0.012741089,-0.00076055527,0.004623413,-0.015899658,0.017059326,-0.00031733513,-0.018005371,0.032196045,-0.00026798248,0.047332764,0.021850586,0.001616478,0.008430481,0.012260437,-0.010871887,0.020217896,-0.040618896,-0.041778564,0.026443481,0.018112183,0.029129028,0.0071372986,-0.012260437,-0.06976318,-0.0076179504,0.006275177,0.0927124,0.06591797,0.026062012,-0.0008444786,-0.012023926,0.0061798096,-0.003856659,0.0010662079,0.029312134,0.07092285,0.025100708,0.03717041,-0.023376465,0.020217896,0.007423401,0.044647217,-0.020401001,0.032196045,0.022323608,0.039276123,0.0014009476,0.0368042,-0.004142761,-0.025482178,0.0368042,-0.008865356,0.0027065277,-0.046173096,0.032562256,-0.0035438538,-0.0069465637,0.023666382,-0.013702393,-0.06976318,0.024230957,0.031234741,-0.053649902,-0.018203735,0.0064201355,-0.038116455,0.010391235,-0.034301758,-0.058624268,0.053649902,-0.018585205,-0.048858643,0.021743774,0.009529114,-0.029510498,-0.038330078,-0.041381836,0.0068969727,0.019256592,-0.004501343,0.01878357,-0.016189575,-0.0054626465,0.026824951,0.013313293,0.024902344,-0.029312134,0.029891968,-0.03869629,-0.028167725,0.050201416,0.04751587,0.020126343,-0.026245117,0.009727478,0.008239746,-0.016189575,-0.028930664,0.024337769,-0.051361084,-0.06665039,-0.011833191,0.05441284,0.040802002,0.047698975,-0.010154724,-0.068237305,0.040618896,-0.060546875,-0.01398468,0.0004849434,0.020690918,-0.029129028,0.05593872,0.0024662018,-0.014846802,-0.0027542114,0.026062012,-0.011734009,0.019454956,-0.0049324036,0.045806885,0.034484863,-0.04675293,-0.032196045,0.041381836,0.008911133,0.05557251,-0.06555176,-0.013031006,-0.0042877197,-0.05633545,-0.043884277,-0.006034851,-0.030273438,-0.026245117,0.031036377,0.013221741,0.015426636,0.03161621,-0.008476257,-0.01158905,0.031799316,0.031799316,-0.03201294,0.037750244,-0.04522705,-0.03564453,-0.0036888123,0.020401001,-0.026245117,-0.0051956177,0.0049324036,-0.02279663,-0.013511658,-0.016479492,-0.012168884,-0.0007214546,0.0030899048,-0.029129028,-0.036987305,0.02720642,0.017913818,0.029312134,0.019546509,0.029129028,-0.070129395,0.0066108704,0.023086548,-0.04156494,-0.023666382,0.017822266,0.025482178,-0.006324768,0.017623901,-0.024230957,-0.021560669,-0.06402588,0.017730713,-0.010063171,0.0015268326,-0.040252686,-0.020309448,-0.03869629,-0.022994995,0.025100708,-0.0006017685,0.0007724762,-0.022705078,0.019348145,-0.013793945,-0.043121338,0.0026817322,0.04272461,0.03756714,-0.0023345947,-0.028930664,0.03717041,-0.0009460449,-0.010681152,0.006706238,0.013221741,-0.037750244,0.019927979,0.0011796951,2.771616e-05,-0.02078247,-0.03353882,-0.022705078,-0.03756714,-0.050598145,-0.0121154785,0.013313293,0.011497498,0.011497498,-0.02078247,0.008331299,-0.019729614,0.048461914,0.002981186,-0.039276123,0.009819031,0.0039749146,0.044830322,0.04425049,0.0104904175,-0.024429321,-0.01360321,0.00819397,-0.022415161,-0.049804688,-0.043884277,0.03717041,0.00920105,0.05670166,-0.0023822784,0.050598145,0.046936035,-0.020217896,0.03314209,0.08392334,0.035247803,-0.010925293,-0.047912598,-0.012069702,-0.020599365,-0.0463562,-0.049804688,-0.0021438599,0.013412476,-0.06707764,-0.04272461,0.0040016174,-0.010253906,0.012069702,0.020126343,0.0068969727,0.06286621,0.021072388,-0.018295288,-0.016479492,0.032562256,-0.015716553,0.031433105,0.04522705,-0.007904053,0.0068511963,0.031234741,-0.0076179504,-0.017242432,0.016281128,0.015037537,-0.012260437,0.034484863,0.043701172,-0.0023002625,-0.048461914,0.0061798096,0.031036377,0.043121338,0.06665039,0.012069702,-0.013702393,0.01398468,-0.029312134,0.015617371,0.009246826,-0.019165039,-0.0037593842,-0.08850098,0.03277588,-0.062469482,0.011062622,0.017822266,-0.0047416687,-0.029312134,-0.0039520264,-0.026443481,-0.050201416,0.036987305,-0.024337769,-0.026626587,-0.0003144741,-0.043701172,0.02835083,0.0010662079,-0.06707764,-0.022705078,0.058258057,0.008529663,0.010345459,-0.008720398,-0.0071372986,-0.0039043427,0.004432678,-0.08850098,-0.0038089752,-0.0028018951,-0.045806885,0.022125244,0.0029582977,0.016479492,-0.010063171,-0.0027065277,-0.03314209,0.008666992,-0.012840271,0.049804688,0.03717041,-0.04156494,0.04751587,0.0463562,0.00090408325,0.0052452087,0.021942139,-0.03869629,0.011688232,0.022125244,-0.004119873,-0.057861328,0.0014734268,0.058624268,-0.048461914,0.007282257,0.0063705444,-0.0024795532,-0.02078247,0.028167725,-0.0025024414,0.0023345947,0.029312134,-0.036987305,-0.04348755,0.03353882,-0.050964355,0.038513184,0.03237915,0.018295288,-0.06439209,0.0038089752,-0.007713318,-0.03717041,0.01398468,-0.09313965,-0.011306763,0.045410156,-0.020019531,-0.0040016174,0.022323608,0.0007214546,-0.011253357,-0.029693604,0.031433105,-0.012741089,0.018005371,-0.019454956,-0.015327454,-0.014175415,-0.040252686,-0.020690918,-0.037750244,0.029510498,-0.00057172775,-0.005962372,0.028930664,-0.008766174,0.044830322,0.0024909973,0.032196045,-0.00699234,0.009101868,0.030853271,0.02758789,0.029693604,0.030853271,-0.050964355,0.0075187683,0.040252686,0.011161804,0.015808105,-0.00920105,-0.009483337,-0.013412476,0.004573822,-0.036010742,0.025100708,0.008049011,0.05404663,-0.006706238,0.045013428,-0.09197998,0.059783936,-0.03390503,0.009773254,0.0013055801,-0.023086548,-0.035247803,-0.052886963,-0.018676758,0.04446411,0.0030059814,0.0155181885,0.038116455,-0.031234741,-0.010253906,0.041015625,-0.0231781,0.0068969727,0.033325195,-0.0017366409,0.06781006,-0.015426636,-0.012840271,-0.004096985,0.049804688,-0.0010299683,-0.032562256,0.0002245903,-0.0056991577,0.02758789,0.040039062,-0.049041748,0.007472992,0.014846802,-0.022415161,0.0023708344,0.0054130554,-0.023086548,-0.00093984604,-0.014755249,-0.006515503,-0.019638062,-0.07623291,-0.028549194,-0.043121338,-0.008430481,-0.023376465,0.011016846,0.0021915436,-0.019638062,0.025680542,0.024902344,-0.036010742,-0.019058228,0.018493652,-0.03869629,0.017242432,0.0041656494,-0.06744385,0.020019531,-0.004310608,-0.017242432,-0.0016050339,0.027023315,-0.026443481,0.016281128,-0.022323608,-0.031433105,-0.002155304,0.008720398,-0.018005371,0.0030651093,0.03488159,-0.015037537,-0.031036377,0.026443481,1.6093254e-06,-0.029510498,0.030075073,0.012840271,-0.007663727,0.01360321,0.0028133392,-0.0028743744,0.044067383,-0.0014247894,-0.0026340485,0.005508423,-0.012741089,0.041381836,0.008621216,0.03756714,0.0023345947,0.009819031,-0.014274597,-0.06976318,0.013412476,-0.030853271,-0.049438477,0.045013428,0.07281494,0.045806885,0.010345459,0.061706543,0.044647217,0.04425049,0.04043579,-0.07891846,-0.06781006,0.03564453,0.028930664,-0.041778564,-0.07244873,0.053649902,0.03390503,-0.021942139,0.0012159348,0.017150879,-0.020690918,0.048675537,0.040618896,-0.0014009476,-0.059783936,0.0095825195,0.011497498,-0.012451172,-0.0024547577,0.020126343,0.0012331009,0.051361084,-0.051361084,-0.022415161,-0.01158905,0.07397461,0.014465332,-0.043884277,0.030654907,0.013793945,0.030654907,0.0013771057,-0.0056037903,-0.025299072,-0.0020828247,-0.00920105,0.016662598,0.013511658,0.057861328,0.01360321,-0.0010480881,0.011878967,-0.0066108704,0.0368042,0.0011911392,-0.0102005005,-0.005531311,-0.07470703,0.05670166,-0.0018920898,0.021072388,-0.012840271,-0.06286621,-0.00920105,-0.0071868896,0.02720642,0.052490234,0.031799316,-0.020126343,-0.013793945,0.047698975,0.012840271,0.015426636,0.031036377,-0.030471802,-0.004333496,-0.03756714,0.0042381287,-0.011543274,-0.030075073,-0.020401001,0.0011434555,0.027023315,-0.0068511963,-0.03161621,-0.012840271,0.024230957,-0.041381836,0.032958984,-0.020507812,-0.02078247,0.0184021,-0.051727295,0.06402588,-0.0031852722,0.041015625,0.012931824,0.01878357,0.0057258606,-0.004573822,-0.006706238,0.046569824,0.009002686,0.038116455,0.03717041,0.03161621,-0.016662598,0.037750244,-0.008003235,-0.02835083,0.019058228,0.017242432,0.051361084,-0.021942139,0.008529663,-0.01398468,-0.015716553,0.04559326,0.04827881,-0.025680542,0.029693604,-0.07434082,0.039093018,-0.009056091,0.06286621,-0.016952515,0.021560669,0.0044784546,0.006324768,-0.033325195,-0.016189575,-0.011062622,0.029129028,-0.018585205,0.05670166,0.03488159,0.0040245056,-0.060150146,0.0016050339,-0.008331299,-0.028549194,0.048095703,0.010108948,-0.022506714,0.022613525,0.024719238,0.020599365,0.003232956,0.04446411,0.026245117,-0.011207581,0.0047416687,-0.008285522]", "client_status_at": "2025-10-02T09:52:29.795910Z", "server_status_at": "2025-10-02T09:52:29.795894Z", "created_at": "2025-10-02T08:52:23.551865Z"}, {"id": "cfc80b7d-7d6e-49ae-b55f-b3ef1ce8ac77", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://mistral.ai/news/codestral-25-08", "canonical_url": null, "title": "Announcing Codestral 25.08 and the Complete Mistral Coding Stack for Enterprise | Mistral AI", "source_site": "Announcing Codestral 25.08 and the Complete Mistral Coding Stack for Enterprise", "publication_date": "2025-01-17T00:00:00Z", "favicon_url": "https://mistral.ai/favicon.ico", "content_markdown": "# Announcing Codestral 25.08 and the Complete Mistral Coding Stack for Enterprise\n\nHow the world’s leading enterprises are using integrated coding solutions from Mistral AI to cut development, review, and testing time by 50%—and why the playbook now fits every company that wants AI-native software development.\n\n## AI-powered coding is taking off, but enterprise adoption still lags due to critical limitations\n\nOver the past year, AI coding assistants have introduced powerful capabilities, such as multi-file reasoning, contextual suggestions, and natural-language agents, all directly within the IDE. Despite these improvements, however, adoption inside enterprise environments has been slow. The reasons have less to do with model performance or the interface, and more with how these tools are built, deployed, and governed.\n\nKey limitations holding back enterprise teams include:\n\n-\nDeployment constraints: Most AI coding tools are SaaS-only, with no options for VPC, on-prem, or air-gapped environments. This is a hard blocker for organizations in finance, defense, healthcare, and other regulated industries.\n\n-\nLimited customization: Enterprises often need to adapt models to their own codebases and development conventions. Without access to model weights, pos-training workflows, or extensibility, teams are locked out of leveraging the best of their codebases.\n\n-\nFragmented architecture: Agents, embeddings, completions, and plugins are frequently decoupled across vendors—leading to integration drift, inconsistent context handling, and operational overhead. Moreover, coding copilots are not well-integrated into full enterprise platforms, such as product development tools, CRMs, and customer issue trackers.\n\n-\nNo unified observability or control: Teams lack visibility into how AI is being used across the development lifecycle. Without telemetry, audit trails, and centralized controls, it’s difficult to scale AI usage responsibly or measure real ROI.\n\n-\nIncompatibility with internal toolchains: Many assistants operate in closed environments, making it hard to connect with internal CI/CD pipelines, knowledge bases, or static analysis frameworks.\n\n\nFor enterprises, these limitations aren’t edge cases—they’re baseline requirements. Solving them is what separates a good developer tool from an AI-native software development platform.\n\n## A Full-Stack Approach Built for AI-Native Software Development\n\nOur approach to enterprise coding isn’t a bundle of isolated tools. It’s an integrated system designed to support enterprise-grade software development across every stage—from code suggestion to autonomous pull requests.\n\nIt starts with fast, reliable completion—and scales up to full codebase understanding and multi-file automation.\n\n### 1. Fast, High-Fidelity Code Completion\n\nAt the foundation of the stack is Codestral, Mistral’s family of code generation models built specifically for high-precision fill-in-the-middle (FIM) completion. These models are optimized for production engineering environments: latency-sensitive, context-aware, and self-deployable.\n\nToday, we announce its latest update. Codestral 25.08 delivers measurable upgrades over prior versions:\n\n-\n+30% increase in accepted completions\n\n-\n+10% more retained code after suggestion\n\n-\n50% fewer runaway generations, improving confidence in longer edits\n\n-\nImproved performance on academic benchmarks for short and long-context FIM completion\n\n\nThese improvements were validated in live IDE usage across production codebases. The model supports a wide range of languages and tasks, and is deployable across cloud, VPC, or on-prem environments—with no architectural changes required.\n\nCodestral-2508 also brings improvements to chat mode:\n\n-\nInstruction following: +5% on IF eval v8\n\n-\nCode abilities: +5% in average MultiplE\n\n\n### 2. Codebase-Scale Search and Semantic Retrieval\n\nAutocomplete accelerates, but only if the model understands your codebase. [Codestral Embed](https://docs.mistral.ai/capabilities/embeddings/code_embeddings/) sets a new standard in this domain. Designed specifically for code rather than general text, it outperforms leading embedding models from OpenAI and Cohere in real-world code retrieval benchmarks.\n\nKey advantages include:\n\n-\nHigh-recall, low-latency search across massive monorepos and poly-repos. Developers can find internal logic, validation routines, or domain-specific utilities using natural language.\n\n-\nFlexible embedding outputs, with configurable dimensions (e.g., 256-dim, INT8) that balance retrieval quality with storage efficiency—while outperforming alternatives even at lower dimensionality\n\n-\nPrivate deployment for maximum control, ensuring no data leakage via third-party APIs. All embedding inference and index storage can run within enterprise infrastructure\n\n\nThis embedding layer serves as both the context foundation for agentic workflows and the retrieval engine powering in‑IDE code search features—without sacrificing privacy, performance, or precision.\n\n### 3. Autonomous Multi-Step Development with Agentic Workflows\n\nWith relevant context surfaced, AI can take meaningful action. [Devstral](https://mistral.ai/news/devstral), powered by the [OpenHands ](https://github.com/All-Hands-AI/OpenHands)agent scaffold, enables enterprise-ready agentic coding workflows. It’s built specifically for engineering tasks—cross-file refactors, test generation, and PR authoring—using structured, context-rich reasoning.\n\nStandout capabilities include:\n\n-\nTop open‑model performance on SWE‑Bench Verified: Devstral Small 1.1 scores 53.6%, and Devstral Medium reaches 61.6%, outperforming Claude 3.5, GPT‑4.1‑mini, and other open models by wide margins\n\n-\nFlexible architecture for any environment: Devstral is available in multiple sizes. The open-weight Devstral Small (24B, Apache-2.0) runs efficiently on a single Nvidia RTX 4090 or Mac with 32 GB RAM—ideal for self-hosted, air-gapped, or experimental workflows. The larger Devstral Medium is available through enterprise partnerships and our API for more advanced code understanding and planning capabilities.\n\n-\nOpen model for extensibility: Teams can fine-tune Devstral Small on proprietary code, build custom agents, or embed it directly into CI/CD workflows—without licensing lock-in. For production environments requiring higher model performance, Devstral Medium is available with enterprise-grade support, including the ability for companies to post-train and fine-tune.\n\n\nDelivering agentic automation within private infrastructure lets engineering organizations reduce friction, ensure compliance, and speed up delivery with repeatable, auditable AI workflows.\n\n### 4. IDE Integration and Operational Control\n\nAll capabilities in the Mistral stack—completion, semantic search, and agentic workflows—are surfaced through Mistral Code, a native plugin for JetBrains and VS Code.\n\nIt provides:\n\n-\nInline completions using Codestral 25.08, optimized for FIM and multi-line editing\n\n-\nOne-click task automations like “Write commit message”, “Fix function”, or “Add docstring”, powered by Devstral\n\n-\nContext awareness from Git diffs, terminal history, and static analysis tools\n\n-\nIntegrated semantic search, backed by Codestral Embed\n\n\nMistral Code is built to support enterprise deployment requirements:\n\n-\nDeploy in any environment: cloud, self-managed VPC, or fully on-prem (GA in Q3)\n\n-\nNo mandatory telemetry, and no external API calls for inference or search\n\n-\nSSO, audit logging, and usage controls for secure, policy-compliant adoption\n\n-\nUsage observability via the Mistral Console, including metrics on AI-generated code, suggestion acceptance, and agent usage\n\n\nThese features give engineering, platform, and security teams the ability to roll out AI tooling safely, incrementally, and with full visibility.\n\n## How It All Fits Together: From Developer Actions to Organizational Impact\n\nThe Mistral coding stack integrates autocomplete, semantic retrieval, and agentic workflows directly into the IDE—while giving platform teams control over deployment, observability, and security. In a typical development task:\n\nSay a developer is working on a payments service written in Python. A recent update to a third-party billing API means they need to update the integration logic and add proper error handling.\n\n-\nThey start by navigating to the billing handler. As they modify the function signature, Codestral fills in the expected parameters and suggests a first-pass implementation, reducing the need to copy patterns from other services.\n\n-\nBefore changing the retry logic, they need to understand how similar failures are handled elsewhere. Instead of switching to Slack or searching GitHub manually, they enter a query directly in the IDE: “How do we handle Stripe timeouts in the checkout flow?” The embedding index, running locally, returns a helper module from another service that wraps retry logic with exponential backoff.\n\n-\nThey copy the pattern into their own handler—but realize three other services are using outdated retry code. They invoke a Devstral-powered agent from within the IDE: “Replace all uses of retry_with_sleep in the billing and checkout services with the new retry_exponential helper, and update the docs.” Devstral scans the codebase using the same embeddings, makes the required edits across files, and generates a draft PR. The agent also writes a changelog and updates the README section on error handling.\n\n\nThe developer reviews the PR, confirms the logic, and merges it. A cross-service update that previously would have required search, coordination, and hand-written boilerplate now completes in one editing session—with traceable, reviewable output.\n\nAt the organization level, this same workflow unlocks broader advantages:\n\n-\nEvery component in the stack can be self-hosted or run on-prem, giving teams control over data, latency, and deployment architecture.\n\n-\nObservability is built in. The Mistral Console tracks usage patterns, model acceptance rates, and agent adoption, providing the data needed to tune rollout and measure ROI.\n\n-\nSecurity and compliance controls—including SSO, audit logging, and telemetry configuration—make it easy to integrate with internal policies and infrastructure.\n\n-\nNo stitching required. Because completion, search, and agents share architecture, context handling, and support boundaries, teams avoid the drift, overhead, and security gaps of piecing together third-party tools.\n\n\nThe result is a development workflow that’s both faster and easier to govern—designed for individual productivity and organizational scale.\n\n## Adopted by Leading Enterprises Across Diverse Environments\n\nThe Mistral coding stack is already being used in production by organizations across consulting, finance, transportation, and industry—each with different requirements, but shared constraints around data control, deployment flexibility, and internal code complexity.\n\n-\nCapgemini has rolled out the stack across global delivery teams to accelerate development while maintaining code ownership and compliance across clients in defense, telecom, and energy.\n\n-\nAbanca, a leading bank in Spain operating under European banking regulations, uses Mistral’s models in a fully self-hosted deployment to meet data residency and network isolation requirements—without sacrificing usability.\n\n-\nSNCF, the French national railway company, uses agentic workflows to modernize legacy Java systems safely and incrementally, with human oversight built into the loop.\n\n\n“Leveraging Mistral’s Codestral has been a game changer in the adoption of private coding assistant for our client projects in regulated industries. We have evolved from basic support for some development activities to systematic value for our development teams“.\n\nAlban Alev, VP head of Solutioning at Capgemini France.\n\n\nIn addition, several tier-1 global banks and industrial manufacturers are actively piloting or scaling adoption across their engineering teams—driven by requirements that hosted copilots and fragmented tooling can’t support.\n\nThese use cases reflect a growing shift: organizations are no longer looking for isolated assistants—they’re adopting integrated AI systems that match the complexity, security posture, and velocity of modern enterprise software development.\n\n## Get Started\n\nThe full Mistral coding stack—Codestral 25.08, Devstral, Codestral Embed, and the Mistral Code IDE extension—is available today for enterprise deployment.\n\nTeams can start with autocomplete and semantic search, then expand to agentic workflows and private deployments at their own pace.\n\nTo begin:\n\n-\nInstall Mistral Code from the\n\n[JetBrains](https://plugins.jetbrains.com/plugin/27493-mistral-code-enterprise)or[VS Code](https://marketplace.visualstudio.com/items?itemName=mistralai.mistral-code)marketplace -\nConnect to your preferred deployment modality (cloud, VPC, or on-prem)\n\n\nIf you would like to use the models for your own copilot, get your keys at [console.mistral.ai](http://console.mistral.ai). For more information on Mistral’s coding solutions, please visit our [website](https://mistral.ai/solutions/coding) and [documentation](https://docs.mistral.ai/).\n\nTo evaluate on-prem options, enterprise-scale deployments, or schedule a hands-on pilot, fill out the demand form on this page. A member of the Mistral team will follow up to help tailor the rollout to your environment.\n\nGet in touch.\n\nExplore Codestral and Mistral Code.", "content_text": "Announcing Codestral 25.08 and the Complete Mistral Coding Stack for Enterprise\nHow the world’s leading enterprises are using integrated coding solutions from Mistral AI to cut development, review, and testing time by 50%—and why the playbook now fits every company that wants AI-native software development.\nAI-powered coding is taking off, but enterprise adoption still lags due to critical limitations\nOver the past year, AI coding assistants have introduced powerful capabilities, such as multi-file reasoning, contextual suggestions, and natural-language agents, all directly within the IDE. Despite these improvements, however, adoption inside enterprise environments has been slow. The reasons have less to do with model performance or the interface, and more with how these tools are built, deployed, and governed.\nKey limitations holding back enterprise teams include:\n-\nDeployment constraints: Most AI coding tools are SaaS-only, with no options for VPC, on-prem, or air-gapped environments. This is a hard blocker for organizations in finance, defense, healthcare, and other regulated industries.\n-\nLimited customization: Enterprises often need to adapt models to their own codebases and development conventions. Without access to model weights, pos-training workflows, or extensibility, teams are locked out of leveraging the best of their codebases.\n-\nFragmented architecture: Agents, embeddings, completions, and plugins are frequently decoupled across vendors—leading to integration drift, inconsistent context handling, and operational overhead. Moreover, coding copilots are not well-integrated into full enterprise platforms, such as product development tools, CRMs, and customer issue trackers.\n-\nNo unified observability or control: Teams lack visibility into how AI is being used across the development lifecycle. Without telemetry, audit trails, and centralized controls, it’s difficult to scale AI usage responsibly or measure real ROI.\n-\nIncompatibility with internal toolchains: Many assistants operate in closed environments, making it hard to connect with internal CI/CD pipelines, knowledge bases, or static analysis frameworks.\nFor enterprises, these limitations aren’t edge cases—they’re baseline requirements. Solving them is what separates a good developer tool from an AI-native software development platform.\nA Full-Stack Approach Built for AI-Native Software Development\nOur approach to enterprise coding isn’t a bundle of isolated tools. It’s an integrated system designed to support enterprise-grade software development across every stage—from code suggestion to autonomous pull requests.\nIt starts with fast, reliable completion—and scales up to full codebase understanding and multi-file automation.\n1. Fast, High-Fidelity Code Completion\nAt the foundation of the stack is Codestral, Mistral’s family of code generation models built specifically for high-precision fill-in-the-middle (FIM) completion. These models are optimized for production engineering environments: latency-sensitive, context-aware, and self-deployable.\nToday, we announce its latest update. Codestral 25.08 delivers measurable upgrades over prior versions:\n-\n+30% increase in accepted completions\n-\n+10% more retained code after suggestion\n-\n50% fewer runaway generations, improving confidence in longer edits\n-\nImproved performance on academic benchmarks for short and long-context FIM completion\nThese improvements were validated in live IDE usage across production codebases. The model supports a wide range of languages and tasks, and is deployable across cloud, VPC, or on-prem environments—with no architectural changes required.\nCodestral-2508 also brings improvements to chat mode:\n-\nInstruction following: +5% on IF eval v8\n-\nCode abilities: +5% in average MultiplE\n2. Codebase-Scale Search and Semantic Retrieval\nAutocomplete accelerates, but only if the model understands your codebase. Codestral Embed sets a new standard in this domain. Designed specifically for code rather than general text, it outperforms leading embedding models from OpenAI and Cohere in real-world code retrieval benchmarks.\nKey advantages include:\n-\nHigh-recall, low-latency search across massive monorepos and poly-repos. Developers can find internal logic, validation routines, or domain-specific utilities using natural language.\n-\nFlexible embedding outputs, with configurable dimensions (e.g., 256-dim, INT8) that balance retrieval quality with storage efficiency—while outperforming alternatives even at lower dimensionality\n-\nPrivate deployment for maximum control, ensuring no data leakage via third-party APIs. All embedding inference and index storage can run within enterprise infrastructure\nThis embedding layer serves as both the context foundation for agentic workflows and the retrieval engine powering in‑IDE code search features—without sacrificing privacy, performance, or precision.\n3. Autonomous Multi-Step Development with Agentic Workflows\nWith relevant context surfaced, AI can take meaningful action. Devstral, powered by the OpenHands agent scaffold, enables enterprise-ready agentic coding workflows. It’s built specifically for engineering tasks—cross-file refactors, test generation, and PR authoring—using structured, context-rich reasoning.\nStandout capabilities include:\n-\nTop open‑model performance on SWE‑Bench Verified: Devstral Small 1.1 scores 53.6%, and Devstral Medium reaches 61.6%, outperforming Claude 3.5, GPT‑4.1‑mini, and other open models by wide margins\n-\nFlexible architecture for any environment: Devstral is available in multiple sizes. The open-weight Devstral Small (24B, Apache-2.0) runs efficiently on a single Nvidia RTX 4090 or Mac with 32 GB RAM—ideal for self-hosted, air-gapped, or experimental workflows. The larger Devstral Medium is available through enterprise partnerships and our API for more advanced code understanding and planning capabilities.\n-\nOpen model for extensibility: Teams can fine-tune Devstral Small on proprietary code, build custom agents, or embed it directly into CI/CD workflows—without licensing lock-in. For production environments requiring higher model performance, Devstral Medium is available with enterprise-grade support, including the ability for companies to post-train and fine-tune.\nDelivering agentic automation within private infrastructure lets engineering organizations reduce friction, ensure compliance, and speed up delivery with repeatable, auditable AI workflows.\n4. IDE Integration and Operational Control\nAll capabilities in the Mistral stack—completion, semantic search, and agentic workflows—are surfaced through Mistral Code, a native plugin for JetBrains and VS Code.\nIt provides:\n-\nInline completions using Codestral 25.08, optimized for FIM and multi-line editing\n-\nOne-click task automations like “Write commit message”, “Fix function”, or “Add docstring”, powered by Devstral\n-\nContext awareness from Git diffs, terminal history, and static analysis tools\n-\nIntegrated semantic search, backed by Codestral Embed\nMistral Code is built to support enterprise deployment requirements:\n-\nDeploy in any environment: cloud, self-managed VPC, or fully on-prem (GA in Q3)\n-\nNo mandatory telemetry, and no external API calls for inference or search\n-\nSSO, audit logging, and usage controls for secure, policy-compliant adoption\n-\nUsage observability via the Mistral Console, including metrics on AI-generated code, suggestion acceptance, and agent usage\nThese features give engineering, platform, and security teams the ability to roll out AI tooling safely, incrementally, and with full visibility.\nHow It All Fits Together: From Developer Actions to Organizational Impact\nThe Mistral coding stack integrates autocomplete, semantic retrieval, and agentic workflows directly into the IDE—while giving platform teams control over deployment, observability, and security. In a typical development task:\nSay a developer is working on a payments service written in Python. A recent update to a third-party billing API means they need to update the integration logic and add proper error handling.\n-\nThey start by navigating to the billing handler. As they modify the function signature, Codestral fills in the expected parameters and suggests a first-pass implementation, reducing the need to copy patterns from other services.\n-\nBefore changing the retry logic, they need to understand how similar failures are handled elsewhere. Instead of switching to Slack or searching GitHub manually, they enter a query directly in the IDE: “How do we handle Stripe timeouts in the checkout flow?” The embedding index, running locally, returns a helper module from another service that wraps retry logic with exponential backoff.\n-\nThey copy the pattern into their own handler—but realize three other services are using outdated retry code. They invoke a Devstral-powered agent from within the IDE: “Replace all uses of retry_with_sleep in the billing and checkout services with the new retry_exponential helper, and update the docs.” Devstral scans the codebase using the same embeddings, makes the required edits across files, and generates a draft PR. The agent also writes a changelog and updates the README section on error handling.\nThe developer reviews the PR, confirms the logic, and merges it. A cross-service update that previously would have required search, coordination, and hand-written boilerplate now completes in one editing session—with traceable, reviewable output.\nAt the organization level, this same workflow unlocks broader advantages:\n-\nEvery component in the stack can be self-hosted or run on-prem, giving teams control over data, latency, and deployment architecture.\n-\nObservability is built in. The Mistral Console tracks usage patterns, model acceptance rates, and agent adoption, providing the data needed to tune rollout and measure ROI.\n-\nSecurity and compliance controls—including SSO, audit logging, and telemetry configuration—make it easy to integrate with internal policies and infrastructure.\n-\nNo stitching required. Because completion, search, and agents share architecture, context handling, and support boundaries, teams avoid the drift, overhead, and security gaps of piecing together third-party tools.\nThe result is a development workflow that’s both faster and easier to govern—designed for individual productivity and organizational scale.\nAdopted by Leading Enterprises Across Diverse Environments\nThe Mistral coding stack is already being used in production by organizations across consulting, finance, transportation, and industry—each with different requirements, but shared constraints around data control, deployment flexibility, and internal code complexity.\n-\nCapgemini has rolled out the stack across global delivery teams to accelerate development while maintaining code ownership and compliance across clients in defense, telecom, and energy.\n-\nAbanca, a leading bank in Spain operating under European banking regulations, uses Mistral’s models in a fully self-hosted deployment to meet data residency and network isolation requirements—without sacrificing usability.\n-\nSNCF, the French national railway company, uses agentic workflows to modernize legacy Java systems safely and incrementally, with human oversight built into the loop.\n“Leveraging Mistral’s Codestral has been a game changer in the adoption of private coding assistant for our client projects in regulated industries. We have evolved from basic support for some development activities to systematic value for our development teams“.\nAlban Alev, VP head of Solutioning at Capgemini France.\nIn addition, several tier-1 global banks and industrial manufacturers are actively piloting or scaling adoption across their engineering teams—driven by requirements that hosted copilots and fragmented tooling can’t support.\nThese use cases reflect a growing shift: organizations are no longer looking for isolated assistants—they’re adopting integrated AI systems that match the complexity, security posture, and velocity of modern enterprise software development.\nGet Started\nThe full Mistral coding stack—Codestral 25.08, Devstral, Codestral Embed, and the Mistral Code IDE extension—is available today for enterprise deployment.\nTeams can start with autocomplete and semantic search, then expand to agentic workflows and private deployments at their own pace.\nTo begin:\n-\nInstall Mistral Code from the JetBrains or VS Code marketplace\n-\nConnect to your preferred deployment modality (cloud, VPC, or on-prem)\nIf you would like to use the models for your own copilot, get your keys at console.mistral.ai. For more information on Mistral’s coding solutions, please visit our website and documentation.\nTo evaluate on-prem options, enterprise-scale deployments, or schedule a hands-on pilot, fill out the demand form on this page. A member of the Mistral team will follow up to help tailor the rollout to your environment.\nGet in touch.\nExplore Codestral and Mistral Code.", "content_token_count": 2520, "client_status": "queued", "server_status": "embedded", "summary": "Mistral AI announces Codestral 25.08 and its complete coding stack for enterprises, addressing key limitations in AI-powered coding tools such as deployment constraints, customization, and integration. The stack includes Codestral for code completion, Codestral Embed for semantic retrieval, Devstral for agentic workflows, and Mistral Code for IDE integration, all designed for enterprise-grade software development.", "expiry_score": 0.3, "mistral_embedding": "[-0.030380249,0.033447266,0.038604736,0.0137786865,0.03201294,0.042388916,0.022598267,-0.01159668,-0.031555176,0.020721436,-0.002412796,0.072509766,-0.042388916,0.014480591,-0.08337402,0.05557251,0.03366089,0.007534027,0.059326172,-0.0070648193,-0.032958984,0.0030326843,-0.058410645,-0.010185242,0.016723633,-0.026489258,-0.006385803,-0.09136963,-0.047332764,-0.02696228,-0.0018692017,-0.005443573,-0.00021612644,-0.002281189,-0.0068893433,0.0047683716,0.004825592,-0.06500244,0.028381348,-0.011711121,-0.012954712,0.011123657,-0.007949829,-0.026489258,-0.028015137,-0.011711121,-0.031311035,-0.010421753,0.018722534,-0.028137207,-0.0056495667,0.043792725,0.008003235,0.00042319298,0.0049438477,0.0045318604,0.012008667,0.0067710876,-0.05392456,0.026733398,-0.047088623,-0.022491455,-0.010475159,-0.017074585,-0.0044136047,0.048034668,0.009063721,-4.0233135e-05,0.009651184,-0.038848877,0.00066947937,-0.014190674,-0.002325058,0.06451416,-0.020843506,-0.045440674,0.037902832,0.024841309,0.027191162,-0.0088272095,-0.02166748,0.025543213,0.04827881,1.0192394e-05,-0.01600647,0.03604126,0.009475708,0.0034732819,0.02331543,0.028015137,0.03390503,-0.014129639,0.031799316,-0.039093018,0.017074585,0.023788452,-0.022018433,-0.009773254,-0.017196655,0.027908325,0.062164307,-0.0036640167,0.077697754,-0.0069770813,0.012718201,-0.026611328,0.0027370453,-0.009536743,-0.0022659302,-0.04309082,-0.079589844,-0.026260376,-0.0072402954,-0.059326172,-0.011070251,-0.0056495667,0.0012359619,0.07110596,-0.03744507,-0.0541687,0.011245728,0.01966858,0.0065345764,-0.0067710876,-0.04119873,-0.024612427,0.017654419,0.040740967,0.0028705597,-0.012359619,-0.014778137,-0.008651733,-0.040252686,-0.01789856,0.01977539,-0.04827881,-0.00034952164,-0.015777588,-0.034606934,-0.014480591,0.01259613,-0.014480591,-0.035095215,0.0028839111,-0.0007762909,0.0057411194,0.022720337,0.029083252,-0.048980713,-0.0076522827,0.013008118,0.026138306,0.079589844,0.012893677,-0.010948181,-0.0115356445,-0.024719238,0.013183594,0.05581665,-0.026367188,-0.027557373,-0.009536743,-0.009300232,0.00021529198,-0.0011262894,-0.022720337,-0.013008118,-0.012130737,0.015304565,0.008125305,0.06500244,-0.051330566,0.034606934,0.042144775,-0.047088623,0.04284668,0.004737854,0.0021343231,0.038604736,0.03390503,-0.031082153,-0.017196655,0.024490356,-0.01159668,0.033447266,-0.03933716,-0.03555298,0.015777588,-0.03060913,0.014007568,-0.0115356445,-0.026611328,-0.047332764,0.049926758,0.011833191,0.057678223,0.035095215,-0.0046195984,0.051574707,0.0019426346,0.00983429,0.01348114,0.023544312,0.06121826,0.020950317,0.02154541,0.013656616,0.03414917,0.034851074,-0.0028839111,-0.032958984,0.022842407,0.0007247925,-0.001376152,-0.030853271,-0.0137786865,-0.00032019615,0.0039138794,-0.010772705,0.0055618286,0.012832642,-0.029312134,-0.0037975311,0.010887146,0.018600464,0.044036865,0.040985107,-0.024368286,-0.030136108,-0.0055351257,0.016952515,0.010658264,-0.016952515,-0.0016479492,0.022247314,-0.04144287,-0.017303467,0.01612854,-0.02166748,0.04284668,-0.03274536,0.028381348,-0.012008667,0.04333496,0.002708435,-0.013069153,-0.0008058548,0.005947113,-0.002368927,0.03744507,-0.06591797,-0.025314331,-0.04498291,0.005329132,-0.099365234,-0.023071289,0.025436401,-0.05581665,0.06549072,-0.045928955,-0.012893677,0.014305115,0.022369385,-0.0023841858,0.030853271,0.025314331,-0.055114746,0.029907227,0.038391113,0.013008118,-0.03439331,-0.023422241,0.01977539,-0.0007138252,0.048736572,0.010772705,-0.004299164,0.040039062,0.011009216,0.021774292,-0.016830444,0.015899658,0.01789856,0.039794922,-0.041900635,0.007888794,-0.009887695,-0.006210327,0.0055618286,-0.036743164,0.0069465637,0.0574646,0.045440674,-0.008651733,0.0026187897,-0.037200928,0.024017334,0.029907227,-0.0012140274,0.047332764,0.058410645,-0.019195557,-0.000415802,-0.0031204224,0.051086426,-0.01637268,-0.023544312,-0.0075950623,-0.042388916,-0.060760498,-0.0073013306,0.009475708,-0.008598328,0.03439331,-0.0021038055,0.0024585724,-0.04827881,-0.037200928,-4.5776367e-05,-0.029556274,-0.029312134,-0.030380249,0.062164307,-0.0140686035,0.006122589,0.0017223358,0.016952515,-0.042388916,-0.010421753,-0.011299133,0.014778137,-0.0031490326,0.031799316,0.034851074,0.014892578,0.021194458,0.054382324,0.020843506,-0.051086426,-0.0085372925,-0.027664185,-0.015777588,0.01777649,-0.054870605,-0.015777588,-0.010360718,-0.030380249,0.003370285,0.0002887249,0.0085372925,0.04144287,0.0030021667,-0.013832092,0.042144775,-0.007534027,0.07489014,0.00674057,-0.04119873,-0.00983429,0.024017334,-0.03439331,-0.0115356445,0.07348633,0.031082153,0.020019531,0.026611328,0.005329132,0.045196533,-0.0574646,0.010299683,0.022018433,0.014129639,0.016723633,-0.0037078857,-0.021072388,0.035095215,-0.0024871826,-0.03225708,-0.00983429,-0.016601562,-0.027435303,0.009475708,0.020019531,0.06549072,0.007713318,-0.002325058,-0.041900635,-0.018600464,-0.0069770813,0.07489014,0.0501709,0.04119873,0.015899658,0.010887146,0.005622864,-0.008773804,0.018249512,0.057678223,0.031555176,-0.012245178,0.001332283,-0.0574646,0.06121826,0.017425537,0.025787354,0.011482239,0.010185242,-0.0042686462,0.043792725,-0.024963379,0.0501709,0.025436401,-0.048980713,-0.0040626526,-0.0085372925,0.004886627,-0.033203125,0.00806427,-0.011711121,-0.007270813,0.0062408447,-0.0015230179,-0.042633057,-0.007827759,0.04119873,-0.039794922,-0.00674057,0.045928955,0.00579834,0.04144287,-0.049926758,-0.000688076,0.03555298,-0.035095215,-0.040985107,0.01600647,0.010063171,-0.04144287,-0.008476257,-0.049682617,0.015068054,0.01637268,0.00048208237,0.038391113,-0.024017334,0.007827759,0.027191162,0.04144287,0.009773254,-0.034606934,0.03933716,-0.042633057,-0.0013837814,0.036254883,0.036956787,0.0034885406,0.0025024414,-0.01159668,0.0049743652,0.011711121,-0.02331543,0.0077705383,-0.048736572,-0.096069336,-0.06451416,0.06311035,0.026367188,0.056518555,-0.026489258,-0.017303467,0.038146973,-0.054870605,-0.031555176,-0.01071167,0.0025024414,0.0009493828,0.0287323,0.0044136047,0.02508545,0.014007568,0.044036865,-0.0031642914,-0.01637268,-0.046844482,0.031311035,0.06311035,-0.06121826,-0.01600647,0.03201294,0.05368042,0.038604736,-0.04333496,-0.012481689,-0.0137786865,-0.048034668,-0.024368286,0.009124756,0.0026931763,-0.04473877,0.025314331,0.0137786865,0.0004451275,-0.012718201,0.007949829,0.0099487305,0.036499023,0.0031490326,-0.029663086,0.051086426,-0.02861023,0.0017147064,-0.0021629333,0.012542725,-0.0007505417,0.016601562,0.02684021,-0.059326172,-0.0017147064,-0.023544312,-0.01966858,-0.02684021,-0.014892578,-0.030853271,-0.023895264,-0.0066223145,-0.0072402954,-0.014602661,-0.028137207,0.034851074,-0.015777588,0.0018100739,0.0019426346,-0.038848877,0.027435303,-0.01159668,0.027557373,-0.018844604,-0.015899658,-0.024139404,-0.018844604,-0.050628662,0.036956787,-0.029083252,-0.0004968643,-0.012481689,-0.020019531,-0.06781006,-0.025787354,0.029556274,0.010887146,-0.0018100739,-0.032958984,0.002193451,-0.009422302,-0.014778137,0.030853271,-0.004180908,0.011947632,0.001162529,0.01424408,0.02166748,-0.018478394,0.029556274,-0.0004892349,0.045928955,-0.03955078,0.011772156,-0.0013685226,-0.015899658,-0.040252686,-0.046142578,0.024719238,-0.035095215,-0.011657715,0.0025901794,0.045684814,-0.015541077,0.029907227,-0.024490356,0.0022525787,0.04144287,0.01625061,-0.0051231384,-0.036254883,0.020248413,0.0085372925,0.02519226,0.01600647,-0.014602661,-0.027557373,0.015304565,0.035095215,-0.053222656,-0.06402588,-0.014305115,0.031799316,-0.014602661,0.07348633,-0.0013027191,0.042144775,0.040740967,-0.001707077,0.002796173,0.05392456,0.023544312,0.02166748,-0.055114746,-0.005268097,-0.0032081604,-0.038604736,-0.06781006,0.011009216,0.045196533,-0.06549072,-0.011482239,-0.024368286,-0.007446289,0.029083252,0.038604736,0.009063721,0.033447266,0.022842407,-0.014190674,0.037200928,0.013183594,0.020370483,0.035308838,0.05886841,-0.01348114,0.01612854,0.011833191,-0.028961182,-0.011360168,0.01777649,0.012130737,0.049224854,0.03414917,0.05038452,0.008651733,-0.015541077,-0.015899658,0.006652832,0.024612427,0.06311035,-0.018127441,0.020370483,-0.0075950623,-0.013069153,-0.019546509,0.02708435,-0.00818634,-0.009475708,-0.04168701,0.00082063675,-0.025543213,-0.0014419556,0.03579712,-0.020370483,-0.0032672882,0.059814453,0.017196655,-0.028961182,0.021774292,-0.036254883,-0.012420654,-0.011360168,-0.04827881,0.024017334,-0.03744507,-0.06732178,-0.00021803379,0.06689453,-0.018478394,-0.046844482,0.0013904572,-0.024017334,0.010475159,0.0010299683,-0.024612427,-0.015777588,0.006298065,-0.05038452,-0.00039196014,-0.002943039,-0.022720337,-0.040039062,-0.022140503,-0.027313232,0.04119873,-0.025543213,0.051574707,0.009010315,-0.002325058,0.027557373,0.04119873,-0.013420105,0.0055351257,-0.014541626,-0.048980713,-0.026611328,0.0044441223,0.031082153,-0.040039062,0.023544312,0.049682617,-0.075805664,0.038391113,0.03555298,-0.023895264,-0.03579712,0.038391113,0.006298065,0.011894226,0.031799316,-0.0013837814,-0.033203125,-0.0030021667,-0.037902832,0.06549072,-0.0048561096,0.030853271,-0.040039062,0.02319336,-0.029083252,-0.022842407,0.008598328,-0.06829834,0.00073575974,0.02142334,0.0027809143,-0.011123657,0.05557251,0.0044136047,-0.030380249,-0.038391113,0.016952515,-0.018371582,-7.176399e-05,-0.008651733,-0.03060913,-0.050872803,-0.018844604,-0.042144775,-0.070617676,0.01625061,0.0073280334,-0.025665283,-0.001750946,-0.020843506,0.04119873,0.0046195984,0.05581665,-0.01966858,-0.0021038055,-0.019546509,0.05392456,0.017654419,-0.03060913,-0.030380249,0.006034851,-0.0032081604,-0.0037231445,0.04852295,0.030853271,-0.00082063675,-0.00447464,0.014892578,-0.078186035,0.013832092,0.0099487305,0.027664185,-0.026367188,0.0020160675,-0.06549072,0.052978516,-0.014831543,0.011360168,0.0052375793,-0.031311035,-0.08337402,-0.05911255,-0.040252686,-0.02166748,-0.040740967,0.013542175,-0.004737854,0.00818634,0.021774292,0.0019130707,0.0013542175,0.048736572,-0.008003235,0.007209778,0.07299805,-0.028381348,-0.012771606,-0.026260376,0.046844482,-0.019424438,-0.029312134,-0.017196655,-0.0010156631,0.016479492,0.01802063,-0.054626465,-0.007507324,0.014419556,-0.03250122,-0.027664185,0.012542725,-0.011360168,-0.016723633,-0.025543213,0.016601562,-0.03579712,-0.073913574,-0.018478394,-0.026260376,-0.03604126,-0.005592346,-0.01789856,-0.0064468384,-0.06262207,0.06311035,0.033203125,0.012359619,-0.028137207,0.00081682205,-0.021774292,0.03933716,-0.023071289,-0.037200928,0.011947632,-0.005592346,-0.03414917,-0.046844482,0.043548584,-0.009651184,0.03366089,-0.005680084,-0.023071289,-0.0006623268,0.008415222,-0.020370483,0.02684021,0.03579712,-0.023895264,-0.05722046,-0.006385803,0.0017585754,-0.053466797,0.021072388,0.008239746,-0.013069153,-0.008598328,0.02331543,0.039093018,0.07159424,0.010475159,0.0045051575,0.024490356,-0.023071289,0.022369385,-0.00983429,0.039794922,0.010185242,0.038848877,0.012130737,-0.0574646,0.020019531,-0.01789856,-0.059814453,-0.006385803,0.07019043,0.033203125,0.019195557,0.03414917,0.033447266,0.059326172,-0.014480591,-0.07348633,-0.06311035,0.0062713623,0.032958984,0.012130737,-0.056518555,0.0015897751,0.047790527,-0.011482239,-0.029663086,-0.03744507,-0.02708435,0.015777588,0.022598267,0.031311035,-0.08428955,0.02696228,0.038146973,0.0049743652,-0.0061798096,-0.007827759,0.008300781,0.06262207,-0.06500244,-0.009597778,0.018478394,0.052734375,-0.0043563843,-0.05886841,-0.004180908,0.02319336,0.009063721,-0.027908325,-0.0011777878,0.021194458,-0.016952515,-0.006298065,0.013420105,0.013542175,0.052734375,0.001376152,-0.04144287,0.031311035,-0.0060653687,-0.009185791,0.0009198189,-0.02319336,0.0063285828,-0.008476257,0.062164307,0.015655518,0.020843506,0.00894928,-0.058410645,0.015541077,0.0068588257,0.04473877,-0.017303467,0.018600464,0.030136108,0.012184143,0.018722534,0.038391113,-0.012771606,0.01637268,-0.036254883,-0.024841309,-0.060058594,0.0038261414,-0.0137786865,-0.01789856,0.024261475,0.008773804,0.02154541,-0.04473877,-0.012718201,-0.033203125,0.04498291,-0.06781006,0.042633057,0.024963379,-0.014831543,0.0074768066,-0.038391113,0.05886841,0.009361267,0.029434204,-0.015426636,0.024017334,0.03250122,0.00043606758,-0.02319336,0.03955078,-0.0045051575,0.025909424,0.03274536,0.04827881,-0.038391113,0.06262207,-0.013656616,-0.048980713,-0.02166748,0.046142578,0.016952515,0.0019426346,-0.009422302,0.018478394,-0.031082153,0.049224854,0.032958984,-0.009597778,0.026611328,-0.024490356,0.016723633,-0.028259277,0.06689453,-0.017303467,0.049224854,0.017196655,-0.018844604,0.0044136047,0.016723633,0.008888245,0.021774292,-0.029205322,0.072509766,0.0040626526,0.014007568,-0.06549072,-0.009063721,0.0023097992,0.0068893433,0.031082153,-0.020721436,-0.040252686,0.01259613,0.03744507,0.025787354,0.022964478,0.06781006,0.03555298,-0.040740967,-0.0052108765,0.008651733]", "client_status_at": "2025-10-02T09:52:16.775512Z", "server_status_at": "2025-10-02T09:52:16.775500Z", "created_at": "2025-10-02T08:52:11.563909Z"}, {"id": "9b305d74-baed-43dd-b9cd-409848d1f3a6", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://www.bbc.co.uk/sport/athletics/articles/c5yj0wyje7lo", "canonical_url": "https://www.bbc.com/sport/athletics/articles/c5yj0wyje7lo", "title": "New championships, new calendar? What next for athletics after Tokyo", "source_site": "BBC Sport", "publication_date": "2025-09-22T00:00:00Z", "favicon_url": "https://www.bbc.com/favicon.ico", "content_markdown": "- Published\n\n**The global calendar of Olympic sports may need to be \"re-engineered\" amid the challenges posed by climate change after athletics' biggest stars had to contend with searing temperatures in Tokyo, World Athletics president Sebastian Coe says.**\n\nThe World Athletics Championships concluded on Sunday night, following nine days of competition during which the sport's household names lit up Japan's National stadium, which four years ago hosted the Olympics behind closed doors because of the coronavirus pandemic.\n\nThe opening days of the championships were dominated by discussion around the conditions the athletes had to contend with - particularly in the endurance events - as temperatures exceeding 30C were accompanied by stifling humidity above 90%.\n\nIn addition to the quadrennial Olympic Games and biennial World Championships, World Athletics will next year stage its inaugural Ultimate Championships in Budapest to bridge the gap between its major events and provide a definitive conclusion to the 2026 season.\n\nBut Lord Coe said events such as the marathon may need to be held separately, at a different time of the year, at such competitions to protect athletes from unsafe conditions in future.\n\n\"Yes, I do see that,\" Coe told BBC Sport. \"That's not easy because you're going even into the autumn, early winter months to cities that are still very hot.\n\n\"But I do think this is probably going to have to happen at some stage, and sooner rather than later.\"\n\nAccording to Coe, World Athletics research has found that 70% of athletes report that climate change and the heat is impacting their training and competition programmes.\n\nThe 68-year-old said \"governments haven't really stepped up to the plate\" on the issue and added: \"I can't see beyond the inevitability of having to collectively, as Olympic sports and probably the Olympic movement, really re-engineering what the international calendar looks like.\n\n\"From World Athletics' point of view, there will be some branding issues but I'm not sure that we can go on asking some of our endurance-based athletes to be competing at times of the year which are really are going to hit their performances and are probably putting them at risk as well.\n\n\"This has to be addressed.\"\n\nCoe also confirmed that all athletes who competed in the female category at the World Championships had undergone gene testing - and that the test is here to stay.\n\nHowever, he reiterated that details of any athletes who may have been prevented from competing as a result of the test, carried out via cheek swab, will remain confidential.\n\nIts introduction comes amid [reports, external](https://www.theguardian.com/sport/2025/sep/19/sex-tests-brought-in-after-data-showed-50-60-dsd-athletes-in-finals-world-athletics-says) that between 50 and 60 athletes who went through male puberty have been finalists in the female category at global and continental track and field championships since 2000.\n\n## Athletics 'moving in right direction', says Bolt\n\nPole vault superstar Armand Duplantis, Americans Sydney McLaughlin-Levrone and Noah Lyles, and Kenya's Faith Kipyegon were among the sport's big names who captivated often sell-out 60,000 crowds at Japan's National Stadium.\n\nSwedish 25-year-old Duplantis produced arguably the standout moment when he soared to his 14th world record with his final attempt after winning a third consecutive world title.\n\nMcLaughlin-Levrone went close to the controversial 40-year women's 400m world record, while compatriot Lyles emulated Jamaican great Usain Bolt with a fourth consecutive world 200m title and Kipyegon won a historic fourth world 1500m title.\n\nSpeaking at a closing news conference on Sunday, Coe said: \"I think our sport is in really good shape. We have broader appeal and a greater level of following than we have had for a long, long time.\n\n\"We have athletes that are performing as well as any generation I have witnessed.\"\n\nNext year, they will compete in Budapest as World Athletics hosts its three-day Ultimate Championships featuring world champions, Diamond League winners and the top performers of 2026 - and which offers a $10m (£7.5m) prize pot.\n\nEarlier this year, the inaugural season of Michael Johnson's Grand Slam Track - which excluded field events - ended with the final leg being cancelled because of financial concerns and its future appears in doubt with [some athletes still yet to receive money](/sport/athletics/articles/cn83949z2z7o).\n\nCoe said World Athletics intend to use the Ultimate Championship to test ideas which could later become part of the sport's established major stages.\n\n\"We've got a four-year cycle and one of those years we don't get the billion eyeballs on our sport. As a global sport we can't afford that,\" said Coe.\n\n\"We thought about what we can do that is different and can be an incubator for change, where we can introduce and test things and maybe they turn up here in a World Championship.\n\n\"I think it will be it'll be a great addition but there is a lot more work to do.\"\n\nSporting icon Bolt, winner of eight Olympic gold medals, told BBC Sport he believes the Ultimate Championships is a \"good start\" as the sport seeks to grow.\n\n\"The sport needs to evolve and I feel like track and field is moving in the right direction,\" Bolt said.\n\n\"Track and field is probably one of the poorest sports and it's one of the biggest sports, so the incentive is very big and I'm happy for the athletes because they deserve it.\"\n\nRegarding the high level of performances witnessed in Tokyo, and increasing standards overall, Coe pinpointed technological advances in the synthetic track surface, the shoes worn by the athletes, and improving coaching skills and methods.\n\nCoe said: \"Some of the performances we've seen here are simply jaw-dropping.\n\n\"The lovely thing about it is that the sport is moving ahead both as track and field in pretty much the same progress.\"\n\nHe added: \"There's never enough prize money for the athletes, we always want more. Where possible, we want the athletes to benefit from the proceeds of growth.\"", "content_text": "- Published\nThe global calendar of Olympic sports may need to be \"re-engineered\" amid the challenges posed by climate change after athletics' biggest stars had to contend with searing temperatures in Tokyo, World Athletics president Sebastian Coe says.\nThe World Athletics Championships concluded on Sunday night, following nine days of competition during which the sport's household names lit up Japan's National stadium, which four years ago hosted the Olympics behind closed doors because of the coronavirus pandemic.\nThe opening days of the championships were dominated by discussion around the conditions the athletes had to contend with - particularly in the endurance events - as temperatures exceeding 30C were accompanied by stifling humidity above 90%.\nIn addition to the quadrennial Olympic Games and biennial World Championships, World Athletics will next year stage its inaugural Ultimate Championships in Budapest to bridge the gap between its major events and provide a definitive conclusion to the 2026 season.\nBut Lord Coe said events such as the marathon may need to be held separately, at a different time of the year, at such competitions to protect athletes from unsafe conditions in future.\n\"Yes, I do see that,\" Coe told BBC Sport. \"That's not easy because you're going even into the autumn, early winter months to cities that are still very hot.\n\"But I do think this is probably going to have to happen at some stage, and sooner rather than later.\"\nAccording to Coe, World Athletics research has found that 70% of athletes report that climate change and the heat is impacting their training and competition programmes.\nThe 68-year-old said \"governments haven't really stepped up to the plate\" on the issue and added: \"I can't see beyond the inevitability of having to collectively, as Olympic sports and probably the Olympic movement, really re-engineering what the international calendar looks like.\n\"From World Athletics' point of view, there will be some branding issues but I'm not sure that we can go on asking some of our endurance-based athletes to be competing at times of the year which are really are going to hit their performances and are probably putting them at risk as well.\n\"This has to be addressed.\"\nCoe also confirmed that all athletes who competed in the female category at the World Championships had undergone gene testing - and that the test is here to stay.\nHowever, he reiterated that details of any athletes who may have been prevented from competing as a result of the test, carried out via cheek swab, will remain confidential.\nIts introduction comes amid reports, external that between 50 and 60 athletes who went through male puberty have been finalists in the female category at global and continental track and field championships since 2000.\nAthletics 'moving in right direction', says Bolt\nPole vault superstar Armand Duplantis, Americans Sydney McLaughlin-Levrone and Noah Lyles, and Kenya's Faith Kipyegon were among the sport's big names who captivated often sell-out 60,000 crowds at Japan's National Stadium.\nSwedish 25-year-old Duplantis produced arguably the standout moment when he soared to his 14th world record with his final attempt after winning a third consecutive world title.\nMcLaughlin-Levrone went close to the controversial 40-year women's 400m world record, while compatriot Lyles emulated Jamaican great Usain Bolt with a fourth consecutive world 200m title and Kipyegon won a historic fourth world 1500m title.\nSpeaking at a closing news conference on Sunday, Coe said: \"I think our sport is in really good shape. We have broader appeal and a greater level of following than we have had for a long, long time.\n\"We have athletes that are performing as well as any generation I have witnessed.\"\nNext year, they will compete in Budapest as World Athletics hosts its three-day Ultimate Championships featuring world champions, Diamond League winners and the top performers of 2026 - and which offers a $10m (£7.5m) prize pot.\nEarlier this year, the inaugural season of Michael Johnson's Grand Slam Track - which excluded field events - ended with the final leg being cancelled because of financial concerns and its future appears in doubt with some athletes still yet to receive money.\nCoe said World Athletics intend to use the Ultimate Championship to test ideas which could later become part of the sport's established major stages.\n\"We've got a four-year cycle and one of those years we don't get the billion eyeballs on our sport. As a global sport we can't afford that,\" said Coe.\n\"We thought about what we can do that is different and can be an incubator for change, where we can introduce and test things and maybe they turn up here in a World Championship.\n\"I think it will be it'll be a great addition but there is a lot more work to do.\"\nSporting icon Bolt, winner of eight Olympic gold medals, told BBC Sport he believes the Ultimate Championships is a \"good start\" as the sport seeks to grow.\n\"The sport needs to evolve and I feel like track and field is moving in the right direction,\" Bolt said.\n\"Track and field is probably one of the poorest sports and it's one of the biggest sports, so the incentive is very big and I'm happy for the athletes because they deserve it.\"\nRegarding the high level of performances witnessed in Tokyo, and increasing standards overall, Coe pinpointed technological advances in the synthetic track surface, the shoes worn by the athletes, and improving coaching skills and methods.\nCoe said: \"Some of the performances we've seen here are simply jaw-dropping.\n\"The lovely thing about it is that the sport is moving ahead both as track and field in pretty much the same progress.\"\nHe added: \"There's never enough prize money for the athletes, we always want more. Where possible, we want the athletes to benefit from the proceeds of growth.\"", "content_token_count": 1193, "client_status": "bookmark", "server_status": "embedded", "summary": "World Athletics president Sebastian Coe discusses the need to re-engineer the global calendar of Olympic sports due to climate change challenges, following the World Athletics Championships in Tokyo. The article also highlights the introduction of the Ultimate Championships and the impact of climate change on athletes' performances and safety.", "expiry_score": 0.3, "mistral_embedding": "[-0.024963379,0.06213379,0.05529785,-0.019485474,0.027114868,-0.0018854141,0.020736694,-0.055664062,-0.0154418945,-0.00969696,-0.0362854,0.042022705,-0.012748718,-0.024429321,-0.03213501,0.034301758,0.0049819946,0.021102905,0.036987305,-0.034118652,-0.024963379,-0.015266418,-0.032684326,0.015174866,-0.028915405,0.014457703,-0.033935547,-0.05783081,-0.035736084,-0.009880066,0.009025574,-0.010864258,0.038604736,-0.008392334,0.011001587,-0.010055542,-0.020645142,-0.01499176,0.0035247803,-0.006511688,-0.020019531,-0.011764526,0.03213501,-0.010322571,-0.025497437,-0.0184021,0.015357971,-0.0069122314,0.017684937,-0.02128601,0.0027503967,0.036987305,0.027648926,0.0023117065,-0.014007568,0.012840271,-0.00749588,0.0010385513,-0.040771484,0.05496216,-0.05316162,-0.0007801056,-0.021820068,0.025680542,0.033935547,0.017242432,-0.0022449493,0.0014476776,0.022979736,-0.006690979,0.022720337,0.02406311,0.009880066,0.046325684,-0.019119263,-0.061767578,0.003232956,0.021377563,0.055664062,0.0368042,-0.0019302368,0.0027046204,0.061767578,-0.01625061,-0.012031555,0.037902832,0.011581421,0.015357971,-0.019485474,-0.009651184,0.009475708,-0.010597229,0.039154053,0.0011787415,0.025146484,0.008934021,-0.04437256,-0.012390137,-0.021453857,0.021911621,0.037719727,-0.029815674,0.07110596,-0.039520264,0.045959473,-0.012298584,0.011856079,0.021728516,-0.012390137,-0.06573486,-0.059265137,-0.026046753,0.021102905,-0.06323242,-0.037353516,-0.018859863,-0.008033752,0.019119263,-0.05783081,-0.0075416565,-0.009605408,0.054229736,0.015975952,0.06463623,-0.044891357,-0.04525757,-0.00023007393,-0.010726929,0.020828247,-0.024246216,0.017593384,-0.047058105,-0.016708374,-0.04309082,0.015533447,-0.023529053,0.039520264,-0.013381958,-0.049560547,0.006465912,0.0037708282,-0.03591919,-0.04812622,-0.00484848,-0.058898926,0.039855957,-0.02218628,0.05029297,-0.015808105,0.007633209,-0.004108429,0.07397461,0.039520264,0.023529053,0.002199173,-0.0038394928,-0.008842468,-0.005744934,0.04058838,-0.015357971,-0.029632568,0.006286621,0.0054779053,0.048858643,-0.008621216,-0.04257202,-0.031951904,-0.015975952,-0.0025482178,0.003435135,0.04525757,-0.040222168,0.07006836,0.029815674,-0.019119263,0.036621094,0.005882263,-0.01436615,0.04562378,0.0017738342,-0.014724731,-5.64456e-05,0.013824463,-0.0028057098,0.01625061,0.03286743,-0.035369873,-0.035186768,-0.007095337,0.006690979,-0.01625061,-0.010955811,-0.027114868,0.008079529,-0.031799316,0.039855957,0.01751709,-0.010147095,-0.011940002,-0.052093506,0.014007568,0.0075416565,0.012748718,0.059631348,0.043823242,0.015625,0.01436615,-0.012214661,0.016784668,-0.049926758,0.023162842,-0.007949829,0.0033664703,-0.018051147,-0.009246826,-0.033050537,-0.00687027,0.019210815,-0.03591919,-0.004333496,0.02909851,-0.06536865,0.009651184,0.019927979,0.0025596619,0.034118652,0.014190674,0.006465912,-0.020385742,0.009651184,0.011047363,-0.024246216,-0.047058105,0.020645142,0.040771484,-0.017333984,0.037719727,-0.007904053,0.0184021,0.008979797,-0.030166626,0.06323242,-0.023345947,0.05496216,0.023712158,-0.026931763,-0.008796692,0.0003759861,0.0012960434,0.042907715,-0.10845947,-0.0035915375,-0.038604736,-0.009521484,-0.08581543,-0.025314331,-0.013648987,-0.01687622,0.056762695,0.021011353,0.017059326,0.0020771027,0.038085938,0.0028057098,0.025146484,0.021194458,-0.038970947,-0.034484863,0.05819702,0.010643005,0.014724731,0.008483887,0.060333252,0.0131073,0.005836487,0.00843811,-0.02909851,0.033569336,-0.023529053,-0.0022792816,-0.033233643,0.02998352,0.007633209,0.050628662,-0.04525757,0.025146484,0.006374359,0.0131073,-0.028198242,-0.07147217,-0.0073165894,0.02748108,-0.03555298,0.010688782,-0.0061950684,-0.023880005,-0.002626419,0.049194336,0.017425537,0.035186768,0.01966858,-0.0018968582,-0.040405273,0.031951904,0.003501892,-0.044006348,-0.009475708,-0.0209198,0.010192871,-0.056762695,0.023162842,0.031799316,-0.00047707558,0.034484863,0.004512787,0.0004069805,-0.026931763,-0.033569336,0.034301758,-0.040771484,-0.049560547,0.008354187,0.066101074,0.020385742,0.026931763,0.052093506,-0.012931824,-0.044891357,-0.013557434,-0.012031555,0.015533447,-0.027114868,0.026763916,-0.0027618408,0.0021324158,0.026046753,0.028015137,0.04849243,-0.047058105,0.0027942657,-0.012214661,-0.04309082,0.0027389526,0.0025253296,-0.024246216,-0.010910034,0.0013132095,-0.0032997131,0.041290283,0.044189453,0.028915405,-0.037719727,-0.010818481,0.027114868,-0.03717041,0.089416504,0.016067505,-0.021102905,-0.04345703,0.02909851,-0.035736084,-0.004131317,0.040039062,0.011131287,-0.020385742,0.0022449493,-0.0014820099,0.025863647,0.0005135536,-0.015625,0.0033226013,-0.005455017,0.0287323,0.012214661,0.030166626,-0.0012626648,-0.00040960312,-0.033569336,-0.015899658,-0.0016498566,-0.04812622,0.020111084,0.054595947,0.030532837,-0.014816284,0.005680084,-0.067871094,-0.0011901855,-0.007987976,0.09698486,0.048858643,0.012390137,0.030715942,-0.031433105,0.008033752,0.023712158,0.013916016,0.02029419,0.041656494,-0.025314331,-0.014099121,-0.023712158,0.049194336,0.016342163,0.03842163,-0.0014314651,0.020736694,0.038085938,0.035186768,-0.027114868,0.020477295,0.022628784,-0.067871094,0.044006348,-0.019042969,-0.021453857,-0.049926758,-0.018310547,0.007587433,0.022094727,0.007904053,-0.005836487,-0.035369873,-0.0067329407,0.030532837,-0.06933594,0.017150879,0.023712158,-0.000813961,0.01966858,-0.015625,-0.019119263,0.021194458,-0.044708252,-0.05996704,-0.039520264,0.041290283,0.0061950684,-0.028915405,-0.031433105,0.01751709,-0.00013113022,6.347895e-05,0.045440674,-0.0076789856,0.021820068,0.04812622,0.06500244,-0.0036811829,-0.04812622,0.011940002,-0.025146484,-0.03591919,0.015174866,0.028549194,-0.011581421,-0.02998352,-0.0026817322,0.031066895,-0.03125,-0.006958008,0.018951416,-0.0057907104,-0.10345459,0.0038394928,0.045440674,0.041290283,0.05819702,-0.013198853,-0.030532837,0.027648926,-0.04257202,-0.0184021,0.022262573,0.0075416565,-0.018585205,0.0002090931,0.00063991547,0.021102905,0.020645142,0.049194336,-0.049560547,-0.033416748,-0.033233643,0.031951904,0.019927979,-0.07183838,-0.03753662,0.0061950684,0.03933716,0.045959473,-0.007095337,-0.0625,-0.014549255,-0.03250122,0.006061554,0.0066452026,0.0074539185,-0.006690979,0.010955811,0.012298584,-0.011405945,0.030715942,-0.000998497,0.011451721,0.043640137,0.007183075,-0.050994873,0.014007568,-0.034301758,-0.019042969,-0.021911621,-0.00037312508,-0.015625,0.00034928322,-0.008125305,-0.0524292,0.018585205,0.022537231,-0.04058838,-0.0029411316,-0.0033664703,-0.0262146,-0.019042969,0.031951904,0.02406311,-0.0011339188,0.004825592,-0.0062408447,-0.015625,0.014724731,0.028549194,-0.033050537,-0.008171082,0.027297974,0.014190674,0.001964569,0.02998352,-0.06427002,-0.006690979,-0.07006836,0.02909851,-0.012481689,-0.0023002625,-0.015625,-0.03286743,-0.06970215,-0.0019874573,0.034301758,-0.012748718,-0.04147339,-0.006690979,0.0054092407,-0.005634308,-0.017959595,0.0074539185,-0.010818481,-0.012840271,-0.030166626,-0.011672974,0.040405273,-0.027114868,0.023345947,0.028915405,-0.0053634644,-0.024597168,0.024780273,0.0057029724,-0.014190674,-0.031951904,-0.021194458,0.017242432,0.014907837,-0.013557434,0.005882263,0.009117126,-0.009429932,-0.018859863,-0.019927979,0.024780273,0.030715942,0.00687027,-0.009925842,-0.009475708,0.023712158,-0.012748718,0.038085938,0.0524292,0.026931763,-0.04776001,0.00390625,0.01436615,-0.023880005,-0.04776001,-0.0005698204,-0.00907135,-0.02154541,0.040222168,0.043640137,0.04776001,0.01687622,0.030349731,0.044189453,0.07647705,0.053863525,-0.01751709,-0.050628662,-0.005432129,0.006286621,-0.03286743,-0.001750946,-0.000103116035,0.025497437,-0.088012695,-0.012931824,-0.005027771,0.022537231,0.009788513,0.0126571655,0.01374054,0.088378906,0.0055656433,0.0181427,0.021636963,0.0060157776,0.0007686615,0.023880005,0.024246216,-0.03503418,-0.011314392,-0.010009766,-0.007587433,-0.028198242,0.0057258606,0.003074646,0.022445679,0.031066895,0.050628662,0.028381348,-0.038970947,0.011856079,0.044525146,0.033233643,0.066467285,-0.024963379,-0.021636963,-0.03213501,-0.01939392,-0.023345947,0.025863647,0.0031871796,-0.003074646,-0.038085938,0.023345947,-0.06536865,0.042755127,0.025146484,-0.042388916,-0.005836487,0.04095459,-0.04776001,-0.06750488,-0.006374359,0.031433105,-0.03717041,-0.034301758,0.051361084,0.023712158,-0.018951416,-0.05709839,-0.009521484,0.06573486,-0.031951904,-0.067871094,0.050994873,-0.025497437,-0.02154541,-0.058898926,-0.015266418,-0.013465881,-0.0044441223,-0.051361084,0.007858276,-0.013198853,0.022903442,-0.03375244,-0.023880005,-0.047058105,-0.009292603,-0.051727295,0.00056123734,-0.02998352,0.023880005,0.07470703,0.075805664,-0.0036144257,0.005142212,0.03503418,-0.037353516,-0.016967773,0.03503418,0.015716553,-0.038085938,0.009025574,0.05529785,-0.041656494,0.00843811,0.012840271,-0.002166748,-0.022094727,-0.024597168,-0.021453857,0.0074539185,0.037902832,0.019210815,-0.013198853,-0.0009317398,-0.02909851,0.031433105,-0.017425537,0.023162842,-0.03250122,-0.00920105,0.0013914108,-0.03842163,0.0028514862,-0.03842163,0.015357971,0.026397705,0.010192871,0.028549194,0.011672974,0.012390137,0.0044441223,-0.041137695,-0.015357971,-0.0013017654,-0.01777649,-0.023529053,-0.0074539185,-0.03753662,-0.027832031,-0.044525146,-0.0067329407,0.035736084,0.0057258606,-0.044708252,0.031066895,-0.007633209,0.046691895,-0.017593384,0.030166626,-0.05496216,0.009246826,0.0013866425,0.040222168,0.010688782,0.014274597,-0.040039062,0.009338379,0.034484863,-0.0075416565,0.0524292,0.035736084,0.021194458,-0.049194336,0.0013132095,-0.04095459,0.031066895,-0.056396484,0.051727295,-0.042907715,0.0062408447,-0.04812622,0.028381348,-0.024429321,0.06573486,0.02658081,0.005455017,-0.08441162,-0.07434082,-0.049926758,-0.009521484,-0.0181427,0.02128601,0.026763916,0.0004799366,0.02658081,-0.0014028549,0.03286743,0.016616821,0.022720337,-0.031799316,0.07507324,0.007904053,-0.018051147,-0.022903442,0.012573242,0.024780273,-0.017593384,-0.011856079,0.008079529,0.009605408,0.010818481,-0.025146484,0.0209198,-0.025314331,-0.00843811,-0.01109314,-0.03717041,0.0005722046,-0.011131287,0.00043201447,-0.02998352,-0.03286743,-0.09265137,0.016159058,0.0033664703,-0.008354187,-0.042022705,0.009880066,0.025146484,0.0011615753,0.01625061,-0.0077209473,0.023880005,0.0009937286,0.021636963,-0.025863647,0.029449463,-0.014457703,-0.088378906,0.03753662,0.016067505,-0.0524292,-0.013381958,0.0036582947,-0.0039520264,0.028198242,0.0057907104,-0.042388916,-0.005744934,0.0020656586,0.022003174,0.019851685,0.007003784,-0.028015137,-0.0020198822,0.043640137,-0.041137695,-0.0287323,0.022003174,-0.0067329407,0.021728516,0.006690979,0.023345947,0.024963379,0.02998352,-0.006286621,0.031433105,0.027832031,-0.033935547,0.007183075,-0.007904053,0.045440674,0.00687027,-0.018310547,0.027832031,-0.06713867,0.012298584,0.003255844,-0.07293701,-0.016784668,0.07324219,0.055664062,-0.019485474,0.0418396,0.03591919,0.019851685,-0.004623413,-0.026931763,-0.030349731,0.0033664703,0.010238647,-0.01499176,-0.015357971,-0.0032100677,0.021194458,0.03286743,-0.031066895,-0.0018405914,-0.008216858,0.061065674,0.038085938,-0.014099121,-0.09124756,0.026046753,0.034118652,0.014633179,0.0074539185,0.011489868,-0.011940002,0.061065674,-0.056396484,-0.013465881,-0.02218628,0.060333252,-0.01939392,-0.039520264,0.028015137,0.044891357,0.011222839,-0.021820068,0.008796692,0.038085938,-0.060333252,0.026046753,0.04437256,-0.012481689,0.08654785,-0.0011167526,-0.015266418,0.047424316,0.032318115,0.005882263,-0.016159058,-0.049560547,0.0060157776,-0.045959473,0.0625,0.036468506,0.025146484,-0.014457703,-0.049926758,-0.029266357,-0.012031555,0.046325684,0.028381348,0.027832031,0.006690979,-0.01625061,0.0287323,0.022445679,0.022628784,-0.0004684925,-0.039855957,-0.019760132,-0.032318115,0.01436615,0.0055885315,-0.059265137,0.00071811676,-0.00018382072,-0.0005221367,0.0006623268,-0.00054979324,-0.013824463,0.022979736,-0.013290405,0.0362854,-0.006778717,-0.013465881,0.029815674,-0.03753662,0.06463623,0.04849243,0.0015268326,0.004310608,0.04257202,0.025146484,-0.0072288513,0.015357971,0.027114868,0.028549194,0.034851074,0.013648987,-0.006511688,-0.0362854,0.035369873,-0.00655365,-0.07006836,0.023529053,0.009925842,-0.0060157776,-0.011672974,-0.00021326542,-0.00983429,-0.03878784,0.030883789,0.012123108,-0.020568848,-0.01436615,-0.0061073303,0.016616821,-0.020019531,0.06536865,-0.03555298,-0.014724731,0.011856079,-0.001953125,0.0014705658,0.0131073,-0.009025574,0.0725708,0.011314392,0.07434082,0.006958008,0.010505676,-0.0524292,0.01436615,-0.022537231,-0.044525146,0.014099121,-0.013023376,-0.027114868,0.003232956,0.003232956,0.030349731,0.025680542,0.080444336,0.0051841736,-0.00843811,0.032684326,-0.021194458]", "client_status_at": "2025-10-02T08:53:46.047000Z", "server_status_at": "2025-10-02T09:51:05.077464Z", "created_at": "2025-10-02T08:51:01.699799Z"}, {"id": "adb9a52a-4c0a-4028-86c9-1b623b14f864", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://www.theguardian.com/us-news/2025/sep/22/trump-h1b-visa-fee-explainer", "canonical_url": "https://www.theguardian.com/us-news/2025/sep/22/trump-h1b-visa-fee-explainer", "title": "What to know about Trump’s new $100,000 H-1B visa fee", "source_site": "The Guardian", "publication_date": "2025-09-22T00:00:00Z", "favicon_url": "https://www.theguardian.com/favicon.ico", "content_markdown": "[Donald Trump’s](https://www.theguardian.com/us-news/donaldtrump) administration has announced an annual $100,000 fee on H-1B visa applications, a type of US visa that allows companies to hire foreign workers in skilled occupations such as IT, healthcare and engineering.\n\nThe new proclamation, signed on Friday, brought a mixed bag of emotions for hundreds of thousands of workers on H-1B visas, as well as many question. Here’s what you need to know.\n\n## What is an H-1B visa?\n\nThe H-1B program dates back to 1990, when George HW Bush [signed a new bill into law](https://www.presidency.ucsb.edu/documents/statement-signing-the-immigration-act-1990) in an effort to encourage the immigration of highly skilled workers, including scientists, engineers and educators, to address labor shortages in specialized fields.\n\nThis temporary visa category allows employers to petition for “highly educated” foreign professionals to work in “specialty occupations” that require at least a bachelor’s degree or the equivalent. The H-1B visa is typically granted for an initial period of three years and can be extended up to a maximum of six years.\n\nThere are as many as 730,000 H-1B holders in the US, and an additional 550,000 dependents, including spouses and children, who together make up nearly 1.3 million residents, according to a January 2025 [report](https://www.fwd.us/news/h1b-visa-program/) from fwd.us, an immigration and criminal justice advocacy group.\n\nPreviously, the cost for an H-1B ranged from about [$1,700 to $4,500](https://international.indianapolis.iu.edu/visas/departments/h1b-workers/h1b-fees.html), depending on whether the visa was expedited. Each year, Congress caps H-1B visas at 85,000, awarded through a lottery system. To enter, companies pay a $215 registration fee, followed by the thousands of dollars more in application fees and legal costs if selected.\n\n## What changes did Trump announce?\n\nThe [Trump administration](https://www.theguardian.com/us-news/trump-administration) increased the fee for skilled foreign workers applying for H-1B visas to $100,000, claiming the visas were being “abused” to undercut American wages and outsource IT jobs. The new fee went into effect on Sunday.\n\nThe fees are intended to be paid by companies sponsoring the visas, rather than applicants themselves. The Trump administration implemented the fee in an effort to encourage companies to hire more American workers.\n\n“Either the person is very valuable to the company and America, or they are going to depart and the company is going to hire an American,” said US commerce secretary Howard Lutnick at Friday’s briefing. “And that’s the point of immigration. Hire Americans and make sure the people coming in are the top, top people. Stop the nonsense.”\n\nOn Tuesday, the Trump administration proposed a new process that would move away from the current random selection process to a weighted selection process, according to a [Federal Register notice]. The proposed process would give more weight to H-1B applications for roles with the highest wages if applications for the visas exceed the annual cap of 85,000.\n\nRoles that meet or exceed the highest level of wages for the industry, as laid out by the Bureau of Labor Statistics, would be entered into the visa selection pool four times, roles that meet or exceed the second-highest level of wages would be entered three times and so on. DHS said that, if the proposal is finalized, the goal would be to incentivize “employers to offer higher wages, or to petition for positions requiring higher skills and higher skilled aliens, that are commensurate with higher wage levels”.\n\n“The proposed process would favor the allocation of H-1B visas to higher skilled and higher paid aliens, while maintaining the opportunity for employers to secure H-1B workers at all wage levels,” the proposal reads.\n\nTrump thoughts on H-1B have been anything but steady. Last year, the president told [the New York Post](https://nypost.com/2024/12/28/us-news/donald-trump-backs-h-1b-visa-program-supported-by-elon-musk/) he supported the program, calling himself a “believer” in the visas, and that he has used it “many times”.\n\nBut, more recently, he said that adding such costly hurdles to the visa process will help protect American jobs. “I think it’s going to be a fantastic thing, and we’re going to take that money and we’re going to reduce taxes, we’re going to reduce debt,” Trump said on Friday.\n\n## What industries are affected?\n\nTech workers [make up one of the largest groups](https://www.sfchronicle.com/projects/2025/bay-area-h1b-visa-tracker/) of H-1B visa recipients, and the fee announcement sent the tech industry scrambling, with businesses in Silicon Valley urging staff not to travel outside the country amid early confusion over the new process.\n\nStem (science, technology, engineering and mathematics) industries rely heavily on the H-1B program, with roughly two-thirds of H-1B jobs in computer-related roles. [According to US Citizenship and Immigration Services](https://www.uscis.gov/tools/reports-and-studies/h-1b-employer-data-hub), Amazon, Google, Meta, Microsoft and Apple were among the companies that employed the most H-1B visa holders last year.\n\nSeveral prominent [tech industry leaders](https://timesofindia.indiatimes.com/world/us/h-1b-visa-success-stories-elon-musk-sundar-pichai-and-other-h-1b-visa-holders-who-shape-the-future-of-the-us-tech-industry/articleshow/124037259.cms), including Elon Musk, Alphabet chief executive Sundar Pichai and Microsoft chief executive Satya Nadella, were at one point H-1B visa holders.\n\nEmployers also use the program to bring in educators and healthcare workers to the US.\n\n## What has the reaction been?\n\nEconomists have warned that the move could lead to [weaker economic growth](https://www.theguardian.com/business/2025/sep/22/trump-h-1b-visa-fees-us-economy-tech-india), since higher costs for employers will make it harder to attract foreign talent. They have also pointed to a potential “brain drain” as skilled international graduates are forced to leave the US.\n\nIn India, the largest beneficiary of H-1B visas last year, officials [said the fee](https://www.theguardian.com/world/2025/sep/22/trump-india-h1-b-visa-fee-hike-response-afraid-of-talent) would have humanitarian consequences “by way of the disruption caused for families”.\n\nThe Indian government said it “hopes that these disruptions can be addressed suitably by the US authorities” and emphasised that the exchange of skilled workers had “contributed enormously” to both nations.\n\nOther business leaders, however, have praised Trump’s move, including Netflix co-founder Reed Hastings. In a [post on X](https://x.com/reedhastings/status/1969828972688248953?s=42), Hastings said he had worked on H-1B politics for 30 years and said the added cost would allot visas for “very high-value jobs”. He said the move would eliminate the lottery and give employers more certainty.\n\nMost major Silicon Valley companies have not publicly commented. Companies have sent internal memos to employees.\n\nBefore the new fee was announced, the visa had been contested across the political spectrum. The democratic socialist senator Bernie Sanders, for example, [has been a longtime critic](https://thehill.com/policy/technology/5064132-sanders-criticizes-musk-h1b-visa/) of the H-1B visa, arguing that the program replaces American jobs with cheaper international workers.\n\n## Do people who already hold an H-1B visa have to worry?\n\nLutnick said the fee would be paid annually, and would only apply to people seeking a new visa.\n\nAmid initial confusion over the weekend, White House press secretary Karoline Leavitt [clarified](https://x.com/PressSec/status/1969495900478488745) that the fee applies only to new applicants, and that those who already hold H-1B visas and are currently outside of the country would not be charged $100,000 to re-enter.\n\n“H-1B visa holders can leave and re-enter the country to the same extent as they normally would; whatever ability they have to do that is not impacted by yesterday’s proclamation,” Leavitt said. “This applies only to new visas, not renewals, and not current visa holders.”", "content_text": "Donald Trump’s administration has announced an annual $100,000 fee on H-1B visa applications, a type of US visa that allows companies to hire foreign workers in skilled occupations such as IT, healthcare and engineering.\nThe new proclamation, signed on Friday, brought a mixed bag of emotions for hundreds of thousands of workers on H-1B visas, as well as many question. Here’s what you need to know.\nWhat is an H-1B visa?\nThe H-1B program dates back to 1990, when George HW Bush signed a new bill into law in an effort to encourage the immigration of highly skilled workers, including scientists, engineers and educators, to address labor shortages in specialized fields.\nThis temporary visa category allows employers to petition for “highly educated” foreign professionals to work in “specialty occupations” that require at least a bachelor’s degree or the equivalent. The H-1B visa is typically granted for an initial period of three years and can be extended up to a maximum of six years.\nThere are as many as 730,000 H-1B holders in the US, and an additional 550,000 dependents, including spouses and children, who together make up nearly 1.3 million residents, according to a January 2025 report from fwd.us, an immigration and criminal justice advocacy group.\nPreviously, the cost for an H-1B ranged from about $1,700 to $4,500, depending on whether the visa was expedited. Each year, Congress caps H-1B visas at 85,000, awarded through a lottery system. To enter, companies pay a $215 registration fee, followed by the thousands of dollars more in application fees and legal costs if selected.\nWhat changes did Trump announce?\nThe Trump administration increased the fee for skilled foreign workers applying for H-1B visas to $100,000, claiming the visas were being “abused” to undercut American wages and outsource IT jobs. The new fee went into effect on Sunday.\nThe fees are intended to be paid by companies sponsoring the visas, rather than applicants themselves. The Trump administration implemented the fee in an effort to encourage companies to hire more American workers.\n“Either the person is very valuable to the company and America, or they are going to depart and the company is going to hire an American,” said US commerce secretary Howard Lutnick at Friday’s briefing. “And that’s the point of immigration. Hire Americans and make sure the people coming in are the top, top people. Stop the nonsense.”\nOn Tuesday, the Trump administration proposed a new process that would move away from the current random selection process to a weighted selection process, according to a Federal Register notice. The proposed process would give more weight to H-1B applications for roles with the highest wages if applications for the visas exceed the annual cap of 85,000.\nRoles that meet or exceed the highest level of wages for the industry, as laid out by the Bureau of Labor Statistics, would be entered into the visa selection pool four times, roles that meet or exceed the second-highest level of wages would be entered three times and so on. DHS said that, if the proposal is finalized, the goal would be to incentivize “employers to offer higher wages, or to petition for positions requiring higher skills and higher skilled aliens, that are commensurate with higher wage levels”.\n“The proposed process would favor the allocation of H-1B visas to higher skilled and higher paid aliens, while maintaining the opportunity for employers to secure H-1B workers at all wage levels,” the proposal reads.\nTrump thoughts on H-1B have been anything but steady. Last year, the president told the New York Post he supported the program, calling himself a “believer” in the visas, and that he has used it “many times”.\nBut, more recently, he said that adding such costly hurdles to the visa process will help protect American jobs. “I think it’s going to be a fantastic thing, and we’re going to take that money and we’re going to reduce taxes, we’re going to reduce debt,” Trump said on Friday.\nWhat industries are affected?\nTech workers make up one of the largest groups of H-1B visa recipients, and the fee announcement sent the tech industry scrambling, with businesses in Silicon Valley urging staff not to travel outside the country amid early confusion over the new process.\nStem (science, technology, engineering and mathematics) industries rely heavily on the H-1B program, with roughly two-thirds of H-1B jobs in computer-related roles. According to US Citizenship and Immigration Services, Amazon, Google, Meta, Microsoft and Apple were among the companies that employed the most H-1B visa holders last year.\nSeveral prominent tech industry leaders, including Elon Musk, Alphabet chief executive Sundar Pichai and Microsoft chief executive Satya Nadella, were at one point H-1B visa holders.\nEmployers also use the program to bring in educators and healthcare workers to the US.\nWhat has the reaction been?\nEconomists have warned that the move could lead to weaker economic growth, since higher costs for employers will make it harder to attract foreign talent. They have also pointed to a potential “brain drain” as skilled international graduates are forced to leave the US.\nIn India, the largest beneficiary of H-1B visas last year, officials said the fee would have humanitarian consequences “by way of the disruption caused for families”.\nThe Indian government said it “hopes that these disruptions can be addressed suitably by the US authorities” and emphasised that the exchange of skilled workers had “contributed enormously” to both nations.\nOther business leaders, however, have praised Trump’s move, including Netflix co-founder Reed Hastings. In a post on X, Hastings said he had worked on H-1B politics for 30 years and said the added cost would allot visas for “very high-value jobs”. He said the move would eliminate the lottery and give employers more certainty.\nMost major Silicon Valley companies have not publicly commented. Companies have sent internal memos to employees.\nBefore the new fee was announced, the visa had been contested across the political spectrum. The democratic socialist senator Bernie Sanders, for example, has been a longtime critic of the H-1B visa, arguing that the program replaces American jobs with cheaper international workers.\nDo people who already hold an H-1B visa have to worry?\nLutnick said the fee would be paid annually, and would only apply to people seeking a new visa.\nAmid initial confusion over the weekend, White House press secretary Karoline Leavitt clarified that the fee applies only to new applicants, and that those who already hold H-1B visas and are currently outside of the country would not be charged $100,000 to re-enter.\n“H-1B visa holders can leave and re-enter the country to the same extent as they normally would; whatever ability they have to do that is not impacted by yesterday’s proclamation,” Leavitt said. “This applies only to new visas, not renewals, and not current visa holders.”", "content_token_count": 1450, "client_status": "queued", "server_status": "embedded", "summary": "The Trump administration has introduced a new $100,000 annual fee for H-1B visa applications, significantly increasing the cost from the previous range of $1,700 to $4,500. This move aims to encourage companies to hire more American workers and is expected to impact various industries, particularly tech, where H-1B visas are heavily utilized.", "expiry_score": 0.3, "mistral_embedding": "[-0.058685303,0.0068855286,0.0413208,0.0037574768,0.040618896,0.0075149536,0.03149414,-0.022445679,-0.024154663,0.0149383545,-0.058685303,0.035247803,0.019500732,0.017089844,-0.04724121,-0.0023822784,0.024154663,0.045806885,0.030593872,-0.043823242,-0.03363037,-0.039886475,-0.0357666,-0.00818634,-0.0014419556,-0.057617188,-0.043640137,-0.08300781,-0.03793335,-0.011138916,-0.014404297,-0.013061523,-0.009841919,0.005859375,0.0023822784,-0.014755249,0.004852295,-0.030410767,0.015205383,-0.008407593,-0.016998291,0.031311035,-0.0014028549,-0.021560669,-0.014846802,-0.0107803345,0.03112793,0.0070228577,-0.009254456,-0.011177063,0.006038666,0.08691406,0.02218628,-0.015113831,0.015838623,0.04095459,0.01725769,-0.032562256,-0.005367279,0.04345703,-0.033447266,0.016723633,0.0036678314,-0.03668213,0.001039505,0.029342651,0.0028057098,-0.0357666,0.011222839,0.029342651,0.011405945,-0.023971558,-0.023788452,-0.0035324097,-0.029693604,-0.07904053,0.0015649796,0.004673004,0.058685303,0.00724411,-0.027008057,0.0028057098,0.028625488,-0.05581665,-0.025756836,0.021560669,-0.0069770813,-0.0079193115,-0.02897644,0.011627197,0.0042266846,-0.028625488,0.03363037,-0.03451538,0.035247803,0.030944824,-0.035949707,-0.01386261,-0.0021247864,0.005142212,0.011001587,-0.008270264,0.036132812,-0.010063171,0.017166138,-0.032196045,0.031829834,-0.005256653,0.001033783,-0.035949707,-0.059387207,-0.027908325,0.04345703,-0.07373047,-0.020477295,0.020217896,-0.027908325,0.04864502,-0.06585693,-0.023971558,0.030593872,-0.008361816,0.012794495,0.027374268,-0.059387207,-0.027542114,0.018066406,0.054382324,-0.0005645752,0.00010061264,-0.012878418,0.005455017,-0.0011177063,-0.037384033,-0.019317627,0.0057029724,0.06585693,-0.040618896,-0.005947113,-0.060821533,0.021820068,-0.033447266,-0.04043579,0.012878418,0.0010290146,-0.009300232,-0.013771057,0.022003174,-0.04864502,-0.010063171,-0.030410767,0.0715332,0.044891357,-0.00055646896,0.015113831,-0.018341064,-0.0033550262,-0.02809143,0.04043579,0.0051879883,-0.01725769,0.010284424,-0.025222778,0.045440674,-0.027542114,-0.012702942,-0.02897644,0.021102905,-0.0057029724,0.0028839111,0.06225586,-0.014129639,0.032562256,0.046142578,-0.047576904,0.0031986237,0.013595581,-0.016998291,0.056884766,0.032562256,-0.021652222,-0.023254395,0.018157959,0.0018787384,0.012969971,-0.046875,-0.05368042,-0.018341064,-0.007423401,0.01600647,-0.022628784,-0.06011963,-0.036132812,0.019134521,-0.0055236816,0.06011963,0.014671326,-0.018157959,0.04937744,-0.016540527,0.037200928,-0.0107803345,0.025222778,0.029342651,0.032928467,0.03488159,0.022903442,0.024154663,-0.01448822,-0.011001587,-0.00049734116,0.018157959,-0.029342651,-0.028442383,-0.030410767,-0.057617188,0.0018558502,-0.003129959,-0.0076942444,-0.043823242,-0.0031757355,-0.05508423,0.035247803,-0.026290894,-0.028259277,0.012161255,0.009971619,-0.015205383,-0.044006348,0.0046958923,0.032562256,0.019226074,-0.041137695,0.022903442,0.038269043,-0.010597229,0.037750244,0.04901123,-0.008140564,0.013771057,-0.03237915,0.043823242,-0.034698486,0.04650879,0.022903442,-0.06011963,-0.020843506,0.014045715,-0.0020008087,0.05581665,-0.11016846,-0.01878357,-0.025756836,-0.029342651,-0.03201294,-0.009750366,-0.017807007,-0.013328552,0.059753418,-0.0103302,-0.0014257431,-0.0055007935,0.030410767,-0.011001587,0.05368042,0.03970337,-0.03201294,0.027374268,0.044891357,0.013145447,-0.013687134,-0.0021800995,0.03845215,-0.012878418,0.031311035,-0.01663208,-0.004940033,0.06939697,-0.0007266998,0.022903442,-0.0036888123,0.023788452,0.015472412,0.021652222,-0.021911621,-0.024154663,0.028259277,0.013595581,0.008140564,-0.029876709,0.0084991455,0.003019333,0.011138916,-0.019226074,-0.041137695,-0.04724121,0.016815186,0.00015377998,0.01725769,0.059753418,-0.0008215904,-0.027191162,-0.070129395,-0.0006594658,0.011810303,-0.027008057,-0.01448822,-0.010238647,0.010238647,-0.070129395,0.010284424,0.015296936,0.0029067993,0.018157959,0.0115356445,-0.009880066,-0.06225586,-0.008628845,0.03881836,-0.04864502,-0.052947998,-0.016189575,0.06011963,-0.029876709,0.02809143,-0.00818634,0.008361816,-0.043121338,0.007827759,-0.021820068,0.0024700165,-0.015113831,0.018432617,-0.03112793,0.057617188,-0.008361816,0.057250977,0.028259277,-0.061553955,-0.0030403137,-0.008628845,0.0051193237,0.01663208,-0.011894226,-0.006259918,-0.058685303,-0.013328552,-0.019943237,0.03488159,0.014404297,0.0076942444,-0.01109314,0.005657196,0.046875,-0.013061523,0.092285156,-0.020309448,-0.014045715,-0.0031528473,0.052581787,-0.042388916,-0.04043579,0.0051879883,0.015472412,0.0096588135,-0.015472412,-0.03201294,0.04006958,0.05618286,0.022277832,0.0030403137,0.023788452,0.025939941,0.051879883,0.012161255,-0.013687134,-0.008903503,-0.0059928894,-0.0035114288,-0.0076026917,-0.059020996,0.03881836,0.035247803,0.0063972473,0.005054474,-0.008232117,-0.029342651,-0.00014674664,-0.020843506,0.06939697,0.086242676,0.021820068,0.031311035,-0.014312744,0.015472412,0.037750244,-0.00970459,0.01637268,0.039001465,-0.0055885315,0.007423401,-0.076538086,0.037017822,0.013061523,0.0037345886,0.040618896,-0.0067100525,0.023254395,0.029876709,-0.007648468,0.0072021484,0.01725769,-0.019317627,0.035949707,-0.043640137,0.00894165,-0.023971558,-0.0102005005,-0.024505615,0.0015993118,0.037750244,-0.021911621,-0.015388489,-0.012969971,0.058685303,-0.031829834,0.0013694763,-0.0035324097,-0.05117798,0.036315918,-0.041503906,-0.032562256,0.050079346,-0.038116455,-0.046875,0.030944824,0.012878418,-0.029159546,-0.022003174,-0.050445557,0.010063171,0.0042495728,0.010513306,0.032928467,-0.040252686,0.012969971,-0.0023822784,0.049743652,-0.011268616,-0.029159546,0.02281189,-0.016464233,0.0010175705,0.04345703,0.02218628,0.015388489,-0.054382324,-0.026290894,0.0065307617,-0.020751953,-0.030410767,0.022445679,-0.027374268,-0.07513428,-0.019943237,0.054382324,0.04650879,0.06439209,-0.035247803,-0.044555664,0.021820068,-0.008049011,-0.026290894,0.033996582,0.06365967,-0.029159546,0.032196045,-0.010017395,0.02558899,0.03451538,0.06439209,-0.014221191,3.4093857e-05,-0.07122803,0.031677246,0.042755127,-0.049743652,-0.059020996,0.017166138,0.023620605,0.030059814,-0.014755249,-0.035614014,-0.0015993118,-0.041870117,-0.033447266,-0.0002529621,0.0096588135,-0.022094727,-0.0069770813,0.013771057,-0.015388489,0.019317627,0.016540527,-0.012702942,0.0011405945,0.016906738,0.0014314651,0.028442383,-0.027374268,-0.01600647,0.010375977,0.049743652,-0.02772522,0.02218628,0.02432251,-0.047943115,0.0025596619,0.011810303,-0.012878418,-0.027542114,-0.018249512,-0.02684021,-0.040802002,0.008094788,0.0132369995,0.0011291504,-0.007827759,0.008094788,-0.02809143,-0.01171875,-0.024154663,-0.013420105,0.0115356445,-0.021728516,0.035949707,0.019592285,0.032562256,-0.034332275,-0.014846802,-0.07373047,0.01574707,-0.014755249,-0.0076026917,-0.028259277,-0.02468872,-0.052947998,-0.023971558,-0.0008330345,-0.026657104,0.002784729,-0.035247803,-0.0068855286,-0.0029735565,-0.0079193115,-0.012702942,-0.030227661,-0.014755249,-0.012611389,-0.029342651,0.059753418,-0.021911621,0.028808594,0.019226074,0.05795288,-0.00025844574,0.0234375,-0.009529114,0.0071105957,-0.045440674,-0.02558899,0.012519836,0.01940918,-0.026290894,0.059387207,0.029693604,0.0013923645,0.0079193115,-0.0034656525,0.008811951,0.0047836304,0.033996582,-0.030593872,-0.04220581,-0.035614014,-0.044189453,0.043640137,0.013595581,0.013420105,0.005947113,-0.026657104,0.04901123,-0.034332275,-0.009529114,-0.04345703,-0.0021572113,0.0012578964,0.043640137,-0.008987427,0.05795288,0.00088882446,-0.0038013458,0.019760132,0.07623291,0.039520264,-0.0051879883,-0.05618286,-0.011314392,0.00060653687,-0.038635254,-0.017807007,0.012878418,0.011627197,-0.051879883,-0.013420105,0.014129639,-0.0069770813,0.013687134,0.028442383,0.025405884,0.06011963,-0.00894165,-0.029876709,0.033081055,-0.02684021,-0.003376007,0.018341064,0.0019006729,0.0021018982,0.01109314,-0.0014533997,0.0062179565,-0.023620605,0.0042037964,0.017440796,-0.03326416,0.012878418,0.018692017,-0.020401001,-0.018249512,-0.0075149536,0.04006958,0.040252686,0.052246094,-0.022445679,-0.05581665,-0.009925842,-0.033813477,-0.015914917,0.029876709,0.045806885,-0.026473999,-0.029876709,0.018966675,-0.054016113,0.051879883,0.07727051,-0.006843567,-0.010551453,0.037384033,0.00308609,-0.068359375,0.020477295,-0.00818634,0.0063972473,-0.022537231,-0.043121338,0.033081055,-0.03451538,-0.038116455,-0.039520264,0.050811768,0.00831604,-0.03149414,-0.035614014,-0.034332275,-0.026123047,-0.038269043,-0.028442383,-0.009925842,0.009033203,0.0009727478,0.0076026917,-0.0070228577,0.0057678223,0.016540527,-0.023788452,-0.0178833,0.005077362,-0.04095459,0.018066406,0.006843567,0.0044937134,0.04220581,0.052581787,-0.012435913,0.005256653,-0.016723633,-0.012878418,-0.013504028,0.0058135986,0.019683838,-0.072631836,0.014404297,0.033081055,-0.035247803,-0.0016775131,0.009796143,-0.02684021,-0.030227661,9.012222e-05,0.019683838,0.0035324097,0.056518555,0.029876709,-0.051513672,0.0029735565,-0.011222839,0.044189453,-0.0011234283,0.009613037,-0.05331421,-0.0014867783,-0.017974854,0.009880066,0.01386261,-0.057617188,0.012611389,0.01574707,-0.0072021484,0.016189575,-2.7239323e-05,-0.014755249,0.006843567,-0.039001465,-0.0021800995,0.01663208,0.040252686,-0.06976318,-0.004047394,-0.013771057,-0.034332275,-0.03237915,-0.023788452,0.02128601,0.036499023,-0.0647583,0.023254395,-0.0019569397,0.013771057,0.015388489,0.047943115,-0.017089844,0.011985779,-0.039886475,0.037200928,-0.0044517517,0.02558899,-0.016540527,0.020401001,-0.0079574585,-0.025405884,0.026123047,0.02897644,0.0015649796,-0.029342651,-0.0013809204,-0.058685303,0.037384033,-0.012161255,0.031829834,-0.017166138,-0.017440796,-0.054016113,0.035247803,0.020126343,0.011451721,0.0070228577,-0.040252686,-0.044891357,-0.10089111,-0.05831909,0.0024604797,-0.0178833,0.017715454,0.012161255,0.0067100525,0.016723633,0.028442383,-0.013954163,0.0044517517,0.018600464,-0.019500732,0.05618286,-0.025222778,0.008720398,-0.028442383,-0.016098022,0.017349243,-0.014312744,0.0115356445,0.017715454,0.037200928,0.031311035,-0.08300781,-0.02772522,-0.00617218,0.025405884,-0.010108948,-0.015655518,-0.002046585,0.004470825,-0.01940918,-0.008361816,-0.017440796,-0.083740234,0.003019333,-0.019592285,-0.0132369995,0.0051002502,0.016815186,0.0037784576,-0.009529114,0.025405884,0.037384033,-0.029342651,-0.043640137,0.0084991455,-0.038269043,0.03149414,0.010734558,-0.06652832,0.057617188,-0.023071289,-0.0036678314,-0.007648468,0.042755127,-0.013954163,0.0032424927,-0.021820068,-0.030410767,0.029159546,0.023254395,0.008049011,0.0008778572,0.037017822,-0.026657104,-0.050079346,0.011627197,-0.019760132,-0.070495605,-0.019683838,-0.012702942,0.0055885315,-0.0075569153,0.0048980713,0.023071289,0.039001465,-0.02684021,-0.015838623,0.031829834,0.006259918,0.006439209,-0.0012578964,0.022354126,0.030410767,0.00894165,0.0065727234,-0.071899414,0.014579773,0.038635254,-0.08477783,-0.006038666,0.07873535,0.05545044,0.015205383,0.060821533,0.023971558,0.04345703,-0.0019006729,-0.01878357,-0.033813477,-0.011360168,0.043640137,0.0033550262,-0.059387207,0.04724121,0.09301758,-0.019760132,-0.012969971,0.028625488,-0.02809143,0.052581787,0.054382324,0.0076942444,-0.059753418,0.017532349,0.009483337,0.035614014,-0.0076942444,-0.015205383,-0.021652222,0.0647583,-0.09375,-0.009033203,-0.0075149536,0.054382324,0.0018558502,-0.01940918,0.0072021484,0.01574707,0.017166138,-0.018508911,-0.025405884,0.0075569153,0.0035114288,0.017974854,0.025405884,-0.0049858093,0.085876465,0.017166138,-0.008766174,0.004650116,0.019943237,0.00021517277,0.022094727,-0.039886475,-0.011047363,-0.0079193115,0.0234375,-0.04043579,-0.014404297,0.006439209,-0.038635254,-0.011627197,-0.0068855286,0.013328552,0.0031089783,0.0023479462,-0.0013532639,0.0039367676,0.048309326,0.008049011,-0.012077332,0.02897644,-0.01878357,-0.008270264,-0.04006958,-0.00818634,0.036132812,-0.02432251,0.018692017,0.023620605,0.038269043,0.0076942444,-0.04650879,-0.004135132,0.01637268,-0.058685303,0.04345703,-0.03149414,-0.041503906,-0.0054130554,-0.05795288,0.057250977,0.03668213,0.014045715,0.012969971,0.041503906,0.027374268,-0.008766174,-0.01725769,0.021194458,0.01386261,0.009796143,0.008857727,-0.0084991455,-0.025405884,0.047576904,0.020568848,-0.0413208,0.06335449,0.02684021,0.0206604,-0.0024929047,0.019683838,-0.031677246,-0.0769043,0.02772522,0.023788452,-0.021026611,0.012794495,-0.00818634,0.0039787292,-0.02003479,0.07513428,-0.05368042,-0.0019893646,0.02558899,0.019683838,-0.005077362,-0.038635254,0.004047394,0.054748535,-0.0010786057,0.08514404,0.046142578,-0.0013532639,-0.03543091,-0.021652222,-0.006439209,0.0060806274,0.002336502,-0.030410767,-0.038635254,0.0031757355,0.008361816,0.026657104,0.00069856644,0.06298828,0.0013189316,-0.0021915436,0.0031757355,-0.0018224716]", "client_status_at": "2025-10-02T09:50:56.796418Z", "server_status_at": "2025-10-02T09:50:56.796395Z", "created_at": "2025-10-02T08:50:51.739779Z"}, {"id": "13667bdc-32e2-4138-869f-aabba5f9911b", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://blog.langchain.com/how-to-build-an-agent/", "canonical_url": "https://blog.langchain.com/how-to-build-an-agent/", "title": "How to Build an Agent", "source_site": "LangChain Blog", "publication_date": "2025-07-10T00:00:00Z", "favicon_url": "https://blog.langchain.com/favicon.ico", "content_markdown": "While seemingly every company is talking about building agents this year, far fewer have done it. It’s easy to let your imagination run wild with how agents can transform your business, but many teams are unsure where to begin, how to make progress, and where to set expectations.\n\n*In this guide, we’ll walk through a framework for going from idea to impact— illustrated with a real-world example of building an email agent.*\n\n## Step 1: Define your agent’s job with examples\n\n**Choose something realistic and something that requires an agent.**\n\nPick something you could teach a smart intern. If your best intern could never complete the task given enough time and resources, the task may be unrealistic or too ambitious. Prove you can get the basics down before activating expert mode.\n\nStart by coming up with 5-10 concrete examples of the task. This serves two purposes:\n\n- First, it validates that your idea is well-scoped - not too trivial or vague\n- Second, gives you a benchmark for measuring performance later.\n\n**Example: Building an Email Agent**\n\nAt this step, we’d define what tasks our agent needs to handle, which likely includes:\n\n- Prioritize urgent emails from key stakeholders\n- Schedule meetings based on calendar availability\n- Ignore spam or emails that don't require responses\n- Answer product questions based on company documentation\n\n**Red flags to avoid:**\n\n- If you can’t come up with concrete examples, your scope is probably too broad.\n- Using an agent when traditional software would work better (e.g., when the logic is simple, fixed, and already implemented elsewhere)\n**.**Agents are slow, expensive, and can be finicky at times. If traditional software gets the job done - just use that! - Expecting magic that doesn't exist (e.g., connecting to APIs or datasets that don’t exist or can’t be built yet)\n\n## Step 2: Design operating procedure\n\n**Write up a detailed standard operating procedure (SOP), with step-by-step instructions for how a human would perform the task or process.**\n\nThis step helps confirm that you’ve chosen a problem with a clear, reasonable scope. It also surfaces the key steps, decisions, and tools your agent will likely need to handle—laying the groundwork for what to build.\n\n**Example: Building an Email Agent**\n\nFor our email agent, a step-by-step procedure could look like below:\n\n- Analyze email content and sender context to categorize response priority\n- Checks calendar availability; schedules video conference meeting\n- Draft a response based on the email, sender, and scheduling context\n- Send the email after a quick human review and approval\n\nWriting this out helps ensure the task is scoped appropriately, and surfaces the tools and logic our agent will need to handle.\n\n## Step 3: Build MVP with prompt\n\nChoosing a place to start is important. If your agent is complex, trying to do it all in one go is too ambitious. Start by designing the agent’s architecture outlined by the SOP: how it will flow, what decisions it needs to make, and where LLM reasoning is essential.\n\nThen, build an MVP by focusing on the **most critical LLM reasoning task(s)** (e.g., classification, decision-making) and **creating a prompt that handles them well.** Most agents fail because the LLM can't reason well enough for the task. Getting a single prompt working with hand-fed data will help you build up confidence before proceeding to build the full agent. [Prompt engineering tools like LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides?ref=blog.langchain.com) can help streamline this process, from managing prompt versions, to testing across scenarios or datasets, and tracking performance over time as you iterate.\n\n**Keep it simple by:**\n\n- Starting with manual inputs for any data or context the prompt needs (hold off on automation for now)\n- Testing against your outlined examples from Step 1 to validate performance across common use cases\n- Focusing on getting the LLM reasoning right\n\n**Example: Building an Email Agent**\n\nAt this stage, we’re identifying and solving *one* high-leverage reasoning task to start with.\n\nFor our email agent, that might mean focusing just on **classifying emails by urgency and intent** (e.g., meeting request, support questions), as this is a foundational step that the rest of the agent depends on.\n\nStart by writing a core prompt that does just this, with hand-fed inputs like:\n\n- Email content:\n*“Can we meet next week about LangChain’s product roadmap?”* - Sender: “\n*Jeff Bezos”, Title: “CEO of Amazon”* - Output:\n*Intent = “Meeting Request”, Urgency = “High”*\n\nOnce the model consistently gets this right across your test cases, you’ll have confidence that the core logic is sound—**and a strong foundation to build on.**\n\n## Step 4: Connect & Orchestrate\n\nNow that we have a working prompt, it’s time to **connect the prompt to real data and user inputs.**\n\nStart by identifying what context or data the prompt needs—such as email content, calendar availability, and documentation of products—and plan how to access it programmatically (e.g., via APIs, databases, or file systems).\n\nThen, write orchestration logic to connect the right data into your prompt. In simple cases, this might just mean passing inputs directly. For more complex workflows, you may need agentic logic to decide which data sources to query, when to call them, and how to combine their outputs before prompting the LLM.\n\n**Example: Building an Email Agent**\n\nFor our email agent, this step could involve integrating with the **Gmail API** (to read incoming emails), **Google Calendar API** (to check availability), and a **CRM or contact database** (to enrich sender context).\n\nWe’d then build orchestration logic like the following :\n\n- A new email triggers the agent\n- The agent fetches sender info from the CRM or via web search\n- It passes the full context into the prompt to determine urgency and whether a response is needed\n- If a meeting is appropriate, it checks calendar availability and proposes times\n- The agent drafts a response\n- After human review, it sends the email\n\n## Step 5: Test & Iterate\n\nBegin by **manually testing** your MVP using the examples you defined in Step 1. The goal is to verify that your agent is producing reasonable, accurate outputs for your core use cases. If your system involves multiple LLM calls or steps, it’s helpful to **set up tracing** using tools like [LangSmith](https://docs.smith.langchain.com/?ref=blog.langchain.com) to visualize the flow and debug how decisions are made at each stage.\n\nOnce manual testing is solid, **scale to automated testing** to ensure consistency and catch edge cases. Teams will often beef up examples to a few dozen to get a better sense of the agent’s strengths and weaknesses. This also helps you quantify performance before adding more complexity:\n\n- Run all examples (original + new) programmatically through your agent\n- Define automated success metrics — this forces clarity around your agent’s expected behavior\n- Use human review selectively to catch issues that metrics might miss\n\n**Example: Building an Email Agent**\n\nFor the email agent, we’d want to define and test success across several key areas:\n\n**Tone and Safety:**Responses should be professional, respectful, and free of hallucinated or inappropriate content**Intent & Priority Detection:**Emails should be correctly categorized and prioritized based on sender and content**Tool Usage Efficiency:**The agent should trigger only the necessary tools (e.g., avoid checking the calendar if no scheduling is required)**Draft Quality:**Suggested replies should be clear, relevant, and accurate based on the input context\n\n## Step 6: Deploy, Scale, and Refine\n\nOnce your MVP is performing reliably, begin expanding its scope—adding new capabilities, broader use cases, or even multi-agent workflows. For every new feature or integration, **repeat the testing process** from Step 5 to ensure you’re not breaking existing functionality.\n\nWhen ready, deploy to production in users' hands. [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/?ref=blog.langchain.com) allows you to quickly ship, scale, and manage your agents with one-click deployment.\n\nMonitor how people actually use your agent. Tools like LangSmith let you trace your agent’s actions in real time, making it easier to spot spikes in cost, accuracy issues, or latency. Real-world usage often differs from your initial assumptions, and these insights can reveal gaps, surface unexpected needs, and guide prioritization during your next iteration.\n\nThe key is treating launch as the beginning of iteration, not the end of development.\n\n**Example: Building an Email Agent**\n\nAfter deploying our email agent, we might discover unaddressed use cases through monitoring traffic and common use cases.\n\nThese emerging patterns signal opportunities to expand scope. From there, we can iteratively add new integrations and update our prompts and orchestration logic—always validating each addition with tests and user feedback before scaling further.\n\n## Conclusion\n\nThis process is designed to help you build agents that are grounded in clear use cases, tested against real examples, and shaped by real-world feedback. It’s not just about getting an agent to run, but about building something useful, reliable, and aligned with how people actually work.\n\nWhether you're automating email triage or orchestrating complex workflows, these six steps offer a practical path from idea to impact. But the work doesn’t stop at deployment—**the best agents are built through iteration.**\n\nSo start small, stay user-focused, and keep refining.", "content_text": "While seemingly every company is talking about building agents this year, far fewer have done it. It’s easy to let your imagination run wild with how agents can transform your business, but many teams are unsure where to begin, how to make progress, and where to set expectations.\nIn this guide, we’ll walk through a framework for going from idea to impact— illustrated with a real-world example of building an email agent.\nStep 1: Define your agent’s job with examples\nChoose something realistic and something that requires an agent.\nPick something you could teach a smart intern. If your best intern could never complete the task given enough time and resources, the task may be unrealistic or too ambitious. Prove you can get the basics down before activating expert mode.\nStart by coming up with 5-10 concrete examples of the task. This serves two purposes:\n- First, it validates that your idea is well-scoped - not too trivial or vague\n- Second, gives you a benchmark for measuring performance later.\nExample: Building an Email Agent\nAt this step, we’d define what tasks our agent needs to handle, which likely includes:\n- Prioritize urgent emails from key stakeholders\n- Schedule meetings based on calendar availability\n- Ignore spam or emails that don't require responses\n- Answer product questions based on company documentation\nRed flags to avoid:\n- If you can’t come up with concrete examples, your scope is probably too broad.\n- Using an agent when traditional software would work better (e.g., when the logic is simple, fixed, and already implemented elsewhere). Agents are slow, expensive, and can be finicky at times. If traditional software gets the job done - just use that!\n- Expecting magic that doesn't exist (e.g., connecting to APIs or datasets that don’t exist or can’t be built yet)\nStep 2: Design operating procedure\nWrite up a detailed standard operating procedure (SOP), with step-by-step instructions for how a human would perform the task or process.\nThis step helps confirm that you’ve chosen a problem with a clear, reasonable scope. It also surfaces the key steps, decisions, and tools your agent will likely need to handle—laying the groundwork for what to build.\nExample: Building an Email Agent\nFor our email agent, a step-by-step procedure could look like below:\n- Analyze email content and sender context to categorize response priority\n- Checks calendar availability; schedules video conference meeting\n- Draft a response based on the email, sender, and scheduling context\n- Send the email after a quick human review and approval\nWriting this out helps ensure the task is scoped appropriately, and surfaces the tools and logic our agent will need to handle.\nStep 3: Build MVP with prompt\nChoosing a place to start is important. If your agent is complex, trying to do it all in one go is too ambitious. Start by designing the agent’s architecture outlined by the SOP: how it will flow, what decisions it needs to make, and where LLM reasoning is essential.\nThen, build an MVP by focusing on the most critical LLM reasoning task(s) (e.g., classification, decision-making) and creating a prompt that handles them well. Most agents fail because the LLM can't reason well enough for the task. Getting a single prompt working with hand-fed data will help you build up confidence before proceeding to build the full agent. Prompt engineering tools like LangSmith can help streamline this process, from managing prompt versions, to testing across scenarios or datasets, and tracking performance over time as you iterate.\nKeep it simple by:\n- Starting with manual inputs for any data or context the prompt needs (hold off on automation for now)\n- Testing against your outlined examples from Step 1 to validate performance across common use cases\n- Focusing on getting the LLM reasoning right\nExample: Building an Email Agent\nAt this stage, we’re identifying and solving one high-leverage reasoning task to start with.\nFor our email agent, that might mean focusing just on classifying emails by urgency and intent (e.g., meeting request, support questions), as this is a foundational step that the rest of the agent depends on.\nStart by writing a core prompt that does just this, with hand-fed inputs like:\n- Email content: “Can we meet next week about LangChain’s product roadmap?”\n- Sender: “Jeff Bezos”, Title: “CEO of Amazon”\n- Output: Intent = “Meeting Request”, Urgency = “High”\nOnce the model consistently gets this right across your test cases, you’ll have confidence that the core logic is sound—and a strong foundation to build on.\nStep 4: Connect & Orchestrate\nNow that we have a working prompt, it’s time to connect the prompt to real data and user inputs.\nStart by identifying what context or data the prompt needs—such as email content, calendar availability, and documentation of products—and plan how to access it programmatically (e.g., via APIs, databases, or file systems).\nThen, write orchestration logic to connect the right data into your prompt. In simple cases, this might just mean passing inputs directly. For more complex workflows, you may need agentic logic to decide which data sources to query, when to call them, and how to combine their outputs before prompting the LLM.\nExample: Building an Email Agent\nFor our email agent, this step could involve integrating with the Gmail API (to read incoming emails), Google Calendar API (to check availability), and a CRM or contact database (to enrich sender context).\nWe’d then build orchestration logic like the following :\n- A new email triggers the agent\n- The agent fetches sender info from the CRM or via web search\n- It passes the full context into the prompt to determine urgency and whether a response is needed\n- If a meeting is appropriate, it checks calendar availability and proposes times\n- The agent drafts a response\n- After human review, it sends the email\nStep 5: Test & Iterate\nBegin by manually testing your MVP using the examples you defined in Step 1. The goal is to verify that your agent is producing reasonable, accurate outputs for your core use cases. If your system involves multiple LLM calls or steps, it’s helpful to set up tracing using tools like LangSmith to visualize the flow and debug how decisions are made at each stage.\nOnce manual testing is solid, scale to automated testing to ensure consistency and catch edge cases. Teams will often beef up examples to a few dozen to get a better sense of the agent’s strengths and weaknesses. This also helps you quantify performance before adding more complexity:\n- Run all examples (original + new) programmatically through your agent\n- Define automated success metrics — this forces clarity around your agent’s expected behavior\n- Use human review selectively to catch issues that metrics might miss\nExample: Building an Email Agent\nFor the email agent, we’d want to define and test success across several key areas:\n- Tone and Safety: Responses should be professional, respectful, and free of hallucinated or inappropriate content\n- Intent & Priority Detection: Emails should be correctly categorized and prioritized based on sender and content\n- Tool Usage Efficiency: The agent should trigger only the necessary tools (e.g., avoid checking the calendar if no scheduling is required)\n- Draft Quality: Suggested replies should be clear, relevant, and accurate based on the input context\nStep 6: Deploy, Scale, and Refine\nOnce your MVP is performing reliably, begin expanding its scope—adding new capabilities, broader use cases, or even multi-agent workflows. For every new feature or integration, repeat the testing process from Step 5 to ensure you’re not breaking existing functionality.\nWhen ready, deploy to production in users' hands. LangGraph Platform allows you to quickly ship, scale, and manage your agents with one-click deployment.\nMonitor how people actually use your agent. Tools like LangSmith let you trace your agent’s actions in real time, making it easier to spot spikes in cost, accuracy issues, or latency. Real-world usage often differs from your initial assumptions, and these insights can reveal gaps, surface unexpected needs, and guide prioritization during your next iteration.\nThe key is treating launch as the beginning of iteration, not the end of development.\nExample: Building an Email Agent\nAfter deploying our email agent, we might discover unaddressed use cases through monitoring traffic and common use cases.\nThese emerging patterns signal opportunities to expand scope. From there, we can iteratively add new integrations and update our prompts and orchestration logic—always validating each addition with tests and user feedback before scaling further.\nConclusion\nThis process is designed to help you build agents that are grounded in clear use cases, tested against real examples, and shaped by real-world feedback. It’s not just about getting an agent to run, but about building something useful, reliable, and aligned with how people actually work.\nWhether you're automating email triage or orchestrating complex workflows, these six steps offer a practical path from idea to impact. But the work doesn’t stop at deployment—the best agents are built through iteration.\nSo start small, stay user-focused, and keep refining.", "content_token_count": 1863, "client_status": "queued", "server_status": "embedded", "summary": "This article from the LangChain Blog provides a comprehensive guide on building an agent, specifically an email agent. It outlines a six-step framework, from defining the agent's job to deploying, scaling, and refining the agent, with practical examples and tips to avoid common pitfalls.", "expiry_score": 0.3, "mistral_embedding": "[-0.03552246,0.040008545,0.02909851,0.0049209595,0.039154053,0.007972717,0.055633545,-0.017868042,0.004547119,0.020431519,-0.015403748,0.062042236,-0.054779053,0.011711121,-0.025878906,0.0368042,-0.0037708282,0.02192688,0.050476074,0.032104492,-0.031021118,-0.011230469,-0.05392456,0.005001068,0.0262146,-0.020751953,0.008239746,-0.07489014,-0.031021118,-0.017654419,0.008453369,-0.04321289,-0.013374329,0.0058021545,-0.001490593,-0.020217896,-0.008560181,-0.0395813,-0.002500534,-0.013908386,0.03552246,-0.0008625984,-0.0066871643,-0.007381439,-0.02128601,-0.0021533966,0.038085938,-0.019577026,0.008293152,0.00019299984,0.0013771057,0.044708252,-0.0058555603,-0.0040397644,-0.010856628,0.009468079,0.020217896,0.0021533966,-0.0395813,0.018081665,-0.033599854,0.011657715,0.02406311,-0.03338623,0.041931152,0.031677246,0.012939453,-0.029953003,-0.020111084,-0.041931152,0.021499634,0.008666992,-0.013427734,0.026428223,-0.035736084,-0.019256592,-7.814169e-05,0.035736084,0.025238037,-0.022781372,-0.014656067,0.018936157,0.071899414,-0.023422241,-0.0552063,-0.021072388,0.00038599968,-0.00548172,0.016052246,0.0044670105,0.020645142,0.031021118,0.034240723,-0.03744507,0.048339844,0.027389526,0.0023403168,0.013427734,-0.0008826256,0.04449463,0.030380249,-0.030807495,0.07873535,-0.013374329,0.03253174,-0.011016846,0.035949707,0.006954193,0.017120361,-0.041290283,-0.08770752,-0.032958984,0.013267517,-0.054351807,-0.0048942566,0.017868042,-0.022781372,0.04534912,-0.072753906,-0.06378174,0.05307007,0.052642822,-0.00039935112,0.0051612854,-0.039367676,-0.008506775,0.019256592,0.049621582,-0.0072746277,0.0046806335,-0.009521484,-0.0072746277,-0.020217896,-0.026535034,0.059906006,-0.017440796,0.04043579,0.013908386,-0.03274536,-0.015083313,0.039367676,0.00028586388,-0.025024414,0.00078868866,0.008346558,-0.022888184,0.004840851,0.028244019,-0.0044136047,-0.0395813,-0.003610611,0.043640137,0.051330566,-0.007972717,0.022354126,-0.028884888,-0.030593872,-0.01701355,0.03765869,0.013267517,-0.019042969,0.00046801567,-0.024383545,0.026321411,-0.0011434555,-0.00920105,-0.01701355,0.006793976,-0.015296936,-0.031677246,0.022033691,-0.045562744,0.025558472,0.059051514,-0.005722046,0.012191772,0.014976501,0.0051078796,0.048980713,0.01133728,0.015296936,-0.032104492,0.04598999,0.027389526,-0.020324707,-0.021820068,-0.046203613,-0.014442444,0.014015198,0.031463623,-0.023101807,-0.045135498,-0.045562744,0.021713257,0.005695343,0.031463623,-0.0024471283,-0.050048828,0.02760315,-0.0072746277,0.031463623,0.012832642,-0.014656067,0.03488159,-0.012458801,0.022567749,0.011016846,0.010803223,0.0151901245,-0.034454346,-0.015083313,0.015510559,-0.0016851425,-0.0001437664,-0.015945435,-0.010482788,0.0060157776,-0.025665283,-0.02909851,0.003168106,-0.020751953,-0.03829956,-0.020111084,0.015838623,0.027389526,0.0736084,0.00818634,0.0090408325,-0.002073288,0.021713257,0.029525757,0.0395813,-0.011871338,0.0037708282,-0.017974854,-0.0028209686,0.017547607,0.049621582,0.0032215118,0.040008545,0.019042969,0.027069092,-0.020645142,0.029525757,0.020324707,-0.02545166,-0.020431519,0.0071144104,-0.005241394,0.04449463,-0.059051514,-0.0066070557,-0.034240723,-0.028884888,-0.08258057,-0.041290283,-0.013801575,-0.05090332,0.071899414,-0.055633545,-0.03253174,0.015083313,0.023635864,0.025344849,-0.0008225441,0.016052246,-0.030807495,0.019683838,0.05734253,0.019897461,0.0049743652,-0.029525757,0.014656067,0.009307861,0.057769775,-0.02909851,0.0058021545,0.016693115,0.023742676,-0.0018053055,-0.019042969,0.014122009,-0.004173279,0.024810791,-0.031677246,-0.019577026,0.009628296,0.0099487305,-0.0018587112,-0.041503906,0.017120361,0.033813477,0.035095215,0.00020480156,-0.01486969,-0.050048828,0.026535034,0.026321411,0.01776123,0.034240723,0.070617676,-0.02696228,-0.018722534,-0.012672424,0.03894043,-0.0101623535,-0.051971436,-0.02909851,-0.03466797,-0.06976318,-0.017227173,-0.030166626,0.008934021,-0.021820068,-0.004760742,0.008666992,-0.056488037,-0.022140503,-0.020111084,-0.00957489,-0.024810791,-0.007701874,0.060333252,-0.0037975311,0.012779236,-0.007972717,0.009521484,-0.034454346,0.022994995,0.0020866394,0.018508911,0.0077552795,0.030593872,0.031463623,0.02973938,-0.018829346,0.035949707,0.025131226,-0.049835205,0.01914978,-0.03744507,-0.014656067,0.046203613,-0.026428223,0.0055351257,-0.031463623,-0.022140503,-0.022354126,0.011177063,-0.019897461,0.021606445,0.021072388,-0.031021118,0.0524292,0.007221222,0.09967041,-0.0340271,-0.047058105,-0.014335632,0.031234741,-0.021072388,-0.008613586,0.021072388,0.032318115,0.023529053,0.031234741,-0.007648468,-0.014015198,-0.011123657,0.037872314,0.010482788,0.032104492,0.044708252,0.047912598,0.014122009,0.02053833,0.0012168884,-0.029312134,0.013320923,-0.0015640259,-0.037231445,0.017333984,0.02909851,0.033172607,-0.0038776398,-0.011764526,-0.03338623,0.009361267,0.0056419373,0.0680542,0.060333252,0.03552246,0.019363403,-0.022140503,0.014015198,-0.03894043,-0.01637268,0.023529053,0.04534912,0.011550903,-0.009468079,-0.0395813,0.038085938,0.047912598,0.022247314,-0.02545166,0.030166626,0.012031555,0.054779053,0.014015198,0.052856445,0.0088272095,-0.026321411,0.023742676,-0.010055542,-0.019256592,-0.03466797,-0.0066070557,-0.030166626,-0.0019388199,0.0058555603,-0.02267456,-0.064208984,0.025878906,0.0552063,-0.04940796,0.008293152,0.027282715,-0.02760315,0.026748657,-0.025665283,-0.03894043,0.050476074,-0.046417236,-0.035949707,0.051757812,0.0073280334,-0.0368042,-0.009361267,-0.045776367,0.02909851,0.0065231323,0.0020599365,0.036376953,-0.005214691,-0.0040397644,0.0072746277,0.03894043,0.010429382,-0.04107666,0.04257202,-0.026641846,-0.031677246,0.024490356,0.044281006,-0.01096344,-0.009895325,-0.0077552795,0.02267456,0.0009160042,-0.03466797,0.035308838,-0.030380249,-0.06463623,-0.040008545,0.07232666,0.035949707,0.022781372,-0.014122009,-0.07531738,0.052856445,-0.06719971,-0.009094238,0.0046539307,0.0101623535,-0.020751953,0.06976318,0.004146576,-0.0014772415,-0.025024414,0.047698975,-0.0023002625,-0.0050811768,-0.0032348633,0.043640137,0.037231445,-0.058624268,-0.048553467,0.008773804,-0.0017118454,0.074035645,-0.030807495,-0.011550903,-0.025665283,-0.046203613,-0.066345215,0.0064697266,-0.022781372,-0.03616333,0.003583908,0.0074882507,0.014015198,-0.002714157,0.014442444,-0.009414673,0.035095215,-0.011070251,-0.045562744,0.056488037,-0.044067383,-0.048553467,-0.017227173,0.039367676,-0.0340271,-0.00024402142,-0.012031555,-0.08129883,-0.017974854,-0.004386902,-0.02609253,-0.003194809,-0.003583908,-0.039367676,-0.06549072,0.024597168,0.014335632,0.04107666,0.02760315,0.0079193115,-0.0368042,0.013801575,0.017227173,-0.023208618,-0.022140503,-0.0036640167,0.020324707,0.0019388199,0.016479492,-0.0071144104,-0.03338623,-0.06890869,0.025985718,-0.014442444,-0.04171753,-0.02909851,-0.020965576,-0.0680542,-0.0070610046,0.0071144104,0.014656067,0.025665283,-0.050689697,0.0211792,-0.019897461,-0.04748535,0.039154053,-0.013267517,0.019256592,-0.0009160042,-0.010643005,0.024810791,-0.0035572052,0.00957489,0.0034770966,0.028457642,-0.04043579,0.012565613,0.001490593,-0.047058105,-0.044281006,-0.048339844,0.0014705658,-0.016159058,-0.039367676,0.01486969,0.02331543,0.044281006,0.017227173,-0.04385376,-0.011764526,0.0016384125,0.068481445,-0.04385376,-0.04257202,0.041290283,0.005722046,0.016693115,0.020965576,-0.002981186,-0.02545166,0.020324707,0.026748657,-0.019256592,-0.03616333,-0.02696228,0.025772095,0.012992859,0.062469482,0.0009899139,0.033813477,0.005455017,-0.0024204254,0.0065231323,0.043640137,0.029525757,-0.015403748,-0.037017822,-0.030807495,-0.014976501,-0.042144775,-0.054779053,0.04107666,0.021072388,-0.076171875,-0.018615723,-0.024810791,-0.0068206787,-0.013267517,0.044921875,0.0060691833,0.074035645,0.019363403,-0.018722534,-0.010803223,0.019897461,-0.01701355,0.04321289,0.017547607,-0.0039043427,-0.003824234,0.027175903,0.003396988,-0.03338623,0.007221222,0.016693115,0.02128601,0.047058105,0.050476074,-0.013046265,-0.047912598,-0.017868042,0.046417236,0.027816772,0.045562744,-0.013694763,0.007007599,0.0051345825,0.015510559,-0.017440796,-0.0028476715,-0.017974854,-0.020111084,-0.06506348,0.024597168,-0.018829346,0.021072388,0.0340271,-0.02053833,-0.0054016113,0.048553467,0.0073280334,-0.06719971,0.03744507,-0.023956299,-0.002286911,0.007381439,-0.035949707,0.04727173,-0.026641846,-0.06378174,0.027069092,0.054779053,-0.005748749,-0.017547607,0.008987427,0.022247314,-0.008132935,-0.012512207,-0.04812622,-0.012619019,0.003824234,-0.06976318,-0.016799927,-0.007381439,-0.008346558,0.0035572052,-0.033813477,-0.03829956,0.046844482,-0.0033302307,0.03488159,0.019042969,-0.009521484,0.023849487,0.0791626,-0.006713867,0.0023269653,-0.034240723,-0.052856445,-0.0065231323,0.025985718,0.0151901245,-0.059051514,-0.015731812,0.0524292,-0.04043579,0.00818634,0.004547119,-0.006793976,-0.0055885315,0.0423584,-0.025131226,-0.009788513,0.05154419,-0.010429382,0.010856628,0.0077552795,-0.078308105,0.06933594,-0.006095886,0.057769775,-0.056488037,-0.008239746,-0.015838623,-0.009788513,0.009681702,-0.06335449,-0.022460938,0.03189087,-0.025024414,-0.0088272095,0.01914978,-0.016159058,-0.033813477,-0.02973938,0.016052246,0.022781372,0.013694763,-0.027282715,-0.016479492,-0.008666992,-0.038513184,-0.029312134,-0.041503906,0.028030396,0.0072746277,-0.018936157,0.037872314,0.011924744,0.0021266937,-0.012298584,0.017654419,-0.017547607,0.0077552795,0.008346558,0.011978149,-0.0019521713,-0.0040664673,-0.050689697,-0.002527237,0.023101807,0.0027942657,0.035949707,0.040863037,0.009895325,-0.04171753,0.010002136,-0.047698975,-0.0022468567,-0.013534546,0.02760315,-0.0017518997,0.016052246,-0.07232666,0.04940796,-0.021820068,0.0042266846,-0.0025939941,-0.014228821,-0.05218506,-0.0680542,-0.036590576,0.009681702,-0.009147644,0.035949707,0.019683838,-0.0037174225,-0.026321411,-0.0040664673,0.020217896,0.008399963,0.013214111,0.0039863586,0.080444336,-0.036590576,0.009147644,-0.025024414,0.0395813,-0.022781372,-0.012619019,-0.0044937134,-0.015731812,0.0423584,0.020858765,-0.0340271,0.012245178,0.02909851,-0.028457642,0.010269165,-0.011497498,-0.00957489,-0.0024204254,-0.008720398,-0.011550903,-0.03744507,-0.072753906,0.0015306473,-0.040008545,-0.02406311,-0.016479492,0.0007519722,-0.003850937,-0.022567749,0.051116943,-0.0048675537,0.002527237,-0.024276733,0.014762878,-0.055633545,0.024597168,0.015945435,-0.04727173,0.044281006,0.011123657,-0.011390686,-0.01626587,0.023422241,-0.008346558,0.032958984,-0.021713257,-0.0042266846,0.024597168,-0.007701874,0.026321411,0.030380249,0.033172607,-0.023742676,-0.05218506,0.0032482147,0.016693115,-0.027816772,-0.0032901764,-0.014762878,-0.016906738,-0.0040130615,0.0131073,0.01486969,0.0736084,0.011497498,-0.017120361,0.0042533875,-0.006954193,0.015083313,-0.009147644,0.06719971,0.020217896,0.033599854,-0.00409317,-0.05947876,0.018615723,0.00045466423,-0.05307007,0.011711121,0.05606079,0.041503906,0.025024414,0.02406311,0.033172607,0.061187744,0.01701355,-0.05392456,-0.04043579,-0.018188477,0.076171875,0.011070251,-0.06506348,0.0680542,0.06677246,-0.018188477,-0.03744507,0.00957489,-0.016799927,0.062927246,0.037017822,-0.0072746277,-0.05328369,0.025772095,0.037017822,-0.013160706,-0.0039043427,-0.023529053,0.011177063,0.02406311,-0.040649414,0.02128601,-0.0021800995,0.085998535,0.0035305023,-0.08642578,0.017120361,0.04321289,0.009307861,-0.0262146,0.014549255,-0.0027675629,0.005027771,0.025238037,0.023956299,0.016052246,0.062042236,0.010643005,-0.032318115,0.012512207,0.00034093857,-0.0058288574,0.0035572052,-0.02128601,-0.00566864,-0.042999268,0.071899414,0.0003976822,0.025024414,0.016693115,-0.036376953,0.0010032654,0.0079193115,0.04257202,0.0064697266,0.03488159,0.018829346,0.0013370514,0.06335449,0.017654419,-0.0040130615,-0.00035595894,-0.038513184,0.008880615,-0.04107666,0.02760315,0.0007686615,0.012191772,0.023635864,0.011871338,0.02331543,-0.028457642,-0.027069092,-0.019577026,0.0101623535,-0.05218506,0.02192688,0.011123657,-0.0010099411,-0.003583908,-0.052856445,0.06378174,0.022781372,0.04107666,0.0014438629,0.023422241,0.04321289,-0.010536194,-0.008666992,0.07659912,-0.010482788,0.056915283,0.04257202,0.022994995,-0.014335632,0.014122009,0.021713257,-0.044067383,0.03189087,0.019897461,0.01701355,-0.016693115,-0.0070610046,0.0038776398,-0.035308838,0.035308838,0.024276733,-0.0131073,0.028457642,-0.036376953,0.03253174,-0.013801575,0.070617676,-0.004360199,0.051116943,0.012245178,0.0079193115,-0.046417236,-0.026321411,0.006149292,0.01914978,0.0035572052,0.047912598,0.021606445,0.041290283,-0.0552063,-0.013534546,-0.012245178,-0.007701874,0.038726807,-0.0024471283,-0.060760498,0.045776367,0.009147644,0.013534546,0.01701355,0.040008545,0.022994995,-0.03189087,0.0014371872,-0.028671265]", "client_status_at": "2025-10-02T09:50:48.497196Z", "server_status_at": "2025-10-02T09:50:48.497130Z", "created_at": "2025-10-02T08:50:45.191805Z"}, {"id": "71f4e0cb-9573-49f4-a735-d98a16e05de9", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://machinelearning.apple.com/research/elegnt-expressive-functional-movement", "canonical_url": "https://machinelearning.apple.com/research/elegnt-expressive-functional-movement", "title": "ELEGNT: Expressive and Functional Movement Design for Non-Anthropomorphic Robot", "source_site": "Apple Machine Learning Research", "publication_date": "2024-12-17T00:00:00Z", "favicon_url": "https://machinelearning.apple.com/favicon.ico", "content_markdown": "Nonverbal behaviors such as posture, gestures, and gaze are essential for conveying internal states, both consciously and unconsciously, in human interaction. For robots to interact more naturally with humans, robot movement design should likewise integrate expressive qualities—such as intention, attention, and emotions—alongside traditional functional considerations like task fulfillment, spatial constraints, and time efficiency. In this paper, we present the design and prototyping of a lamp-like robot that explores the interplay between functional and expressive objectives in movement design. Using a research-through-design methodology, we document the hardware design process, define expressive movement primitives, and outline a set of interaction scenario storyboards. We propose a framework that incorporates both functional and expressive utilities during movement generation, and implement the robot behavior sequences in different function- and social- oriented tasks. Through a user study comparing expression-driven versus function-driven movements across six task scenarios, our findings indicate that expression-driven movements significantly enhance user engagement and perceived robot qualities. This effect is especially pronounced in social-oriented tasks.\n\n## Related readings and updates.\n\n\nEMOTION: Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning\n\nJanuary 24, 2025[research area Human-Computer Interaction](/research/?domain=Human-Computer%20Interaction)\n\nThis paper introduces a framework, called EMOTION, for generating expressive motion sequences in humanoid robots, enhancing their ability to engage in human-like non-verbal communication. Non-verbal cues such as facial expressions, gestures, and body movements play a crucial role in effective interpersonal interactions. Despite the advancements in robotic behaviors, existing methods often fall short in mimicking the diversity and subtlety of…\n\n[Read more](/research/emotion-expressive-motion)\n\n\nARMADA: Augmented Reality for Robot Manipulation and Robot-Free Data Acquisition\n\nDecember 17, 2024[research area Data Science and Annotation](/research/?domain=Data%20Science%20and%20Annotation), [research area Human-Computer Interaction](/research/?domain=Human-Computer%20Interaction)\n\nTeleoperation for robot imitation learning is bottlenecked by hardware availability. Can high-quality robot data be collected without a physical robot? We present a system for augmenting Apple Vision Pro with real-time virtual robot feedback. By providing users with an intuitive understanding of how their actions translate to robot motions, we enable the collection of natural barehanded human data that is compatible with the limitations of…\n\n[Read more](/research/armada-augmented-reality)", "content_text": "Nonverbal behaviors such as posture, gestures, and gaze are essential for conveying internal states, both consciously and unconsciously, in human interaction. For robots to interact more naturally with humans, robot movement design should likewise integrate expressive qualities—such as intention, attention, and emotions—alongside traditional functional considerations like task fulfillment, spatial constraints, and time efficiency. In this paper, we present the design and prototyping of a lamp-like robot that explores the interplay between functional and expressive objectives in movement design. Using a research-through-design methodology, we document the hardware design process, define expressive movement primitives, and outline a set of interaction scenario storyboards. We propose a framework that incorporates both functional and expressive utilities during movement generation, and implement the robot behavior sequences in different function- and social- oriented tasks. Through a user study comparing expression-driven versus function-driven movements across six task scenarios, our findings indicate that expression-driven movements significantly enhance user engagement and perceived robot qualities. This effect is especially pronounced in social-oriented tasks.\nRelated readings and updates.\nEMOTION: Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning\nJanuary 24, 2025research area Human-Computer Interaction\nThis paper introduces a framework, called EMOTION, for generating expressive motion sequences in humanoid robots, enhancing their ability to engage in human-like non-verbal communication. Non-verbal cues such as facial expressions, gestures, and body movements play a crucial role in effective interpersonal interactions. Despite the advancements in robotic behaviors, existing methods often fall short in mimicking the diversity and subtlety of…\nARMADA: Augmented Reality for Robot Manipulation and Robot-Free Data Acquisition\nDecember 17, 2024research area Data Science and Annotation, research area Human-Computer Interaction\nTeleoperation for robot imitation learning is bottlenecked by hardware availability. Can high-quality robot data be collected without a physical robot? We present a system for augmenting Apple Vision Pro with real-time virtual robot feedback. By providing users with an intuitive understanding of how their actions translate to robot motions, we enable the collection of natural barehanded human data that is compatible with the limitations of…", "content_token_count": 433, "client_status": "queued", "server_status": "embedded", "summary": "This paper presents the design and prototyping of a lamp-like robot that integrates expressive qualities in movement design to enhance human-robot interaction. A user study shows that expression-driven movements significantly improve user engagement and perceived robot qualities, especially in social-oriented tasks.", "expiry_score": 0.1, "mistral_embedding": "[-0.07733154,0.024551392,0.036224365,-0.011352539,0.04663086,-0.0107421875,0.040802002,0.005947113,0.01940918,-0.01411438,-0.045715332,0.0435791,-0.018493652,-0.008705139,-0.030532837,0.030075073,-0.016113281,0.006023407,0.019332886,-0.025924683,-0.025009155,-0.0048713684,-0.0435791,-0.009094238,-0.012046814,0.00932312,-0.010894775,-0.061676025,-0.009552002,-0.013504028,0.022247314,-0.024246216,-0.004852295,0.034362793,0.026245117,-0.018188477,-0.003145218,-0.07458496,0.017410278,-0.01335144,0.0010738373,0.008094788,0.007709503,-0.024093628,0.011657715,-0.043884277,0.028076172,-0.001203537,0.010047913,-0.025924683,-0.02607727,0.025772095,0.0022830963,-0.010124207,-0.049713135,0.026397705,-0.009628296,-0.015266418,-0.060150146,0.031463623,-0.062286377,-0.021942139,-0.0064430237,-0.022247314,0.024093628,0.057678223,0.023483276,-0.005180359,0.008552551,-0.019332886,0.011047363,0.016418457,-0.0008583069,0.015731812,-0.019256592,-0.0619812,0.0064048767,-0.0043144226,-0.011276245,0.002445221,-0.0027999878,-0.0018987656,0.0847168,0.0005636215,-0.028076172,-0.0033569336,-0.02178955,0.0023593903,0.017715454,0.025161743,0.040802002,0.017028809,0.00881958,-0.028686523,0.061676025,0.040496826,-0.032989502,-0.008476257,-0.00031876564,0.0395813,0.022705078,-0.007785797,0.06384277,-0.033294678,0.05340576,0.014038086,0.037750244,-0.021026611,0.018341064,-0.022247314,-0.058929443,0.0011558533,0.0052948,-0.06384277,-0.01902771,0.0016298294,-0.006904602,0.046325684,-0.055236816,-0.006828308,0.043273926,0.05215454,-0.0025501251,0.030227661,-0.046966553,-0.048187256,0.0003812313,0.027160645,-0.0015153885,-0.027618408,-0.015113831,-0.009475708,-0.013122559,-0.027160645,-0.0075569153,-0.0030117035,0.051849365,0.0064430237,-0.008514404,-0.011276245,0.028076172,-0.023483276,-0.014038086,-0.008476257,-0.005256653,-0.047576904,-0.010360718,0.04019165,-0.03466797,-0.024398804,-0.0057525635,0.04019165,0.06628418,-0.020874023,-0.00774765,-0.03729248,-0.031463623,-0.008094788,0.062927246,-0.024856567,-0.030227661,0.043273926,-0.03161621,0.012123108,-0.0395813,-0.023010254,-0.02394104,0.03527832,-0.014419556,-0.003145218,0.01235199,-0.030227661,0.04510498,0.03161621,-0.0317688,0.02746582,0.039123535,-0.04019165,0.06628418,0.03161621,-0.035125732,0.0002361536,0.0030879974,-0.0075569153,-0.012962341,-0.0082092285,-0.0027618408,-0.009819031,0.00166893,0.054016113,-0.002023697,-0.04788208,-0.022705078,0.002407074,0.004852295,0.05831909,0.019256592,-0.055236816,-0.019943237,-0.0072517395,0.027160645,-0.003490448,-0.0435791,0.083496094,0.005947113,0.017868042,-0.028839111,0.05154419,0.04019165,-0.0317688,0.016418457,0.009628296,-0.0026855469,-0.011276245,-0.038513184,-0.037597656,0.021636963,-0.0309906,-0.0029354095,0.012199402,-0.02607727,-0.019943237,-0.003566742,0.0154953,0.03729248,0.055541992,0.0635376,-0.022094727,-0.016571045,0.029769897,-0.0016498566,0.021331787,0.011047363,0.0012083054,-0.006214142,-0.017562866,0.029464722,0.03390503,0.015419006,0.019256592,0.00843811,0.05831909,-0.03161621,0.020095825,0.009742737,-0.01902771,-0.029159546,-0.002954483,0.015655518,0.053100586,-0.04663086,0.006061554,-0.075805664,-0.0031833649,-0.04663086,-0.00440979,-0.021636963,-0.049102783,0.039276123,-0.034973145,-0.0435791,-0.023635864,0.025466919,-0.016952515,0.030075073,0.004085541,-0.05706787,0.0026283264,0.09454346,0.013961792,0.014503479,-0.008705139,0.05432129,0.008934021,0.025619507,-0.014266968,-0.003030777,0.016952515,-0.012199402,-0.0009636879,0.0021190643,0.009780884,0.018569946,0.009513855,-0.057373047,0.0073661804,0.005256653,0.007633209,0.0033187866,-0.06842041,-0.008361816,0.03729248,0.0151901245,0.004047394,-0.0074043274,0.01826477,0.004699707,0.021026611,0.022705078,0.020721436,0.060150146,-0.015731812,-0.019638062,0.020095825,0.03543091,-0.044189453,-0.035125732,0.0132751465,-0.015342712,-0.07733154,-0.003528595,-0.0132751465,-0.0009160042,0.0082092285,-0.037597656,-0.018798828,-0.04788208,-0.07733154,-0.0035858154,-0.027313232,-0.02178955,-0.006214142,0.06628418,-0.015419006,0.034362793,0.016189575,-0.034362793,-0.02394104,0.015960693,-0.014190674,0.010276794,-0.007785797,0.034057617,0.031921387,0.014038086,0.010513306,0.060455322,0.03729248,-0.022857666,-0.0073661804,-0.01411438,0.024246216,0.028533936,0.018951416,0.010894775,-0.025009155,-0.01979065,0.012580872,-0.0032997131,0.0036830902,0.0054855347,0.016342163,-0.0023975372,0.0063667297,-0.017562866,0.07977295,-0.011276245,-0.032836914,0.009895325,0.060455322,-0.041107178,-0.030380249,0.022857666,0.016342163,-0.0073661804,-0.0037212372,-0.030685425,0.031921387,0.0030117035,-0.006214142,0.00299263,0.026550293,0.055236816,0.0423584,0.006023407,-0.004852295,-0.005138397,-0.029159546,0.011657715,-0.05154419,-0.048187256,0.021636963,0.0008249283,0.018035889,0.011657715,-0.028381348,-0.05831909,-0.010894775,0.015655518,0.11291504,0.06689453,0.023635864,0.03314209,-0.023483276,-0.0021095276,-0.009628296,0.009666443,0.034362793,0.055847168,0.004447937,0.01626587,-0.046966553,0.060760498,0.053710938,0.0064048767,-0.017868042,0.04449463,0.033447266,0.016952515,-0.0017261505,0.017105103,0.024551392,-0.017486572,0.039123535,0.0007238388,-0.009819031,-0.034820557,0.026855469,0.0045661926,-0.016036987,-0.0056381226,0.010437012,-0.030685425,0.022399902,0.042053223,-0.02670288,-0.025161743,0.029464722,-0.032073975,0.048187256,-0.022857666,-0.04144287,0.04510498,-0.021636963,-0.014038086,0.015113831,0.0158844,-0.0075187683,-0.034973145,-0.036224365,0.020553589,-0.001783371,-0.0158844,0.041107178,-0.03466797,0.019104004,0.034973145,0.018798828,-0.005870819,-0.043884277,0.0423584,-0.0619812,-0.0023593903,-0.014732361,0.048797607,-0.005256653,-0.037750244,0.0020427704,0.017181396,-0.014961243,-0.03805542,0.008399963,-0.0126571655,-0.044799805,-0.040496826,0.037597656,0.0064048767,0.057373047,-0.01626587,-0.10620117,0.039886475,-0.009590149,-0.05645752,-0.013961792,0.021484375,-0.022399902,0.07550049,-0.0024929047,-0.027008057,-0.0073661804,0.024093628,0.018417358,0.014343262,-0.006790161,0.04727173,0.04296875,-0.055847168,-0.014190674,0.0066375732,0.018569946,0.06994629,-0.03527832,-0.033447266,-0.025161743,-0.048797607,-0.016189575,-0.008132935,-0.0074424744,-0.010437012,-0.0014381409,0.0022830963,0.006713867,0.03390503,-0.0074424744,-0.0072898865,0.0004723072,-0.004776001,-0.036224365,0.02394104,-0.07086182,-0.042663574,0.0054092407,0.03390503,-0.022094727,-0.001745224,0.021942139,-0.03668213,-0.015113831,-0.010971069,0.00166893,-0.0107421875,-0.003145218,-0.036987305,-0.044189453,0.0028190613,0.0034332275,-0.009437561,0.0035858154,0.0032997131,-0.013122559,0.03668213,0.009170532,-0.03253174,0.012275696,-0.020553589,0.0019569397,-0.027923584,0.032684326,-0.034057617,-0.04144287,-0.076416016,0.012123108,-0.0126571655,-0.0309906,-0.021331787,0.002368927,-0.046325684,0.00075769424,0.03543091,-0.01763916,0.016952515,-0.06323242,0.0158844,0.0038547516,-0.013656616,0.036071777,-0.021484375,0.014266968,-0.011047363,0.021636963,0.015960693,0.022705078,0.031311035,-0.0015153885,0.055236816,-0.026855469,0.017181396,-0.01940918,-0.028839111,-0.016189575,-0.043884277,0.026550293,-0.010437012,-0.019485474,0.00088214874,-0.0025119781,0.04019165,0.0066375732,-0.014656067,0.010360718,0.026855469,0.06384277,-0.015655518,-0.04849243,0.008934021,0.0057144165,0.059539795,0.0158844,0.0013999939,0.0045280457,0.015113831,0.010665894,-0.0047187805,-0.030227661,-0.06414795,0.05706787,0.023483276,0.019943237,-0.024856567,0.083496094,0.043884277,0.006252289,-0.022705078,0.06536865,0.05154419,-0.02822876,-0.060455322,-0.0012750626,-0.020248413,-0.06506348,-0.075805664,0.026550293,0.034973145,-0.04849243,0.0019664764,-0.024551392,-0.010360718,-0.02331543,0.04510498,0.0074043274,0.053710938,0.031143188,-0.042053223,-0.029769897,-0.0102005005,-0.008514404,0.042053223,0.0317688,-0.0107421875,-0.0010261536,0.049102783,0.017333984,-0.028839111,0.04296875,0.017028809,0.030532837,0.024551392,0.027008057,-0.034210205,-0.042663574,-0.020095825,0.050933838,0.06628418,0.04144287,0.03161621,0.0018701553,0.0055618286,-0.034057617,-0.00667572,0.018188477,0.00843811,-0.0017261505,-0.04510498,0.014343262,-0.041748047,-0.010437012,0.017562866,-0.0028190613,-0.011352539,0.04510498,-0.019180298,-0.0847168,0.03390503,-0.030532837,-0.040802002,-0.020095825,-0.048797607,0.03591919,-0.032226562,-0.055847168,-0.014503479,0.070251465,0.0028381348,-0.013038635,0.012809753,-0.016952515,0.0030879974,0.00096845627,-0.047576904,-0.00066185,0.025161743,-0.038208008,-0.01626587,0.023483276,-0.01235199,0.0024261475,-0.046020508,-0.04144287,0.011123657,-0.004623413,0.054626465,0.02670288,-0.008934021,0.026245117,0.057678223,0.016799927,0.014808655,0.0052948,-0.053100586,0.011352539,0.010360718,0.026550293,-0.019638062,-0.030380249,0.06384277,-0.06536865,0.0154953,0.011199951,0.024093628,-0.0154953,0.045715332,-0.013809204,-0.03161621,0.03375244,-0.013122559,-0.044799805,0.008323669,-0.06137085,0.054016113,0.004371643,0.0211792,-0.024398804,0.011428833,-0.014266968,0.00022053719,-0.039886475,-0.04940796,0.013122559,0.03253174,0.009246826,-0.024246216,-0.021331787,-0.0126571655,-0.008857727,-0.030227661,0.020401001,-0.015731812,0.020553589,0.0057144165,0.014732361,-0.009513855,-0.050323486,-0.0309906,-0.04849243,0.0013618469,0.010894775,-0.011199951,-0.0015630722,0.032684326,0.041107178,0.035736084,0.03744507,0.0027999878,0.04788208,-0.0132751465,-0.0031261444,0.024093628,0.009971619,-0.026245117,0.008552551,0.02822876,-0.018493652,0.043884277,0.022094727,-0.014656067,-0.017333984,0.0024738312,-0.043884277,0.003107071,-0.0126571655,0.03543091,-0.016113281,0.035736084,-0.074279785,0.058013916,-0.031921387,-0.00116539,0.006713867,-0.028533936,-0.06262207,-0.058013916,-0.042663574,0.01940918,-0.006980896,0.011428833,-0.015960693,0.006214142,-0.020401001,0.01235199,-0.020248413,0.040496826,0.02670288,-0.012046814,0.064453125,-0.014343262,-0.008514404,-0.02746582,0.028839111,0.013809204,-0.053100586,-0.0017261505,-0.05001831,0.043884277,0.009590149,-0.0211792,0.029312134,-0.009513855,-0.02961731,-0.02331543,-0.03652954,0.009628296,0.0034332275,-0.015419006,-0.013961792,-0.0309906,-0.06628418,-0.021636963,-0.049713135,-0.011199951,0.0045661926,0.0025310516,0.016799927,-0.024398804,0.014038086,0.006828308,-0.034057617,0.0046043396,-0.020721436,-0.024398804,0.019332886,0.027923584,-0.028686523,0.025314331,-0.009094238,-0.025619507,-0.029159546,0.04663086,-0.013038635,0.0055999756,-0.022399902,-0.04019165,0.010971069,0.008552551,-0.0050239563,-0.0054092407,0.058013916,-0.009361267,-0.043273926,-0.0054092407,-0.017410278,-0.064453125,0.018035889,0.050933838,-0.01235199,0.024246216,0.014579773,0.020095825,0.039276123,0.0010881424,-0.011199951,0.007671356,-0.0027618408,0.04940796,0.0055618286,0.04663086,0.018875122,-0.012123108,0.0019369125,-0.083496094,0.0073661804,-0.038360596,-0.04940796,0.020553589,0.05154419,0.0046806335,-0.0049476624,0.005870819,0.023788452,0.030838013,0.0036258698,-0.050323486,-0.03237915,0.013809204,0.0049858093,0.0013237,-0.08959961,0.04510498,0.07116699,-0.009017944,-0.04019165,0.020248413,-0.02255249,0.022705078,0.012046814,-0.0075950623,-0.019943237,0.020721436,0.04449463,0.0021953583,-0.011894226,-0.01626587,0.00023496151,0.06628418,-0.04510498,-0.026245117,-0.012046814,0.06262207,0.0050621033,-0.040802002,0.0049095154,0.016113281,-0.024093628,-0.03253174,-0.008552551,-0.022857666,-0.010437012,0.0064811707,0.013885498,0.02961731,0.070251465,-0.0056381226,0.012962341,0.014503479,0.03161621,0.020095825,0.021026611,-0.006866455,-0.016036987,-0.060150146,0.038970947,-0.009399414,0.041107178,0.004333496,-0.06137085,0.0064430237,0.0030498505,0.023010254,0.028686523,0.02746582,-0.005218506,-0.0052948,0.054016113,0.01235199,-0.0011606216,0.008399963,-0.040496826,-0.01826477,-0.05001831,-0.007019043,0.015342712,-0.009971619,0.0030498505,-0.009971619,0.021942139,-0.02470398,-0.027160645,0.0030879974,0.05432129,-0.032226562,0.013198853,0.029159546,-0.023162842,0.031921387,-0.034820557,0.0619812,0.0046806335,0.023010254,-0.005332947,0.03390503,0.032226562,0.0007095337,-0.0013523102,0.048797607,0.034820557,0.031143188,0.057678223,0.0027046204,-0.06506348,0.0105896,-0.029312134,-0.06536865,0.029006958,0.010276794,0.047576904,-0.025009155,0.014808655,-0.033599854,-0.036834717,0.043884277,0.007133484,-0.045410156,0.03451538,-0.0395813,0.014038086,-0.0309906,0.039886475,-0.035583496,0.039886475,-0.015655518,-0.0132751465,-0.014343262,0.011505127,-0.0158844,0.061065674,-0.04940796,0.045410156,0.0435791,0.01373291,-0.051849365,-0.019638062,0.0126571655,-0.019943237,0.042663574,-0.02822876,-0.044189453,0.004585266,0.008132935,0.037750244,0.0016107559,0.05831909,-0.016647339,-0.011505127,0.00932312,-0.0019569397]", "client_status_at": "2025-10-02T09:50:44.473936Z", "server_status_at": "2025-10-02T09:50:44.473877Z", "created_at": "2025-10-02T08:50:40.248013Z"}, {"id": "6d8c8ad9-b74e-4335-a34f-01fe2ce1be3a", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://aeon.co/videos/after-centuries-of-trying-weve-yet-to-arrive-at-a-perfect-way-to-map-colour", "canonical_url": "https://aeon.co/videos/after-centuries-of-trying-weve-yet-to-arrive-at-a-perfect-way-to-map-colour", "title": "After centuries of trying, we’ve yet to arrive at a perfect way to map colour | Aeon Videos", "source_site": "Aeon", "publication_date": "2025-08-18T00:00:00Z", "favicon_url": "https://aeon.co/favicon.ico", "content_markdown": "# After centuries of trying, we’ve yet to arrive at a perfect way to map colour\n\nCan colour be understood geometrically? If so, what’s the best way to map it out, capturing the variables of hue, brightness and saturation? These questions have deep implications for art, physics and perception, and have been pondered for centuries. In this extraordinary dive into how thinkers from Isaac Newton to today have mapped colour, the French video essayist Alessandro Roussel of the YouTube channel ScienceClic makes the case that there’s not just one way to map colour in two or three dimensions, but many – each of them communicating different truths about the nature of the phenomenon. To capture the mutable nature of colour in the human experience, and our always-evolving understanding of it, Roussel concludes with the fascinating story of ‘olo’ – a new ‘impossible colour’, outside of the usual visible spectrum, which scientists were only recently able to produce in the laboratory.\n\n\nvideoAstronomy\n\nVisualisations explore what the deep future holds for our night sky\n\n6 minutes\n\n\nvideoBiology\n\nDive deep into an egg cell to see how ageing reboots when a new life begins\n\n2 minutes\n\n\nvideoMusic\n\nA riveting audiovisual dive into what makes sounds harmonious, or not\n\n28 minutes\n\n\nvideoMathematics\n\nSpiral into the ‘golden ratio’ – and separate the myths from the maths\n\n4 minutes\n\n\nvideoHistory of science\n\nHow we came to know the size of the Universe – and what mysteries remain\n\n26 minutes\n\n\nvideoOceans and water\n\nA stunning visualisation explores the intricate circulatory system of our oceans\n\n5 minutes\n\n\nvideoHistory of science\n\nIdeas ‘of pure genius’ – how astronomers have measured the Universe across history\n\n29 minutes\n\n\nvideoAnimals and humans\n\nOne man’s quest to save an orphaned squirrel, as narrated by David Attenborough\n\n14 minutes\n\n\nvideoComputing and artificial intelligence\n\nA future in which ‘artificial scientists’ make discoveries may not be far away\n\n9 minutes", "content_text": "After centuries of trying, we’ve yet to arrive at a perfect way to map colour\nCan colour be understood geometrically? If so, what’s the best way to map it out, capturing the variables of hue, brightness and saturation? These questions have deep implications for art, physics and perception, and have been pondered for centuries. In this extraordinary dive into how thinkers from Isaac Newton to today have mapped colour, the French video essayist Alessandro Roussel of the YouTube channel ScienceClic makes the case that there’s not just one way to map colour in two or three dimensions, but many – each of them communicating different truths about the nature of the phenomenon. To capture the mutable nature of colour in the human experience, and our always-evolving understanding of it, Roussel concludes with the fascinating story of ‘olo’ – a new ‘impossible colour’, outside of the usual visible spectrum, which scientists were only recently able to produce in the laboratory.\nvideoAstronomy\nVisualisations explore what the deep future holds for our night sky\n6 minutes\nvideoBiology\nDive deep into an egg cell to see how ageing reboots when a new life begins\n2 minutes\nvideoMusic\nA riveting audiovisual dive into what makes sounds harmonious, or not\n28 minutes\nvideoMathematics\nSpiral into the ‘golden ratio’ – and separate the myths from the maths\n4 minutes\nvideoHistory of science\nHow we came to know the size of the Universe – and what mysteries remain\n26 minutes\nvideoOceans and water\nA stunning visualisation explores the intricate circulatory system of our oceans\n5 minutes\nvideoHistory of science\nIdeas ‘of pure genius’ – how astronomers have measured the Universe across history\n29 minutes\nvideoAnimals and humans\nOne man’s quest to save an orphaned squirrel, as narrated by David Attenborough\n14 minutes\nvideoComputing and artificial intelligence\nA future in which ‘artificial scientists’ make discoveries may not be far away\n9 minutes", "content_token_count": 416, "client_status": "queued", "server_status": "embedded", "summary": "The video explores the historical and scientific challenges of mapping color geometrically, highlighting various methods and the recent discovery of an 'impossible color' called 'olo'. It also touches on other scientific visualizations across different fields.", "expiry_score": 0.3, "mistral_embedding": "[0.002532959,0.062286377,0.054656982,-0.029510498,0.040527344,-0.024612427,0.021896362,-0.017684937,-0.006767273,-0.040802002,-0.028289795,0.066101074,-0.026382446,-0.019989014,-0.04324341,0.0021343231,-0.024887085,0.03616333,0.055755615,-0.008705139,-0.01461792,-0.004486084,-0.051940918,0.017410278,-0.0002039671,-0.007106781,-0.026245117,-0.060913086,0.0038757324,-0.02230835,0.0128479,-0.03250122,0.014961243,0.015777588,-0.02420044,0.017684937,0.010536194,-0.057128906,-0.0006246567,-0.021759033,-0.002166748,-0.033599854,-0.023391724,0.011695862,-0.0015382767,-0.01108551,-0.021484375,-0.010536194,-0.011695862,-0.00819397,0.014350891,0.044891357,0.016998291,0.005744934,-0.021896362,0.019439697,0.042144775,-0.015029907,-0.041625977,0.010536194,-0.02760315,-0.009109497,-0.023529053,-0.0011987686,0.038635254,-0.00856781,-0.0013008118,-0.004386902,0.019439697,0.0152282715,0.019577026,0.017547607,-0.009719849,-0.0014705658,0.0128479,-0.03250122,0.04107666,0.027191162,0.004131317,0.021896362,-0.048950195,-0.0079193115,0.07672119,-0.028427124,0.014549255,0.030197144,0.02394104,0.0024814606,-0.011421204,0.03564453,0.021072388,0.0079574585,0.03942871,0.004589081,0.062286377,0.017410278,-0.02583313,0.0046577454,0.0034503937,0.03086853,0.033447266,0.0034160614,0.089782715,-0.0034503937,0.021896362,-0.060638428,0.025436401,-0.04324341,-0.0124435425,-0.04107666,-0.08050537,-0.053588867,-0.012237549,-0.06878662,-0.022445679,0.009109497,0.0075798035,0.043792725,-0.04977417,-0.033721924,0.014007568,0.038360596,-0.0076141357,0.025436401,-0.04324341,-0.07287598,0.0060195923,0.028701782,-0.014549255,-0.02394104,-0.010475159,-0.03564453,0.018630981,-0.037261963,-0.011833191,-0.02217102,0.012237549,0.0045547485,-0.024475098,-0.0061187744,0.015098572,-0.052764893,-0.033599854,0.0008583069,-0.047058105,0.004180908,0.005268097,0.031677246,-0.040252686,0.008430481,-0.02420044,0.042419434,0.066101074,-0.0037231445,0.0012922287,-0.0031108856,-0.021484375,-0.019439697,0.0552063,-0.03439331,0.0090408325,0.008224487,0.0037231445,0.016525269,-0.03643799,-0.028015137,-0.020950317,0.0026340485,-0.048950195,0.0073776245,0.020126343,-0.02583313,0.029785156,0.033721924,-0.005847931,0.052490234,0.017410278,-0.023117065,0.061187744,0.045959473,-0.0067329407,0.0065956116,-0.017822266,-0.007106781,0.035095215,-0.04840088,-0.047058105,-0.011016846,0.0040779114,0.012916565,-0.033172607,-0.033050537,-0.025024414,0.02067566,-0.028015137,0.08325195,0.011489868,-0.015914917,-0.024612427,0.03387451,-0.004421234,0.015571594,-0.033996582,0.05331421,0.019439697,-0.007820129,-0.009521484,0.021209717,0.052764893,0.0119018555,0.010475159,-0.0003335476,-0.0028896332,0.010475159,-0.039978027,-0.027191162,0.0030593872,-0.0065612793,0.010810852,-0.020812988,0.018081665,-0.050323486,0.00055265427,-0.0029411316,-0.017822266,0.032104492,0.02923584,-0.013465881,0.00053977966,0.01876831,-0.013938904,0.0064926147,-0.004234314,0.0048294067,0.02746582,-0.035369873,0.066345215,0.028961182,0.033721924,-0.0024642944,-0.039154053,0.061187744,-0.031280518,0.011352539,0.01210022,-0.016448975,-0.016860962,-0.013122559,-0.013328552,0.033325195,-0.11584473,-0.019577026,-0.038360596,-0.013053894,-0.044891357,0.0046920776,-0.00730896,-0.06555176,0.050872803,-0.03781128,-0.017547607,0.011489868,0.031402588,-0.0036201477,0.003774643,0.028427124,-0.0040130615,0.016052246,0.060394287,0.035888672,0.02230835,-0.02053833,0.035888672,-0.02583313,0.038879395,-0.02217102,0.020812988,0.019851685,0.015914917,-0.00020289421,0.016723633,-0.0016489029,0.0010967255,0.033325195,-0.07562256,0.037261963,0.0014619827,-0.013259888,-0.0016832352,-0.06201172,-0.0041122437,0.038360596,-0.025436401,0.018356323,-0.016662598,-0.049224854,0.029373169,0.04623413,-0.0072746277,0.038085938,0.009521484,-0.016937256,-0.039978027,0.020126343,0.035095215,-0.07287598,-0.03781128,0.010131836,-0.024887085,-0.053588867,-0.025161743,0.011695862,-0.027877808,0.029373169,-0.0030765533,0.0073432922,-0.011627197,-0.0055770874,0.028015137,-0.04623413,-0.047332764,0.012039185,0.055480957,-0.00026345253,0.002532959,0.029922485,-0.0036201477,-0.014282227,-0.036712646,-0.019851685,0.023803711,-0.000115811825,0.0690918,0.021621704,0.0020561218,-0.02230835,0.045410156,0.031143188,-0.01625061,0.008598328,0.0009813309,-0.0128479,0.025299072,-0.00028681755,-0.017822266,-0.025161743,-0.013938904,-0.000828743,0.038635254,0.0062561035,0.025299072,-0.016937256,-0.027877808,0.050872803,-0.0056114197,0.060394287,-0.003042221,-0.029922485,0.0040779114,0.0552063,-0.0059509277,-0.0020656586,0.045959473,0.0029411316,-0.030319214,0.0014190674,-0.021621704,0.019989014,-0.02420044,-0.019439697,-0.00073957443,0.021209717,0.048950195,0.02746582,0.011352539,0.010879517,0.00856781,-0.0345459,0.025436401,-0.027069092,-0.054382324,0.02909851,0.021896362,0.03100586,0.007205963,-0.016860962,-0.05114746,-0.020950317,-0.00017213821,0.057922363,0.08483887,0.028015137,0.039154053,-0.013191223,-0.031280518,0.029922485,0.020126343,0.015640259,0.05847168,0.0051002502,0.015975952,-0.008911133,0.03942871,0.025161743,0.03277588,-0.0128479,0.044891357,0.035888672,0.03250122,-0.02067566,0.045684814,0.016860962,-0.03564453,0.059295654,-0.005508423,-0.0029067993,-0.021484375,-0.002243042,0.03387451,-0.012786865,-0.0011386871,0.0063934326,-0.059295654,0.0059165955,0.04107666,-0.071777344,-0.008598328,0.026519775,-0.044891357,0.030197144,-0.017684937,-0.058746338,0.016860962,-0.028015137,-0.040802002,0.015701294,0.03616333,-0.025161743,-0.050598145,-0.0513916,-0.0146865845,-0.009178162,-0.021896362,0.00067567825,0.015296936,-0.00043988228,0.010131836,0.03781128,-0.009590149,-0.016998291,0.0096588135,-0.01625061,-0.07397461,-0.0009689331,0.030731201,-0.02407837,-0.044067383,-0.03387451,0.018218994,0.0019378662,0.008430481,0.03387451,-0.056030273,-0.081604004,-0.03753662,0.033599854,-0.0029754639,0.044891357,-0.0064926147,-0.088134766,0.05114746,-0.04815674,-0.01461792,0.0077171326,-0.0057792664,-0.03781128,0.04650879,0.007987976,-0.015640259,-0.012039185,0.03753662,-0.025299072,0.0010538101,-0.052764893,0.024749756,0.03466797,-0.04949951,0.016937256,0.029510498,0.027740479,0.07885742,-0.040252686,-0.033996582,0.020126343,-0.042144775,-0.025161743,-0.035888672,-0.022842407,-0.0256958,0.017822266,0.02407837,0.027877808,0.012168884,0.014823914,0.036712646,0.015777588,0.025024414,-0.04949951,0.023391724,-0.035888672,-0.019577026,-0.0016317368,0.019577026,-0.02053833,-0.008773804,-0.0064582825,0.0029754639,0.0035705566,0.0003209114,-0.008293152,-0.016052246,0.0039596558,-0.020401001,-0.021621704,0.03086853,0.026245117,-0.012237549,0.015975952,-0.018218994,-0.043518066,0.026657104,0.03643799,0.0055770874,0.04623413,-0.009719849,-0.0021076202,-0.022705078,0.033447266,-0.032104492,-0.016799927,-0.057922363,0.031143188,0.0064582825,-0.022979736,-0.029647827,-0.013664246,-0.04815674,0.0028896332,0.033325195,0.015029907,0.022445679,-0.045684814,-0.022842407,-0.025161743,-0.016662598,0.0019798279,-0.0112838745,0.004047394,0.028823853,-0.020401001,0.0066986084,-0.0256958,0.01033783,0.03277588,-0.0033493042,-0.039978027,-0.007751465,-0.0040130615,-0.023529053,-0.05819702,-0.00060749054,0.019577026,-0.03753662,-0.018218994,0.02557373,0.028152466,0.012718201,-0.020263672,0.0005569458,0.016799927,0.0021591187,0.04815674,-0.043792725,-0.045410156,-0.014961243,-0.0006289482,0.06964111,0.008773804,-0.03970337,-0.0047950745,-0.012512207,0.02420044,-0.019439697,-0.029922485,-0.018081665,0.022705078,0.013053894,0.03643799,0.011222839,0.08538818,0.050598145,-0.018081665,0.04788208,0.060913086,0.052764893,-0.033996582,-0.030456543,-0.04434204,0.009857178,-0.036987305,-0.038879395,0.01625061,0.010604858,-0.08758545,-0.031280518,-0.01550293,0.0040130615,0.0015554428,0.008094788,0.00090932846,0.059020996,0.0065612793,0.025024414,-0.02394104,0.012649536,-0.022979736,0.051940918,0.021759033,-0.023117065,-0.019042969,0.022979736,0.032226562,-0.030197144,0.0102005005,0.022567749,0.019180298,0.018081665,0.044891357,-0.047058105,-0.0446167,-0.0079193115,0.05819702,0.015296936,0.039154053,0.011222839,-0.033447266,0.016998291,-0.017822266,-0.0037403107,0.015777588,0.015777588,-0.013870239,-0.035095215,0.045410156,-0.026245117,0.02394104,0.0047950745,0.0014791489,0.0073776245,0.02407837,-0.047332764,-0.059295654,0.019577026,-0.017684937,-0.04788208,-0.03427124,-0.0070381165,0.07128906,0.021896362,-0.054138184,-0.02923584,0.07342529,-0.01033783,-0.04840088,-0.0018873215,-0.013397217,0.021621704,-0.012237549,-0.06335449,-0.009384155,0.023803711,-0.017547607,-0.005847931,-0.022979736,-0.020126343,0.01713562,0.0020313263,-0.04815674,0.06365967,-0.013191223,0.015365601,0.023529053,-0.00075626373,0.016113281,0.066345215,-0.008224487,-0.035369873,-0.014282227,-0.070739746,0.014007568,-0.00012373924,0.019851685,-0.060638428,-0.009521484,0.06689453,-0.04107666,0.0056419373,0.0031795502,0.000875473,-0.04788208,0.029647827,-0.035369873,-0.010406494,0.062286377,0.040252686,-0.009925842,0.021072388,-0.035095215,0.04840088,0.03427124,0.048950195,-0.07232666,0.023529053,0.011489868,-0.041900635,0.0256958,-0.062805176,-0.029510498,0.01210022,0.0020561218,-0.02067566,0.018493652,-0.0012235641,0.013191223,-0.052490234,0.016113281,0.003162384,-0.00063323975,-0.023666382,-0.03564453,-0.014282227,-0.0657959,-0.028015137,-0.023529053,0.033447266,0.025970459,0.003774643,0.00598526,0.00472641,0.006526947,0.004962921,0.062561035,-0.033996582,0.014076233,-0.008399963,0.00472641,0.060638428,0.021209717,-0.02230835,0.012649536,0.01373291,0.0036888123,0.019714355,0.01625061,0.0043182373,-0.02746582,0.003332138,-0.064208984,0.025024414,0.014549255,0.023529053,-0.0152282715,-0.0050315857,-0.07507324,0.059570312,-0.0084991455,0.04107666,0.026931763,-0.005710602,-0.06173706,-0.034820557,-0.043518066,0.023117065,0.0038757324,0.02217102,-0.0077171326,-0.035888672,0.0119018555,-0.0072402954,0.035888672,0.045684814,0.021209717,-0.034820557,0.056030273,-0.005847931,0.032226562,-0.0072746277,0.021347046,0.035369873,-0.00015830994,-0.0345459,0.005847931,0.016586304,0.026107788,-0.009857178,-0.0056762695,-0.0010881424,-0.033325195,-0.014144897,0.0018787384,0.022445679,-0.009246826,0.009590149,-0.056854248,-0.041900635,-0.110961914,-0.03564453,-0.04434204,-0.031555176,-0.0002708435,0.022979736,0.013397217,-0.044891357,0.028427124,-0.011695862,0.014892578,0.0046920776,-0.0046577454,-0.010948181,0.026382446,-0.0033988953,-0.03289795,0.047607422,0.0056419373,-0.038635254,0.033599854,0.03427124,0.00097322464,0.03250122,-0.0446167,-0.011695862,-0.008842468,-0.02583313,-0.011695862,-0.008842468,0.027740479,-0.040802002,-0.02909851,0.03237915,0.027740479,-0.059020996,0.002193451,0.01197052,-0.0015211105,-0.015701294,0.016723633,0.036712646,0.0128479,-0.009925842,0.016662598,0.009590149,0.003332138,0.045410156,0.01727295,0.052215576,-0.020401001,-0.03100586,-0.002855301,-0.047058105,0.04815674,-0.02394104,-0.05657959,0.021072388,0.048950195,0.051940918,0.0039787292,0.03970337,0.012580872,0.027069092,0.010063171,-0.0067329407,-0.056854248,0.017684937,0.004928589,-0.001572609,-0.03262329,0.066101074,0.07287598,-0.0067329407,-0.031402588,0.018493652,-0.0035705566,0.023803711,0.02230835,-0.00730896,-0.07019043,0.026931763,0.053588867,-0.03616333,-0.014282227,-0.0063934326,-0.026931763,0.050872803,-0.026931763,0.028961182,-0.0124435425,0.07232666,0.022979736,-0.026931763,-0.0077171326,0.035095215,0.001996994,-0.032226562,0.004623413,0.008842468,-0.030319214,0.0256958,0.03289795,0.005302429,0.06365967,0.00070142746,-0.019042969,0.012168884,0.0119018555,0.009246826,0.0076828003,-0.07672119,-0.03753662,-0.06390381,0.02420044,-0.017547607,-0.0021591187,0.0657959,-0.03277588,-0.0032634735,0.015975952,0.0063934326,0.045135498,0.0054397583,0.01108551,-0.042144775,0.044067383,0.0063934326,-0.0048599243,-0.02230835,-0.038360596,0.014007568,-0.0513916,0.0025844574,0.017074585,0.0033988953,0.017547607,0.019714355,0.015098572,-0.014480591,-0.0090408325,-0.018218994,0.031402588,-0.054656982,0.034820557,0.0011644363,0.006290436,0.050598145,-0.038360596,0.023391724,0.014076233,0.038360596,-0.01638794,0.030456543,0.03781128,0.010063171,-0.0054740906,0.017410278,0.018630981,0.01890564,-0.01197052,0.0128479,-0.04296875,0.019317627,-0.0048599243,-0.07397461,0.033721924,0.026245117,0.008972168,0.009246826,-0.0045204163,-0.009994507,-0.05493164,0.028152466,0.013595581,-0.057922363,0.031280518,-0.03970337,0.0069351196,-0.03387451,0.045410156,-0.019714355,0.03781128,0.0072402954,-0.002380371,-0.0030097961,0.02217102,0.020812988,0.057647705,-0.0032634735,0.024887085,0.010673523,-0.011016846,-0.062805176,-0.011627197,0.0079193115,-0.0056114197,0.009857178,0.015701294,-0.0035877228,-0.0046577454,0.010475159,0.022445679,0.022842407,0.03277588,0.0345459,-0.021072388,0.02909851,-0.031402588]", "client_status_at": "2025-10-02T09:50:32.153361Z", "server_status_at": "2025-10-02T09:50:32.153343Z", "created_at": "2025-10-02T08:50:29.883809Z"}, {"id": "52e79d22-fd8d-4bbe-b5df-19ee10c9a5ab", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://www.amazon.science/blog/simplifying-book-discovery-with-ml-powered-visual-autocomplete-suggestions", "canonical_url": "https://www.amazon.science/blog/simplifying-book-discovery-with-ml-powered-visual-autocomplete-suggestions", "title": "Simplifying book discovery with ML-powered visual autocomplete suggestions", "source_site": "Amazon Science", "publication_date": "2025-09-02T00:00:00Z", "favicon_url": "https://www.amazon.science/favicon.ico", "content_markdown": "Every day, millions of customers search for books in various formats (audiobooks, e-books, and physical books) across Amazon and [Audible](https://www.audible.com/). Traditional keyword autocomplete suggestions, while helpful, usually require several steps before customers find their desired content. Audible took on the challenge of making book discovery more intuitive and personalized while reducing the number of steps to purchase.\n\nWe developed an instant visual autocomplete system that enhances the search experience across Amazon and Audible. As the user begins typing a query, our solution provides visual previews with book covers, enabling direct navigation to relevant landing pages instead of the search result page. It also delivers real-time personalized format recommendations and incorporates multiple searchable entities, such as book pages, author pages, and series pages.\n\nOur system needed to understand user intent from just a few keystrokes and determine the most relevant books to display, all while maintaining low latency for millions of queries. Using historical search data, we match keystrokes to products, transforming partial inputs into meaningful search suggestions. To ensure quality, we implemented confidence-based filtering mechanisms, which are particularly important for distinguishing between general queries like \"mystery\" and specific title searches. To reflect customers’ most recent interests, the system applies time-decay functions to long historical user interaction data.\n\nTo meet the unique requirements of each use case, we developed two distinct technical approaches. On Audible, we deployed a deep pairwise-learning-to-rank (DeepPLTR) model. The DeepPLTR model considers pairs of books and learns to assign a higher score to the one that better matches the customer query.\n\nThe DeepPLTR model’s architecture consists of three specialized towers. The left tower factors in contextual features and recent search patterns using a long-short-term-memory model, which processes data sequentially and considers its prior decisions when issuing a new term in the sequence. The middle tower handles keyword and item engagement history. The right tower factors in customer taste preferences and product descriptions to enable personalization. The model learns from paired examples, but at runtime, it relies on books’ absolute scores to assemble a ranked list.\n\nFor Amazon, we implemented a two-stage modeling approach involving a probabilistic information-retrieval model to determine the book title that best matches each keyword and a second model that personalizes the book format (audiobooks, e-books, and physical books). This dual-strategy approach maintains low latency while still enabling personalization.\n\nIn practice, a customer who types \"dungeon craw\" in the search bar now sees a visual recommendation for the book *Dungeon Crawler Carl*, complete with book cover, reducing friction by bypassing a search results page and sending the customer directly to the product detail page. On Audible, the system also personalizes autocomplete results and enriches the discovery experience with relevant connections. These include links to the author's complete works (Matt Dinniman's author page) and, for titles that belong to a series, links to the full collection (such as the Dungeon Crawler Carl series).\n\nOn Amazon, when the customer clicks on the title, the model personalizes the right book-format (audiobooks, e-books, physical books) recommendation and directs the customer to the right product detail page.\n\nIn both cases, after the customer has entered a certain number of keystrokes, the system employs a model to detect customer intent (e.g., book title intent for Amazon or author intent for Audible) and determine which visual widget should be displayed.\n\nAudible and Amazon books’ visual autocomplete provides customers with more relevant content more rapidly than traditional autocomplete, and its direct navigation reduces the number of steps to find and access desired books — all while handling millions of queries at low latency.\n\nThis technology is not just about making book discovery easier; it is laying the foundation for future improvements in search personalization and visual discovery across Amazon's ecosystem.\n\n*Acknowledgements**: Jiun Kim, Sumit Khetan, Armen Stepanyan, Jack Xuan, Nathan Brothers, Eddie Chen, Vincent Lee, Soumy Ladha, Justine Luo, Yuchen Zeng, David Torres, Gali Deutsch, Chaitra Ramdas, Christopher Gomez, Sharmila Tamby, Melissa Ma, Cheng Luo, Jeffrey Jiang, Pavel Fedorov, Ronald Denaux, Aishwarya Vasanth, Azad Bajaj, Mary Heer, Adam Lowe, Jenny Wang, Cameron Cramer, Emmanuel Ankrah, Lydia Diaz, Suzette Islam, Fei Gu, Phil Weaver, Huan Xue, Kimmy Dai, Evangeline Yang, Chao Zhu, Anvy Tran, Jessica Wu, Xiaoxiong Huang, Jiushan Yang*", "content_text": "Every day, millions of customers search for books in various formats (audiobooks, e-books, and physical books) across Amazon and Audible. Traditional keyword autocomplete suggestions, while helpful, usually require several steps before customers find their desired content. Audible took on the challenge of making book discovery more intuitive and personalized while reducing the number of steps to purchase.\nWe developed an instant visual autocomplete system that enhances the search experience across Amazon and Audible. As the user begins typing a query, our solution provides visual previews with book covers, enabling direct navigation to relevant landing pages instead of the search result page. It also delivers real-time personalized format recommendations and incorporates multiple searchable entities, such as book pages, author pages, and series pages.\nOur system needed to understand user intent from just a few keystrokes and determine the most relevant books to display, all while maintaining low latency for millions of queries. Using historical search data, we match keystrokes to products, transforming partial inputs into meaningful search suggestions. To ensure quality, we implemented confidence-based filtering mechanisms, which are particularly important for distinguishing between general queries like \"mystery\" and specific title searches. To reflect customers’ most recent interests, the system applies time-decay functions to long historical user interaction data.\nTo meet the unique requirements of each use case, we developed two distinct technical approaches. On Audible, we deployed a deep pairwise-learning-to-rank (DeepPLTR) model. The DeepPLTR model considers pairs of books and learns to assign a higher score to the one that better matches the customer query.\nThe DeepPLTR model’s architecture consists of three specialized towers. The left tower factors in contextual features and recent search patterns using a long-short-term-memory model, which processes data sequentially and considers its prior decisions when issuing a new term in the sequence. The middle tower handles keyword and item engagement history. The right tower factors in customer taste preferences and product descriptions to enable personalization. The model learns from paired examples, but at runtime, it relies on books’ absolute scores to assemble a ranked list.\nFor Amazon, we implemented a two-stage modeling approach involving a probabilistic information-retrieval model to determine the book title that best matches each keyword and a second model that personalizes the book format (audiobooks, e-books, and physical books). This dual-strategy approach maintains low latency while still enabling personalization.\nIn practice, a customer who types \"dungeon craw\" in the search bar now sees a visual recommendation for the book Dungeon Crawler Carl, complete with book cover, reducing friction by bypassing a search results page and sending the customer directly to the product detail page. On Audible, the system also personalizes autocomplete results and enriches the discovery experience with relevant connections. These include links to the author's complete works (Matt Dinniman's author page) and, for titles that belong to a series, links to the full collection (such as the Dungeon Crawler Carl series).\nOn Amazon, when the customer clicks on the title, the model personalizes the right book-format (audiobooks, e-books, physical books) recommendation and directs the customer to the right product detail page.\nIn both cases, after the customer has entered a certain number of keystrokes, the system employs a model to detect customer intent (e.g., book title intent for Amazon or author intent for Audible) and determine which visual widget should be displayed.\nAudible and Amazon books’ visual autocomplete provides customers with more relevant content more rapidly than traditional autocomplete, and its direct navigation reduces the number of steps to find and access desired books — all while handling millions of queries at low latency.\nThis technology is not just about making book discovery easier; it is laying the foundation for future improvements in search personalization and visual discovery across Amazon's ecosystem.\nAcknowledgements: Jiun Kim, Sumit Khetan, Armen Stepanyan, Jack Xuan, Nathan Brothers, Eddie Chen, Vincent Lee, Soumy Ladha, Justine Luo, Yuchen Zeng, David Torres, Gali Deutsch, Chaitra Ramdas, Christopher Gomez, Sharmila Tamby, Melissa Ma, Cheng Luo, Jeffrey Jiang, Pavel Fedorov, Ronald Denaux, Aishwarya Vasanth, Azad Bajaj, Mary Heer, Adam Lowe, Jenny Wang, Cameron Cramer, Emmanuel Ankrah, Lydia Diaz, Suzette Islam, Fei Gu, Phil Weaver, Huan Xue, Kimmy Dai, Evangeline Yang, Chao Zhu, Anvy Tran, Jessica Wu, Xiaoxiong Huang, Jiushan Yang", "content_token_count": 933, "client_status": "queued", "server_status": "embedded", "summary": "Amazon Science discusses a new ML-powered visual autocomplete system designed to simplify book discovery on Amazon and Audible. This system provides visual previews and personalized format recommendations, reducing the steps needed for customers to find and purchase books.", "expiry_score": 0.3, "mistral_embedding": "[-0.04724121,0.028823853,0.056610107,-0.011375427,0.024139404,0.03125,0.03265381,0.0075531006,-0.0071640015,0.008552551,-0.035247803,0.07745361,-0.014587402,-0.0003080368,-0.0317688,0.025009155,-0.0039711,0.011978149,0.014930725,-0.0112838745,-0.019714355,-0.017364502,-0.049316406,0.004016876,0.02178955,0.046539307,0.0103302,-0.06945801,-0.060760498,-0.0070762634,0.035064697,-0.035949707,0.0158844,0.023788452,-0.010719299,-0.008422852,0.0033855438,-0.03994751,0.010894775,-0.017196655,-0.03125,0.0128479,-0.006164551,-0.020401001,0.002040863,-0.009857178,0.012069702,0.004776001,0.013893127,0.017974854,-0.035247803,0.037841797,0.012588501,0.0017795563,-0.00907135,0.014060974,0.0054893494,-0.0017471313,-0.056610107,0.040100098,-0.043060303,-0.0041656494,-0.008987427,-0.0060768127,0.014587402,0.0031909943,-0.012588501,-0.021881104,-0.005470276,0.013717651,0.020309448,0.005859375,-0.008857727,0.024139404,-0.0053596497,-0.044799805,0.024307251,-0.010635376,0.015281677,0.013282776,-0.016921997,-0.010414124,0.047576904,-0.008811951,-0.018844604,0.017364502,0.01449585,0.010635376,0.007118225,0.054504395,0.021957397,0.004447937,0.004146576,-0.010894775,0.0670166,0.035247803,-0.020492554,0.0004014969,-0.020233154,0.038360596,0.025527954,-0.03543091,0.075683594,-0.032470703,0.006465912,-0.02204895,0.050354004,-0.060424805,-0.0151901245,-0.028656006,-0.0680542,-0.025344849,-0.044281006,-0.056610107,-0.020309448,0.0066833496,0.0105896,0.020233154,-0.0513916,-0.04827881,0.03768921,0.039245605,-0.015716553,0.026565552,-0.0413208,-0.027954102,0.0046653748,0.025863647,-0.017623901,-0.015365601,-0.008728027,-0.018753052,-0.0011072159,-0.03717041,-0.0059051514,-0.0043411255,0.048950195,-0.0184021,-0.032470703,0.0005750656,0.043914795,-0.045837402,-0.017456055,-0.0131073,-0.059387207,-0.025344849,0.0057296753,0.066345215,-0.0093307495,-0.0048179626,-0.035247803,0.064941406,0.025527954,-0.04046631,-0.05557251,0.0031261444,-0.018051147,-0.011070251,0.039245605,-0.021621704,-0.025863647,0.046875,-0.00655365,0.025344849,-0.023086548,-0.004600525,-0.0012588501,0.023620605,-0.030563354,-0.0068130493,0.021270752,-0.060760498,0.0715332,0.07116699,0.0062065125,0.058685303,-0.004535675,0.00032019615,0.07397461,0.031951904,-0.048614502,-0.020401001,0.029510498,-0.03265381,0.030899048,-0.01701355,-0.008857727,-0.0034294128,-0.034729004,-0.0069885254,-0.001627922,-0.0289917,-0.019363403,-0.0063819885,0.0042762756,0.046875,-0.0028438568,-0.025177002,0.008033752,-0.020141602,0.04324341,0.030380249,-0.031951904,0.087524414,0.027786255,0.008987427,-0.02760315,0.010375977,0.0289917,-0.008987427,-0.02482605,0.017791748,-0.030212402,-0.016403198,-0.023956299,-0.013282776,0.0029296875,-0.012588501,0.025177002,-0.022567749,-0.021621704,-0.05105591,-0.009635925,0.015541077,0.015625,0.031951904,0.03265381,0.009719849,-0.023086548,0.009117126,0.013626099,0.019882202,-0.024475098,-0.015541077,0.014152527,-0.019622803,0.061798096,0.0440979,0.010284424,0.015365601,-0.009941101,0.0256958,-0.04827881,0.0184021,0.032989502,-0.025527954,-0.021362305,0.0044059753,0.0010414124,0.032470703,-0.06390381,-0.0065994263,-0.0715332,-0.014671326,-0.046539307,-0.046875,-0.033172607,-0.0579834,0.027435303,-0.003232956,0.0079422,-0.008331299,0.024307251,0.013366699,0.021438599,0.0075950623,-0.076416016,0.018585205,0.10211182,0.016921997,-0.007987976,-0.014755249,0.023956299,-0.0057296753,0.03543091,-9.429455e-05,0.016403198,0.031951904,0.030044556,-0.006034851,-0.026916504,0.017791748,0.0011014938,0.046875,-0.032989502,-0.013282776,-0.008163452,-0.0027999878,-0.012588501,-0.07293701,0.0073776245,0.06112671,0.018753052,0.0262146,-0.0121536255,-0.019180298,0.013282776,0.056243896,-0.00390625,0.04046631,0.043914795,-0.021270752,-0.0357666,0.0121536255,0.033691406,-0.045135498,-0.05001831,-0.024139404,-0.011810303,-0.06768799,0.0041007996,0.0009279251,-0.025527954,-0.015022278,-0.011199951,0.019882202,-0.04550171,-0.040618896,-0.00258255,0.004600525,-0.065979004,-0.025009155,0.07220459,-0.014671326,-0.014411926,-0.0069007874,0.015975952,-0.0340271,0.011978149,-0.022750854,0.026733398,-0.011375427,0.017456055,0.03543091,-0.0010089874,0.017364502,0.0423584,0.014328003,-0.043060303,-0.018920898,-0.006729126,-0.02760315,0.006511688,0.00522995,-0.015716553,-0.017623901,-0.027435303,-0.002603531,0.011978149,0.018585205,-0.0037765503,-0.0131073,0.026046753,0.031433105,-0.009719849,0.09515381,-0.0031032562,-0.01927185,-0.0060768127,0.0395813,-0.041503906,0.0049934387,0.035949707,0.02482605,0.023086548,0.030731201,-0.017456055,0.026565552,-0.011116028,0.0014324188,0.016586304,0.025009155,0.039764404,0.046539307,-0.014755249,-0.0016927719,0.029693604,-0.019882202,-0.0010471344,-0.019439697,-0.0715332,0.015106201,-0.00081920624,0.056610107,0.013366699,-0.012672424,-0.06561279,-0.006641388,-0.021621704,0.08441162,0.06317139,0.036621094,0.010246277,-0.026916504,0.029510498,0.0011444092,0.0335083,0.013023376,0.05279541,0.005382538,-0.0023326874,-0.03768921,0.020233154,-0.0029735565,0.059020996,-0.0044059753,0.03265381,0.021621704,0.042877197,-0.02708435,0.038024902,0.017974854,0.0027999878,0.034729004,-0.004928589,-0.013023376,-0.0496521,0.0034503937,-0.033325195,-0.007896423,0.009376526,-0.0023231506,-0.03213501,0.009460449,0.0262146,-0.043395996,-0.032806396,-0.0037326813,-0.012069702,0.04220581,-0.032989502,-0.018234253,0.05557251,-0.005405426,-0.045837402,0.0357666,0.0028438568,-0.020309448,-0.017623901,-0.022567749,0.034729004,0.002149582,0.0020179749,0.032287598,-0.0059890747,0.0009713173,0.030563354,-0.01701355,0.00068616867,-0.0552063,0.02986145,-0.01953125,-0.011199951,0.038360596,0.031082153,-0.013893127,-0.047576904,-0.007858276,-0.0007324219,-0.021530151,0.01727295,0.01675415,-0.022399902,-0.056243896,-0.0670166,0.033172607,0.017623901,0.034729004,-0.008682251,-0.07220459,0.035064697,-0.053833008,0.0010576248,-0.013801575,0.006164551,0.006034851,0.0513916,0.015625,0.013626099,-0.022750854,0.06665039,0.008636475,0.019439697,-0.021697998,0.035247803,0.046539307,-0.0597229,-0.0579834,0.03942871,0.014328003,0.0826416,-0.038879395,-0.017196655,-0.038879395,-0.05834961,-0.04220581,0.0023326874,-0.0418396,-0.036621094,0.016143799,0.013893127,-0.009681702,0.018844604,-0.020233154,0.017623901,0.0289917,-0.00051021576,-0.054870605,0.030212402,-0.06530762,-0.029510498,-0.004688263,0.005947113,-0.022399902,-0.019882202,-0.0093307495,-0.054504395,0.0178833,-0.020141602,0.006511688,-0.0042533875,0.004623413,-0.026733398,-0.009025574,0.019104004,0.015625,0.02204895,-0.009025574,0.0068588257,-0.06427002,0.038360596,0.01927185,-0.0390625,-0.019012451,0.016662598,0.02204895,-0.012237549,0.033325195,-0.005252838,-0.038360596,-0.059020996,0.012931824,-0.0024642944,0.0023441315,-0.029342651,-0.018493652,-0.024475098,-0.007423401,0.046875,0.040618896,0.019622803,-0.043060303,0.0079422,-0.0053596497,-0.041137695,0.020233154,-0.018585205,0.025527954,-0.006465912,0.02204895,0.027954102,0.0051651,0.04550171,0.030731201,0.011116028,-0.026916504,0.0234375,0.001074791,-0.018585205,-0.032989502,-0.0151901245,0.021438599,-0.022567749,-0.011543274,0.0010957718,0.019439697,0.016143799,-0.015975952,-0.008377075,-0.014152527,0.009025574,0.014671326,0.010505676,-0.018051147,0.011459351,-0.0036678314,0.0049705505,0.039764404,0.005771637,0.0065994263,-0.006164551,0.0256958,-0.021530151,-0.051727295,-0.036468506,0.004447937,0.030212402,0.066345215,0.012069702,0.071899414,0.030899048,0.02708435,0.01979065,0.044799805,0.012504578,-0.02178955,-0.0826416,-0.015449524,-0.0005235672,-0.056243896,-0.041503906,0.0158844,0.039764404,-0.06768799,0.0056419373,-0.029510498,-0.004299164,0.0038852692,0.016235352,0.00894165,0.038726807,0.0005507469,-0.013023376,0.017532349,-0.008857727,-0.017532349,0.023620605,0.028656006,0.004512787,0.0015954971,0.02986145,-0.021270752,-0.025177002,0.026733398,0.020568848,0.030731201,0.011543274,0.07434082,-0.042541504,-0.0006213188,0.0036888123,0.047912598,0.020050049,0.039764404,-0.031433105,0.001996994,0.01979065,-0.031951904,-0.014671326,0.025344849,0.001584053,-0.023086548,-0.017456055,0.022567749,-0.054504395,0.0039482117,0.035949707,0.013198853,-0.015365601,0.049316406,-0.051727295,-0.05001831,0.014587402,-0.026046753,-0.043914795,-0.0018234253,-0.041503906,0.03768921,-0.015975952,-0.07952881,0.011543274,0.06008911,0.012763977,-0.0368042,-0.019012451,0.0112838745,0.012763977,-0.004711151,-0.03717041,-0.01449585,-0.0151901245,-0.036468506,0.0005044937,-0.010848999,-0.050354004,0.008071899,-0.040283203,-0.029510498,0.058685303,0.008331299,0.020492554,0.03213501,-0.01675415,0.04046631,0.049316406,0.012931824,-0.0284729,0.009025574,-0.060760498,0.010635376,-0.018585205,0.024307251,-0.054870605,0.0028858185,0.029342651,-0.045135498,0.04446411,0.023788452,-0.004081726,-0.022232056,0.024475098,-0.05001831,0.0036468506,0.06842041,-0.009422302,-0.011810303,-0.004016876,-0.036102295,0.0670166,0.016235352,0.03942871,-0.07293701,0.044799805,0.026916504,-0.0010910034,0.008895874,-0.07672119,-0.04272461,0.053833008,-0.003320694,-0.014587402,0.024139404,-0.018051147,-0.0158844,-0.043395996,0.023086548,0.004840851,0.0026474,-0.0082473755,-0.001627922,-0.010375977,-0.019622803,-0.000808239,-0.0395813,0.0018987656,0.031433105,-0.047576904,0.0121536255,0.0071640015,0.032989502,0.03125,0.030212402,-0.015365601,-0.0234375,0.0015630722,-0.0009870529,-0.030731201,0.037506104,-0.020568848,0.030212402,0.038879395,-0.0234375,0.0079422,0.048950195,-0.014411926,-0.0093307495,-0.026397705,-0.049316406,0.0390625,-0.015022278,0.0102005005,0.017364502,0.05105591,-0.07293701,0.08227539,-0.0317688,-0.0076828003,-0.0063819885,0.006164551,-0.0715332,-0.038879395,-0.053131104,0.003232956,-0.027786255,0.008293152,0.03491211,0.003232956,-0.029342651,0.00038790703,0.01675415,0.044799805,0.024658203,0.011024475,0.05001831,-0.024307251,-0.0027561188,-0.013717651,0.008331299,0.017623901,-0.032287598,-0.032470703,0.009681702,0.0059890747,-0.005470276,-0.039764404,0.014755249,0.015449524,-0.011810303,-0.020492554,-0.017532349,-0.0016708374,0.0345459,0.008682251,-0.036621094,-0.0335083,-0.0881958,-0.0418396,-0.062164307,-0.0262146,-0.039245605,-0.0063819885,0.012069702,-0.02708435,0.026397705,0.013717651,-0.0158844,-0.0054244995,0.007118225,-0.0362854,0.038726807,-0.017196655,-0.014846802,0.023956299,0.0079422,-0.021270752,-0.020233154,0.006034851,-0.025009155,0.022567749,-0.051727295,-0.011810303,-0.013893127,0.019622803,0.0317688,-0.0029296875,0.027252197,-0.010765076,-0.04446411,0.012413025,0.0012044907,-0.046173096,0.010551453,0.003276825,-0.0496521,-0.0017576218,0.015716553,0.030899048,0.08441162,-0.0053596497,-0.006641388,-0.0029296875,-0.0049705505,0.013717651,0.026565552,0.0340271,0.013626099,0.015716553,0.00081920624,-0.0541687,0.015449524,-0.01927185,-0.05731201,-0.0027561188,0.0597229,0.022567749,0.023620605,0.0018014908,0.052093506,0.04827881,0.027786255,-0.028823853,-0.0413208,-0.019439697,0.05557251,0.011894226,-0.08679199,0.038208008,0.050354004,0.012672424,-0.04724121,0.005470276,-0.021621704,0.038360596,0.001616478,0.024475098,-0.05279541,0.016921997,0.02482605,0.0041885376,0.0016384125,0.014060974,-0.022567749,0.04324341,-0.013023376,0.017623901,0.024307251,0.08782959,-0.011459351,-0.0234375,-0.008377075,-0.0054893494,0.0102005005,-0.050689697,-0.012588501,0.008766174,-0.016143799,0.030899048,0.027954102,0.011459351,0.07116699,-0.024139404,-0.022918701,0.006641388,0.012931824,-0.022140503,0.0008845329,-0.03439331,-0.008117676,-0.07501221,0.06530762,-0.0048828125,0.004299164,0.009811401,-0.0513916,0.017105103,0.017364502,0.038879395,0.031433105,-0.0052948,-0.017974854,-0.034729004,0.056243896,-0.0012369156,0.0011072159,0.016662598,-0.037322998,0.011634827,-0.0435791,0.01449585,-0.022399902,-0.0069007874,-0.0005373955,0.017623901,0.014328003,-0.036468506,-0.021270752,-0.0025291443,0.0413208,-0.033325195,0.0209198,-0.0014219284,-0.01953125,0.001627922,-0.07086182,0.0552063,-0.0024967194,-0.00096035004,-0.0206604,0.010978699,0.054870605,-0.018753052,0.03213501,-0.009765625,0.01979065,0.015106201,0.023956299,0.015449524,-0.041503906,0.03439331,0.0011234283,-0.042022705,0.0024738312,0.0018119812,0.020141602,0.02204895,-0.008071899,-0.013717651,-0.020233154,0.0725708,-0.006729126,-0.032470703,0.0066833496,-0.044281006,0.060760498,-0.009857178,0.07397461,-0.04272461,0.0030822754,-0.0046463013,0.021957397,-0.059387207,0.0029945374,-0.0038414001,0.019622803,-0.04220581,0.056610107,-0.016586304,0.0038852692,-0.056610107,-0.020309448,-0.015625,0.012588501,0.048950195,-0.0024528503,-0.050689697,0.020309448,0.03439331,0.03994751,0.010505676,0.06842041,-0.012931824,0.012931824,-0.046539307,-0.007858276]", "client_status_at": "2025-10-02T09:50:27.953327Z", "server_status_at": "2025-10-02T09:50:27.953311Z", "created_at": "2025-10-02T08:50:23.567752Z"}, {"id": "459297f4-3c2e-4009-8df9-c3694e16a4d4", "user_id": "1b641c6d-6772-42de-81f7-178f74fa65a3", "url": "https://agentclientprotocol.com/overview/introduction", "canonical_url": null, "title": "Introduction - Agent Client Protocol", "source_site": "Agent Client Protocol", "publication_date": "2025-07-16T00:00:00Z", "favicon_url": "https://agentclientprotocol.com/favicon.ico", "content_markdown": "The Agent Client Protocol standardizes communication between code editors (IDEs, text-editors, etc.) and coding agents (programs that use generative AI to autonomously modify code).The protocol is still under development, but it should be complete enough to build interesting user experiences using it.\n\nAI coding agents and editors are tightly coupled but interoperability isn’t the default. Each editor must build custom integrations for every agent they want to support, and agents must implement editor-specific APIs to reach users.\nThis creates several problems:\n\nIntegration overhead: Every new agent-editor combination requires custom work\n\nLimited compatibility: Agents work with only a subset of available editors\n\nDeveloper lock-in: Choosing an agent often means accepting their available interfaces\n\nACP solves this by providing a standardized protocol for agent-editor communication, similar to how the Language Server Protocol (LSP) standardized language server integration.Agents that implement ACP work with any compatible editor. Editors that support ACP gain access to the entire ecosystem of ACP-compatible agents.\nThis decoupling allows both sides to innovate independently while giving developers the freedom to choose the best tools for their workflow.\n\nACP assumes that the user is primarily in their editor, and wants to reach out and use agents to assist them with specific tasks.Agents run as sub-processes of the code editor, and communicate using JSON-RPC over stdio. The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.", "content_text": "The Agent Client Protocol standardizes communication between code editors (IDEs, text-editors, etc.) and coding agents (programs that use generative AI to autonomously modify code).The protocol is still under development, but it should be complete enough to build interesting user experiences using it.\nAI coding agents and editors are tightly coupled but interoperability isn’t the default. Each editor must build custom integrations for every agent they want to support, and agents must implement editor-specific APIs to reach users.\nThis creates several problems:\nIntegration overhead: Every new agent-editor combination requires custom work\nLimited compatibility: Agents work with only a subset of available editors\nDeveloper lock-in: Choosing an agent often means accepting their available interfaces\nACP solves this by providing a standardized protocol for agent-editor communication, similar to how the Language Server Protocol (LSP) standardized language server integration.Agents that implement ACP work with any compatible editor. Editors that support ACP gain access to the entire ecosystem of ACP-compatible agents.\nThis decoupling allows both sides to innovate independently while giving developers the freedom to choose the best tools for their workflow.\nACP assumes that the user is primarily in their editor, and wants to reach out and use agents to assist them with specific tasks.Agents run as sub-processes of the code editor, and communicate using JSON-RPC over stdio. The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.", "content_token_count": 330, "client_status": "queued", "server_status": "embedded", "summary": "The Agent Client Protocol (ACP) aims to standardize communication between code editors and AI coding agents, addressing issues like integration overhead, limited compatibility, and developer lock-in. It allows for independent innovation and tool choice by providing a common protocol for agent-editor interaction.", "expiry_score": 0.3, "mistral_embedding": "[-0.039978027,0.0028362274,0.02746582,0.031204224,0.035339355,-0.0061912537,0.047973633,0.012573242,-0.034576416,0.017410278,-0.053894043,0.0541687,-0.018951416,0.00040102005,-0.0435791,0.03894043,-0.017410278,0.022949219,0.02178955,0.02166748,-0.023345947,0.006931305,-0.048217773,-0.026184082,-0.017288208,-0.019729614,-3.451109e-05,-0.064453125,-0.025146484,0.011413574,0.03274536,-0.015602112,0.0435791,0.006641388,-0.008834839,-0.006416321,-0.0041923523,-0.041778564,0.016052246,0.0066719055,0.0009226799,0.0049324036,0.011932373,0.01586914,0.00793457,0.009605408,0.020629883,-0.007671356,0.023208618,0.0060272217,0.022567749,0.040740967,-0.018569946,-0.010253906,0.0037879944,0.026443481,-0.0012731552,0.012893677,-0.035064697,0.031463623,-0.046936035,0.0020141602,0.0112838745,-0.007736206,0.019348145,0.019470215,0.0124435425,-0.04901123,-0.02720642,-0.042053223,-0.004547119,0.0029335022,-0.0061912537,0.03289795,-0.047454834,-0.015090942,0.016113281,0.0058670044,0.0006327629,-0.020507812,-0.013221741,0.018051147,0.056488037,-0.0053520203,-0.068115234,0.013801575,0.046417236,0.006126404,-0.011932373,0.014511108,0.017150879,-0.018447876,0.020507812,-0.037384033,0.04953003,0.035858154,-0.024887085,0.014831543,0.0023536682,0.063964844,0.03869629,0.0064811707,0.089782715,-0.0064811707,0.009155273,-0.030044556,0.024887085,0.026046753,-0.019729614,-0.028625488,-0.07684326,-0.021530151,0.010444641,-0.06112671,-0.008895874,-0.0041923523,0.00017726421,0.062408447,-0.064453125,-0.018829346,0.045654297,0.010383606,-0.020370483,0.0064811707,-0.046691895,-0.028244019,0.025283813,0.02192688,-0.030822754,0.0064468384,-0.023208618,-0.023727417,-0.02166748,-0.036376953,0.030441284,-0.021026611,0.042053223,0.014572144,-0.035583496,-0.01373291,0.034576416,-0.03842163,-0.030181885,0.0079956055,-0.012832642,-0.03326416,-0.0022563934,0.036376953,-0.01586914,-0.016189575,0.0112838745,-0.0038852692,0.06707764,-0.01637268,0.025665283,-0.031082153,0.0067367554,-0.024887085,0.036102295,-0.026947021,-0.022567749,0.0107040405,-0.028244019,0.023864746,-0.02230835,0.008834839,-0.026046753,0.0006327629,-0.0015392303,-0.0079956055,0.024627686,-0.014701843,0.041015625,0.018188477,-0.01625061,0.03817749,-0.007091522,0.012382507,0.046936035,-0.014183044,-0.01689148,-0.019470215,0.04849243,0.040222168,0.02128601,-0.023208618,-0.059570312,0.004802704,-0.025283813,0.009414673,-0.023468018,-0.034057617,-0.06060791,0.007381439,0.014831543,0.068115234,0.029525757,-0.045654297,0.0552063,-0.006931305,0.036895752,0.023605347,-0.012573242,0.030441284,0.02192688,-0.021408081,-0.027862549,0.0034980774,0.0056419373,-0.014701843,-0.013153076,0.015541077,0.007575989,-0.03173828,0.0118637085,-0.024108887,0.062408447,0.0118637085,-0.00013196468,0.009223938,-0.010124207,-0.04257202,-0.006965637,-0.030685425,0.020248413,0.031082153,0.03894043,-0.01386261,-0.012763977,-0.0107040405,0.008125305,0.046936035,-0.023208618,0.023864746,-0.026046753,-0.031341553,0.052886963,0.030685425,0.010055542,0.013542175,4.9591064e-05,0.016052246,0.000374794,0.05441284,0.036621094,-0.012962341,0.009674072,-0.008575439,0.026687622,0.034057617,-0.06137085,0.006931305,-0.04925537,0.007736206,-0.07891846,-0.030441284,-0.0011529922,-0.021148682,0.029144287,-0.041015625,-0.018951416,0.034057617,0.029281616,0.01109314,0.039215088,0.020111084,-0.047973633,0.017150879,0.062408447,-0.021148682,-0.0062561035,-0.03378296,-0.0124435425,-0.00029826164,0.051849365,-0.02166748,-0.018188477,0.023864746,0.028503418,0.017532349,-0.017028809,0.022827148,0.004611969,0.009605408,-0.030181885,-0.024108887,-0.007835388,2.0384789e-05,-0.024887085,-0.031463623,0.023208618,0.02746582,0.030044556,0.015792847,-0.0077705383,-0.040222168,0.023605347,0.019470215,0.0070610046,0.06036377,0.061645508,-0.020507812,0.0028533936,0.010574341,0.026184082,0.0071258545,-0.056732178,0.001950264,-0.016311646,-0.09442139,-0.04437256,0.006511688,0.00019037724,-0.011802673,-0.040496826,0.01676941,-0.06188965,-0.027862549,0.0025138855,8.159876e-05,-0.009735107,-0.02128601,0.064208984,0.010574341,-0.011474609,0.017791748,-0.01651001,-0.039733887,0.0041923523,-0.0030956268,0.004901886,-0.01663208,0.025787354,0.03894043,0.016189575,0.029403687,0.041778564,0.02268982,-0.024627686,0.01360321,-0.024505615,0.009025574,0.057769775,-0.03945923,-0.0022087097,-0.017791748,-0.021530151,-0.023605347,0.0061912537,0.013801575,0.005027771,-0.008384705,-0.007575989,0.03765869,-0.007156372,0.07116699,-0.025787354,-0.01651001,0.017791748,0.011993408,-0.035858154,0.008514404,0.0067710876,0.008644104,0.015151978,0.002023697,-0.008384705,0.042816162,-0.017288208,0.012313843,0.004611969,0.0118637085,-0.008155823,0.028884888,0.012641907,0.025665283,0.012184143,-0.04977417,0.0029659271,-0.01689148,-0.0637207,0.01663208,0.053649902,0.035858154,0.026184082,-0.015731812,-0.03302002,0.011024475,-0.023345947,0.086120605,0.078430176,0.02218628,-0.003835678,-0.008644104,0.007446289,-0.035858154,0.0022563934,0.012054443,0.074279785,-0.009155273,-0.008094788,-0.045654297,0.020111084,0.042816162,0.02230835,0.012962341,0.024246216,0.013153076,0.050811768,-0.014381409,0.028762817,-0.004257202,-0.0637207,0.02810669,-0.028884888,-0.007545471,-0.030044556,0.023727417,-4.863739e-05,0.008766174,0.02708435,0.0043525696,-0.034301758,-0.011474609,0.08251953,-0.05984497,-0.045654297,0.05493164,-0.0435791,0.007835388,-0.03326416,-0.036376953,0.06549072,-0.009155273,-0.0446167,0.017150879,0.018188477,-0.01651001,-0.040222168,-0.018051147,0.026565552,0.011543274,0.008705139,0.022445679,-0.009155273,-0.016311646,0.007835388,0.03765869,0.00014805794,-0.046936035,0.045898438,0.018188477,-0.019348145,0.053649902,0.046936035,-0.025283813,-0.04537964,0.017791748,-0.0071258545,-0.012512207,0.0011367798,0.041259766,-0.039733887,-0.07220459,0.017150879,0.0446167,0.031463623,0.036895752,0.005256653,-0.074279785,0.031204224,-0.05545044,-0.0058021545,0.008895874,0.013412476,-0.022567749,0.0552063,0.019729614,-0.003320694,0.025405884,0.050567627,-0.007156372,-0.0028858185,-0.042297363,0.053894043,-0.0015478134,-0.052368164,-0.02268982,0.0030784607,0.016113281,0.05105591,-0.039733887,-0.0065460205,-0.055725098,-0.05545044,-0.042053223,0.017028809,-0.00039887428,-0.05029297,0.020248413,0.019210815,0.0021915436,0.020507812,-0.004047394,0.010253906,0.03353882,0.011993408,-0.02218628,0.02798462,-0.051574707,-0.03765869,0.013023376,0.031463623,-0.028366089,0.008644104,0.0124435425,-0.030563354,-0.013221741,-0.03945923,0.0008020401,0.007091522,-0.003967285,-0.034301758,-0.046173096,0.029525757,0.030944824,0.0070610046,0.011672974,0.046691895,-0.028625488,0.035583496,0.02166748,-0.02798462,-0.020767212,0.004047394,0.019989014,0.0006084442,0.0031280518,-0.019210815,-0.03842163,-0.052093506,0.009864807,-0.014892578,-0.015602112,-0.03173828,-0.027862549,-0.036895752,0.03817749,0.035583496,0.036376953,0.02760315,-0.03353882,0.015220642,-0.00096321106,-0.040496826,0.03173828,-0.025787354,-0.0016202927,-0.002708435,0.0055770874,0.011672974,0.0025959015,0.019088745,0.035858154,0.03237915,-0.058807373,0.01689148,-0.018310547,-0.045135498,-0.034576416,-0.023208618,0.009414673,-0.028503418,-0.037902832,-0.019088745,0.039733887,-0.0014505386,-0.003982544,-0.047973633,0.036621094,0.0016527176,0.04901123,0.010643005,-0.045898438,0.008964539,-0.02708435,0.0435791,0.03353882,-0.029144287,-0.026824951,0.004737854,0.03817749,-0.014381409,-0.0541687,-0.025146484,0.01689148,0.009094238,0.05984497,-0.003353119,0.061645508,0.015220642,-0.018310547,-0.013412476,0.048217773,0.05493164,0.012763977,-0.02218628,-0.020767212,-0.026306152,-0.06756592,-0.062927246,-0.0124435425,0.035858154,-0.07476807,-0.029281616,0.0017652512,-0.007255554,-0.00061655045,0.01083374,0.003578186,0.052093506,0.0077705383,-0.035339355,-0.0061912537,0.030944824,0.01096344,0.017028809,0.06500244,-0.0060920715,0.019607544,0.01109314,0.025146484,-0.0024337769,0.035583496,0.01689148,0.00038075447,0.053375244,0.05105591,-0.009994507,-0.045898438,-0.019210815,0.02166748,0.014640808,0.0541687,0.0034828186,-0.017410278,0.009223938,0.00422287,0.014183044,0.011413574,-0.020248413,-0.022567749,-0.051849365,0.024765015,-0.03289795,-0.0043525696,0.051849365,-0.023864746,0.00548172,0.022949219,-0.0034828186,-0.045898438,0.037139893,-0.047973633,-0.05105591,-0.035064697,-0.047729492,0.036102295,-0.010513306,-0.06756592,-0.025024414,0.07684326,0.017410278,-0.024765015,0.011222839,0.002368927,-0.032104492,-0.026046753,-0.04385376,0.0019025803,-0.028366089,-0.0647583,-0.018310547,-0.025024414,-0.0013303757,0.018569946,-0.029663086,-0.051574707,0.037902832,-0.030181885,0.021026611,0.023468018,-0.0029506683,0.041778564,0.04953003,-0.014831543,-0.008644104,-0.02772522,-0.07220459,-0.01109314,0.006416321,0.026687622,-0.06347656,-0.037902832,0.06604004,-0.052886963,-0.024368286,0.05105591,-0.030822754,-0.004512787,0.026046753,-0.03326416,-0.0005402565,0.026443481,0.01109314,-0.031082153,-0.013931274,-0.0657959,0.056732178,0.019607544,0.035339355,-0.037139893,0.023727417,-0.026443481,-0.017028809,0.014381409,-0.1104126,-0.03378296,0.025024414,0.014892578,0.0011119843,-0.0017814636,0.0060272217,-0.026443481,-0.039215088,0.016113281,0.029663086,0.0015392303,-0.035339355,-0.021148682,-0.019989014,-0.03945923,-0.046691895,-0.04977417,0.03250122,0.03894043,-0.019470215,0.020111084,-0.0011529922,0.006641388,-0.004611969,0.047210693,-0.034301758,0.017150879,-0.006351471,0.024505615,0.026824951,0.029785156,-0.023208618,0.015731812,0.030044556,0.014511108,0.036895752,0.034576416,-0.0025634766,-0.032104492,-0.0059661865,-0.052886963,0.02166748,0.0033054352,0.04925537,0.021026611,-0.0053520203,-0.086120605,0.053894043,-0.00068092346,0.0064468384,-0.004257202,-0.030944824,-0.06915283,-0.0552063,-0.046691895,0.021530151,-0.037384033,0.021026611,0.04257202,-0.0024337769,0.017028809,0.012763977,0.008384705,0.044891357,0.009674072,-0.02204895,0.10058594,-0.013412476,-0.017791748,0.0044174194,0.024368286,0.003917694,-0.019470215,0.0012655258,0.0012817383,0.028366089,0.039215088,-0.018692017,-0.0015392303,0.0016117096,-0.019348145,0.013801575,-0.00070905685,-0.012962341,0.00025701523,-0.03765869,0.0003566742,-0.03945923,-0.0892334,-0.010574341,-0.046417236,-0.024627686,-0.009674072,0.019729614,0.025405884,-0.022949219,0.012962341,0.036621094,-0.013412476,-0.023345947,0.029525757,0.0066719055,0.03250122,0.0052871704,-0.044891357,0.018051147,-0.004966736,-0.037902832,-0.015930176,0.042053223,-0.017669678,0.024627686,-0.037902832,-0.007965088,-0.009475708,0.010185242,0.03378296,0.020767212,0.023605347,-0.020629883,-0.06036377,0.011672974,0.019210815,-0.062927246,0.026687622,0.02166748,-0.0040130615,0.01134491,0.026443481,0.018051147,0.08148193,0.0065460205,-0.02192688,-0.011024475,-0.013542175,-0.0009994507,-0.013671875,0.057250977,0.010185242,0.039733887,-0.003868103,-0.05493164,0.006931305,-0.007545471,-0.059051514,0.009735107,0.073791504,0.04953003,0.0061912537,0.023468018,0.04257202,0.074279785,0.010894775,-0.029022217,-0.036895752,0.00066518784,0.031585693,-0.012054443,-0.064453125,0.034576416,0.047729492,-0.0028209686,-0.036621094,0.011154175,-0.014701843,0.052368164,0.037139893,0.01373291,-0.07067871,0.019729614,-0.015670776,0.004047394,-0.04901123,-0.0040283203,0.028762817,0.030303955,-0.057006836,-0.0041923523,0.005092621,0.07116699,0.0044822693,-0.07171631,-0.013092041,0.030044556,0.00818634,-0.0031280518,0.0029659271,0.010513306,-0.024887085,-0.007801056,0.018447876,0.0033359528,0.06060791,-0.017150879,-0.02746582,0.019607544,-0.009864807,0.048736572,-0.0060920715,0.0030460358,-0.02798462,-0.03842163,0.035339355,0.036376953,0.029785156,0.019866943,-0.020889282,0.011413574,-0.0030136108,0.031341553,0.04977417,0.015281677,0.020111084,-0.01625061,0.046691895,0.036895752,-0.00056409836,0.050048828,-0.024505615,0.0020961761,-0.042053223,0.024627686,-0.0041275024,-0.0079956055,0.0034980774,-0.026565552,0.041534424,-0.026947021,-0.018310547,-0.018188477,0.023468018,-0.040740967,0.030303955,-0.009284973,0.014183044,0.019866943,-0.03869629,0.059570312,-0.0021762848,0.039978027,0.014831543,0.034057617,0.03842163,-0.02720642,-0.0009589195,0.039978027,-0.024246216,0.046691895,0.02720642,0.028762817,-0.035064697,0.005092621,-0.018447876,-0.040496826,0.031463623,0.003610611,0.022827148,0.0016527176,0.010383606,-0.0026111603,-0.012252808,0.035339355,0.020248413,-0.015411377,0.034820557,-0.02218628,0.05493164,-0.009735107,0.057006836,-0.017410278,0.058563232,0.0044174194,0.0034179688,-0.026824951,-0.0107040405,0.00844574,0.018692017,-0.013542175,0.041778564,0.051574707,0.015151978,-0.0552063,-0.009803772,-0.031585693,0.0027236938,0.015991211,-0.0068359375,-0.034057617,0.046936035,0.000995636,0.02708435,0.010124207,0.06188965,0.017532349,-0.011604309,-0.017288208,-0.0007901192]", "client_status_at": "2025-10-02T09:49:03.591180Z", "server_status_at": "2025-10-02T09:49:03.591149Z", "created_at": "2025-10-02T08:49:00.219830Z"}], "item_chunks": {"d1e2d227-0aa1-42c3-8d06-83aa8d73da37": [{"position": 0, "content_text": "We've worked with dozens of teams building LLM agents across industries. Consistently, the most successful implementations use simple, composable patterns rather than complex frameworks. Over the past year, we've worked with dozens of teams building large language model (LLM) agents across industries. Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns. In this post, we share what we’ve learned from working with our customers and building agents ourselves, and give practical advice for developers on building effective agents. What are agents? \"Agent\" can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as agentic systems, but draw an important architectural distinction between workflows and agents:\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths. Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks. Below, we will explore both types of agentic systems in detail. In Appendix 1 (“Agents in Practice”), we describe two domains where customers have found particular value in using these kinds of systems. When (and when not) to use agents\nWhen building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense. When more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough. When and how to use frameworks\nThere are many frameworks that make agentic systems easier to implement, including:\nRivet, a drag and drop GUI LLM workflow builder; and\nVellum, another GUI tool for building and testing complex workflows. These frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together.", "content_token_count": 383, "mistral_embedding": null}, {"position": 1, "content_text": "When more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough. When and how to use frameworks\nThere are many frameworks that make agentic systems easier to implement, including:\nRivet, a drag and drop GUI LLM workflow builder; and\nVellum, another GUI tool for building and testing complex workflows. These frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice. We suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error. In this section, we’ll explore the common patterns for agentic systems we’ve seen in production. We'll start with our foundational building block—the augmented LLM—and progressively increase complexity, from simple compositional workflows to autonomous agents. Building block: The augmented LLM\nThe basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilities—generating their own search queries, selecting appropriate tools, and determining what information to retain. We recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently released Model Context Protocol, which allows developers to integrate with a growing ecosystem of third-party tools with a simple client implementation. For the remainder of this post, we'll assume each LLM call has access to these augmented capabilities. Workflow: Prompt chaining\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.", "content_token_count": 398, "mistral_embedding": null}, {"position": 2, "content_text": "While there are many ways to implement these augmentations, one approach is through our recently released Model Context Protocol, which allows developers to integrate with a growing ecosystem of third-party tools with a simple client implementation. For the remainder of this post, we'll assume each LLM call has access to these augmented capabilities. Workflow: Prompt chaining\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track. When to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task. Examples where prompt chaining is useful:\nGenerating Marketing copy, then translating it into a different language. Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline. Workflow: Routing\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs. When to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm. Examples where routing is useful:\nDirecting different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools. Routing easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to optimize cost and speed. Workflow: Parallelization\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations:\nSectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs. When to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results.", "content_token_count": 384, "mistral_embedding": null}, {"position": 3, "content_text": "Routing easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to optimize cost and speed. Workflow: Parallelization\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations:\nSectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs. When to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect. Examples where parallelization is useful:\nSectioning:\nImplementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response. Automating evals for evaluating LLM performance, where each LLM call evaluates a different aspect of the model’s performance on a given prompt. Voting:\nReviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem. Evaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives. Workflow: Orchestrator-workers\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results. When to use this workflow: This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren't pre-defined, but determined by the orchestrator based on the specific input. Example where orchestrator-workers is useful:\nCoding products that make complex changes to multiple files each time. Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information. Workflow: Evaluator-optimizer\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.", "content_token_count": 383, "mistral_embedding": null}, {"position": 4, "content_text": "When to use this workflow: This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren't pre-defined, but determined by the orchestrator based on the specific input. Example where orchestrator-workers is useful:\nCoding products that make complex changes to multiple files each time. Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information. Workflow: Evaluator-optimizer\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop. When to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document. Examples where evaluator-optimizer is useful:\nLiterary translation where there are nuances that the translator LLM might not capture initially, but where an evaluator LLM can provide useful critiques. Complex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted. Agents\nAgents are emerging in production as LLMs mature in key capabilities—understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain “ground truth” from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it’s also common to include stopping conditions (such as a maximum number of iterations) to maintain control. Agents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop.", "content_token_count": 399, "mistral_embedding": null}, {"position": 5, "content_text": "During execution, it's crucial for the agents to gain “ground truth” from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it’s also common to include stopping conditions (such as a maximum number of iterations) to maintain control. Agents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 (\"Prompt Engineering your Tools\"). When to use agents: Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments. The autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails. Examples where agents are useful:\nThe following examples are from our own implementations:\nA coding Agent to resolve SWE-bench tasks, which involve edits to many files based on a task description;\nThese building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity only when it demonstrably improves outcomes. Summary\nSuccess in the LLM space isn't about building the most sophisticated system. It's about building the right system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short. When implementing agents, we try to follow three core principles:\nMaintain simplicity in your agent's design. Prioritize transparency by explicitly showing the agent’s planning steps. Carefully craft your agent-computer interface (ACI) through thorough tool documentation and testing. Frameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production.", "content_token_count": 388, "mistral_embedding": null}, {"position": 6, "content_text": "Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short. When implementing agents, we try to follow three core principles:\nMaintain simplicity in your agent's design. Prioritize transparency by explicitly showing the agent’s planning steps. Carefully craft your agent-computer interface (ACI) through thorough tool documentation and testing. Frameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production. By following these principles, you can create agents that are not only powerful but also reliable, maintainable, and trusted by their users. Acknowledgements\nWritten by Erik Schluntz and Barry Zhang. This work draws upon our experiences building agents at Anthropic and the valuable insights shared by our customers, for which we're deeply grateful. Appendix 1: Agents in practice\nOur work with customers has revealed two particularly promising applications for AI agents that demonstrate the practical value of the patterns discussed above. Both applications illustrate how agents add the most value for tasks that require both conversation and action, have clear success criteria, enable feedback loops, and integrate meaningful human oversight. A. Customer support\nCustomer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:\nSupport interactions naturally follow a conversation flow while requiring access to external information and actions;\nTools can be integrated to pull customer data, order history, and knowledge base articles;\nActions such as issuing refunds or updating tickets can be handled programmatically; and\nSuccess can be clearly measured through user-defined resolutions. Several companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness. B. Coding agents\nThe software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:\nCode solutions are verifiable through automated tests;\nAgents can iterate on solutions using test results as feedback;\nThe problem space is well-defined and structured; and\nOutput quality can be measured objectively. In our own implementation, agents can now solve real GitHub issues in the SWE-bench Verified benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.", "content_token_count": 392, "mistral_embedding": null}, {"position": 7, "content_text": "Coding agents\nThe software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:\nCode solutions are verifiable through automated tests;\nAgents can iterate on solutions using test results as feedback;\nThe problem space is well-defined and structured; and\nOutput quality can be measured objectively. In our own implementation, agents can now solve real GitHub issues in the SWE-bench Verified benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements. Appendix 2: Prompt engineering your tools\nNo matter which agentic system you're building, tools will likely be an important part of your agent. Tools enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a tool use block in the API response if it plans to invoke a tool. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. In this brief appendix, we describe how to prompt engineer your tools. There are often several ways to specify the same action. For instance, you can specify a file edit by writing a diff, or by rewriting the entire file. For structured output, you can return code inside markdown or inside JSON. In software engineering, differences like these are cosmetic and can be converted losslessly from one to the other. However, some formats are much more difficult for an LLM to write than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes. Our suggestions for deciding on tool formats are the following:\nGive the model enough tokens to \"think\" before it writes itself into a corner. Keep the format close to what the model has seen naturally occurring in text on the internet. Make sure there's no formatting \"overhead\" such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes. One rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good agent-computer interfaces (ACI).", "content_token_count": 396, "mistral_embedding": null}, {"position": 8, "content_text": "Our suggestions for deciding on tool formats are the following:\nGive the model enough tokens to \"think\" before it writes itself into a corner. Keep the format close to what the model has seen naturally occurring in text on the internet. Make sure there's no formatting \"overhead\" such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes. One rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good agent-computer interfaces (ACI). Here are some thoughts on how to do so:\nPut yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it’s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other tools. How can you change parameter names or descriptions to make things more obvious? Think of this as writing a great docstring for a junior developer on your team. This is especially important when using many similar tools. Test how the model uses your tools: Run many example inputs in our workbench to see what mistakes the model makes, and iterate. Poka-yoke your tools. Change the arguments so that it is harder to make mistakes. While building our agent for SWE-bench, we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths—and we found that the model used this method flawlessly. Get the developer newsletter\nProduct updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.", "content_token_count": 318, "mistral_embedding": null}, {"position": 9, "content_text": "While building our agent for SWE-bench, we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths—and we found that the model used this method flawlessly. Get the developer newsletter\nProduct updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.", "content_token_count": 80, "mistral_embedding": null}, {"position": 10, "content_text": "For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths—and we found that the model used this method flawlessly. Get the developer newsletter\nProduct updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.", "content_token_count": 62, "mistral_embedding": null}, {"position": 11, "content_text": "To fix this, we changed the tool to always require absolute filepaths—and we found that the model used this method flawlessly. Get the developer newsletter\nProduct updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.", "content_token_count": 37, "mistral_embedding": null}, {"position": 12, "content_text": "Get the developer newsletter\nProduct updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.", "content_token_count": 16, "mistral_embedding": null}, {"position": 13, "content_text": "Delivered monthly to your inbox.", "content_token_count": 5, "mistral_embedding": null}], "cb3a0131-2904-4b4e-abaf-459b86cb70c7": [{"position": 0, "content_text": "Tadej Pogačar turned in yet another performance for the ages at the elite men's road race at the UCI Road World Championships on Sunday, securing his second gold in the category in as many years with a trademark solo long-distance breakaway in Kigali, Rwanda. Having first opened up the throttle on the dauntingly difficult Mont Kigali and caused the field to shatter behind him, the Slovenian then dropped the last rider to stay on his wheel, Isaac del Toro (Mexico), with 66 kilometres left and headed away for another epic lone win. Slovenia's Pogačar finally crossed the finish line with over a minute's advantage over Remco Evenepoel (Belgium), who staged a remarkable comeback following numerous mechanical difficulties to claim silver. Ben Healy (Ireland) dropped Mattias Skjelmose (Denmark) on the final climb to take the bronze medal. But if Evenepoel's ability to turn misfortune into an honourable podium position could not be ignored, neither could Pogačar's latest stunning success, following up a fourth Tour de France title this summer with a second World Championships victory. His latest triumph, too, further confirmed his hegemony as the world's number one, and - coming on a course widely rated as the most difficult World Championships course ever - propelled Pogačar even closer to the unofficial title of cycling's all-time greatest in history. Asked about why he had launched his first attack so early, with 105 kilometres to go, Pogačar answered, \"I think the course was designed for this, but I was hoping that a small group would form, like we did with Juan [Ayuso, Spain] and [Isaac] Del Toro [Mexico]. It was a perfect combo'.\"\nAfter breaking away with two trade teammates from UAE Team Emirates, Pogačar said, \"I was like - this is a dream, no, to ride together as far as possible, as a trio, but Juan had a problem like soon on the Mur and Del Toro had some stomach problems in the race.\"\nThe latest race content, interviews, features, reviews and expert buying guides, direct to your inbox! \"So I was left alone quite early, and I was solo, fighting by myself, but I'm so happy I made it.\"\nAs for whether he had moments of doubt in what looked, from the outside, to be such a convincing performance, Pogačar said, \"For sure.", "content_token_count": 383, "mistral_embedding": null}, {"position": 1, "content_text": "It was a perfect combo'.\"\nAfter breaking away with two trade teammates from UAE Team Emirates, Pogačar said, \"I was like - this is a dream, no, to ride together as far as possible, as a trio, but Juan had a problem like soon on the Mur and Del Toro had some stomach problems in the race.\"\nThe latest race content, interviews, features, reviews and expert buying guides, direct to your inbox! \"So I was left alone quite early, and I was solo, fighting by myself, but I'm so happy I made it.\"\nAs for whether he had moments of doubt in what looked, from the outside, to be such a convincing performance, Pogačar said, \"For sure. Because the climbs were getting harder and harder on every lap, but on the downhills, I had to pedal a lot too. Towards the end, the energy resources were going.\"\n\"It was so hard, the last final laps. Of course, you doubt, but you have to push through and hope for the best.\"\nVictorious for the second year running in Africa's first-ever Road World Championships, Pogačar rounded off by saying - with some considerable understatement - \"It was an incredible experience altogether. Let's say it was a successful week.\"\nHow it unfolded\nThe opening move of the day came after just 500 metres courtesy of Red Walters, Grenada's sole representative in the 92nd edition of the World Championships Road Race. But in a high-octane start, Walters was quickly succeeded by a much more numerous move containing Andrea Foldager (Denmark), Menno Huising (Netherlands), Ivo Oliveira (Portugal), Fabio Christen (Switzerland), and Marius Mayrhofer (Germany). The six were then joined by Julien Bernard (France) as they went through the finish for the first of 16 laps on the toughest Worlds circuit - 5475 metres of elevation gain - since the Nurburgring's 5,844 metres of climbing in Germany, way back in 1966. While the very fast start to the elite men's race proved too much for former double World Champion Julien Alaphilippe (France), who abandoned, ill, a move by Raúl García Pierna (Spain) to bridge across to the six leaders on the opening 10 laps of the Kigali city centre circuit sparked a strong response by numerous nations.", "content_token_count": 371, "mistral_embedding": null}, {"position": 2, "content_text": "The six were then joined by Julien Bernard (France) as they went through the finish for the first of 16 laps on the toughest Worlds circuit - 5475 metres of elevation gain - since the Nurburgring's 5,844 metres of climbing in Germany, way back in 1966. While the very fast start to the elite men's race proved too much for former double World Champion Julien Alaphilippe (France), who abandoned, ill, a move by Raúl García Pierna (Spain) to bridge across to the six leaders on the opening 10 laps of the Kigali city centre circuit sparked a strong response by numerous nations. Slovenia and Belgium, both fielding major favourites, kept things firmly under control, the latter despite the early loss of outside contender Ilan van Wilder to a crash, whilst the USA were also sticking close to the front. After 100 kilometres of steadily hard riding, the seven had an advantage of just 2:33, a very small gap for an early break in the Worlds. That scant margin for hope drained away quickly, too, as the peloton headed out of the city at a relentlessly high pace for the race's single ascents of the deceptively difficult Mont Kigali and the ultra-steep Mur de Kigali, immediately afterwards. Greeted by enormous crowds, seemingly even bigger than on the city centre finishing circuit, the three strongest riders from the break - Foldager, Oliveira and Bernard - hit the Mont Kigali with barely two minutes' advantage. To judge by the ferocious fight in a pack already reduced to just 80 riders to reach the foot of the Kigali as well-placed as possible, it was clear that nobody wanted to lose out on the hardest climb of the entire race. But that said, given the way Pogačar promptly proceeded to leave the field for dust at its summit, perhaps they'd have been wiser to save their energy. As Bernard pressed home a fast-dimishing advantage on the upper slopes of the Mont Kigali, both Oliveira and Foldager were dropped, while Slovenia still kept a firm control on the pack, with Pogačar preceding a seemingly ever-vigilant Evenepoel and Juan Ayuso (Spain). But then, very close to the summit, the first key moment of the race suddenly materialised as Pogačar notably raised the pace, whilst Evenepoel, despite being so close behind, abruptly became one of the biggest victims of his searing acceleration.", "content_token_count": 394, "mistral_embedding": null}, {"position": 3, "content_text": "As Bernard pressed home a fast-dimishing advantage on the upper slopes of the Mont Kigali, both Oliveira and Foldager were dropped, while Slovenia still kept a firm control on the pack, with Pogačar preceding a seemingly ever-vigilant Evenepoel and Juan Ayuso (Spain). But then, very close to the summit, the first key moment of the race suddenly materialised as Pogačar notably raised the pace, whilst Evenepoel, despite being so close behind, abruptly became one of the biggest victims of his searing acceleration. Then when the Slovenian closed in on the break over the top of the hill and powered past Bernard, only Ayuso could follow. With 104 kilometres to go, such a long-distance show of strength looked like a mirror move to Zurich's Road World Championships 12 months earlier, albeit with the difference that this time the Slovenian was not (yet) alone. Furthermore, after blasting down the far side of the Mont Kigali at speed, Isaac Del Toro (Mexico) bridged across, briefly making it a trio of UAE riders ahead. The next big development in what proved to be the winning move of the race came when Del Toro launched an emphatic challenge on the lungburstingly steep Mur de Kigali, testing Pogačar and pushing Ayuso out of the running. Pogačar was only a couple of bike lengths back, though, at the top, and as he rejoined Ayuso and the two headed back to the final six laps in Kigali city centre together, the joint commitment of the Mexican and the defending World Champion to their long-distance bid for glory was clear. Still, with a first chase group at 40 seconds containing rivals as important as Ben Healy (Ireland), Carapaz, Ayuso and Jai Hindley (Australia), not to mention Evenepoel pushing to close the gap from a little further back, all was not lost for the opposition. But by the time Pogačar and Del Toro went under the finish line for the final six laps to go, their advantage over Sivakov and Healy had risen to 50 seconds, and the organisation in a chase group behind of just 30 riders was not yet good enough to try and reverse the gap. Last but not least, the presence of Pogačar's teammate Primož Roglič, cheerfully chasing down counter-attacks to ensure Pogačar gained as much of a margin as possible, can hardly have been encouraging, either.", "content_token_count": 391, "mistral_embedding": null}, {"position": 4, "content_text": "But by the time Pogačar and Del Toro went under the finish line for the final six laps to go, their advantage over Sivakov and Healy had risen to 50 seconds, and the organisation in a chase group behind of just 30 riders was not yet good enough to try and reverse the gap. Last but not least, the presence of Pogačar's teammate Primož Roglič, cheerfully chasing down counter-attacks to ensure Pogačar gained as much of a margin as possible, can hardly have been encouraging, either. Another of multiple setbacks for Evenepoel followed, too, when after hitting a pothole on the Mont Kigali that put his bike out of synch, he had no choice but to stop for a bike change. Belgium initially battled on, though, with Evenepoel gamely following teammate Quinten Hermans up the Côte de Kigali, even as the number of significant abandons - Michael Matthews (Australia), Egan Bernal (Colombia) and Fred Wright (Great Britain) amongst them - began to multiply. However, Evenepoel's predicament worsened sharply again as he was delayed in a much more significant fashion when needing a second bike change, forcing him to spend the rest of the race battling to regain any chance of contention. The sense that events were running out of control for Evenepoel increased even further when the first cohesive counter-attack finally formed behind Pogačar, courtesy of Sivakov, Healy and Mikkel Honoré (Denmark), but lacking the Belgian, still weaving through the team cars and jumping from group to group in a furious game of catch-up. Then just as the Belgian was on the point of joining the Healy group, yet more bad news filtered through for his chances: far from losing steam as Evenepoel surely would have wanted, ahead Pogačar had dropped Del Toro and gone for it alone. The definitive turning point of the battle for gold took place at 66.6 kilometres to go, on the Côte de Kigali, not rated as being as hard as the cobbled Côte de Kimihurara, but for a rider with Pogačar's condition and strength, you'd suspect that was largely irrelevant. Indeed, there was seemingly no exceptional acceleration by Pogačar as he moved into solo-break mode, just a powerful drive that Del Toro, shadowing him as best he could, proved unable to follow.", "content_token_count": 378, "mistral_embedding": null}, {"position": 5, "content_text": "The definitive turning point of the battle for gold took place at 66.6 kilometres to go, on the Côte de Kigali, not rated as being as hard as the cobbled Côte de Kimihurara, but for a rider with Pogačar's condition and strength, you'd suspect that was largely irrelevant. Indeed, there was seemingly no exceptional acceleration by Pogačar as he moved into solo-break mode, just a powerful drive that Del Toro, shadowing him as best he could, proved unable to follow. Once again, we were back in the same scenario as Zurich, and if Pogačar later said this latest long-distance move was unplanned, the end result was destined to be exactly the same. It was true that as Pogačar was fulfilling the script predicted by so many to perfection, grinding steadily up the daunting Kigali slopes without any seeming effort, behind Evenepoel was able to stage a remarkable comeback. Having pushed his way through numerous chasing groups, the Belgian had eventually ended up with the closest pursuers behind the Slovenian, alongside Mattias Skjelmose (Denmark) and Healy. But even if Evenepoel's determination, making a massive contribution to the break and constantly cajoling the other two to push harder, was admirable and ended up with the trio distancing lone chaser Tom Pidcock (Great Britain) for good, their chances of catching a Pogačar in blazingly good form never looked more than minimal, either. Of course, the key question remained if Pogačar's latest long-distance charge was too ambitious, even for him. As his margin remained stubbornly at around a minute for nearly 20 kilometres, the memory of how he struggled slightly in the finale of Zurich last September was inescapable, not to mention his sudden dip in strength at Amstel Gold this spring - with Skjelmose, present alongside Healy and Evenepoel, the man who defeated him in April too. Yet as the Slovenian team car was finally let through the convoy and up to Pogačar with around 40 kilometres to go, it was lost on nobody watching that his gap was not coming down, either. As the trio of chasers headed towards the bell lap, basically set on going for a podium placing given Pogačar's evident superiority, Evenepoel darted clear, now moving into his favoured time trial mode as he went for it alone.", "content_token_count": 379, "mistral_embedding": null}, {"position": 6, "content_text": "As his margin remained stubbornly at around a minute for nearly 20 kilometres, the memory of how he struggled slightly in the finale of Zurich last September was inescapable, not to mention his sudden dip in strength at Amstel Gold this spring - with Skjelmose, present alongside Healy and Evenepoel, the man who defeated him in April too. Yet as the Slovenian team car was finally let through the convoy and up to Pogačar with around 40 kilometres to go, it was lost on nobody watching that his gap was not coming down, either. As the trio of chasers headed towards the bell lap, basically set on going for a podium placing given Pogačar's evident superiority, Evenepoel darted clear, now moving into his favoured time trial mode as he went for it alone. It had been a nightmare scenario for the Belgian in the first two-thirds of the race, yet his ability to return to the fray and push as hard as he could in the finale could only be admired. The biggest plaudits, though, had to be reserved for Pogačar, grimacing slightly with the effort as he blasted through the last lap, but with the prospects of a gold medal surely easing the pain a little. Visibly putting everything into it, the defending World Champion might well have had the memory of being overtaken by Evenepoel in the elite men's time trial the last week in mind as he ramped up the pace towards a certain victory for no apparent reason. But whatever the motivation for that last push to gain more time, his gap was once again increasing with every turn of the pedals. Behind, Healy managed to follow Evenepoel up the road and drop Skjelmose on the final ascent of the Côte de Kigali. But by this point, though, Pogačar and his team car were already all but celebrating the win. The final ascent of the Côte de Kimihurara, so decisive in the other Worlds' races this week, was tackled by Pogačar with no indication of last-minute weakening. If the last six laps were effectively laps of honour for Pogačar after such a dominant ride, the last time up, he could continue to relax and even smile a little.", "content_token_count": 372, "mistral_embedding": null}, {"position": 7, "content_text": "Behind, Healy managed to follow Evenepoel up the road and drop Skjelmose on the final ascent of the Côte de Kigali. But by this point, though, Pogačar and his team car were already all but celebrating the win. The final ascent of the Côte de Kimihurara, so decisive in the other Worlds' races this week, was tackled by Pogačar with no indication of last-minute weakening. If the last six laps were effectively laps of honour for Pogačar after such a dominant ride, the last time up, he could continue to relax and even smile a little. Standing out of the saddle on the last little rise to the finish, Pogačar celebrated his second straight rainbow title with his arms spread wide and a subsequent exchange of high-fives with his support staff. The contrast with silver medallist Evenepoel, after he reached the finish 1:28 later, slumped against the barriers for long minutes afterwards and left wondering what might have been without those bike changes and his bad luck, could not have been clearer. While Healy was delighted with Ireland's first road race medal since Sean Kelly's bronze back in 1989, the truth was that merely reaching the line on such a tough course with just 30 finishers was a major achievement in itself. As for Pogačar, the Slovenian could celebrate becoming the eighth rider in history to take back-to-back Worlds wins - and yet another massive milestone in his career. Results\nResults powered by FirstCycling\nAlasdair Fotheringham has been reporting on cycling since 1991. He has covered every Tour de France since 1992 bar one, as well as numerous other bike races of all shapes and sizes, ranging from the Olympic Games in 2008 to the now sadly defunct Subida a Urkiola hill climb in Spain. As well as working for Cyclingnews, he has also written for The Independent, The Guardian, ProCycling, The Express and Reuters. You must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name.", "content_token_count": 340, "mistral_embedding": null}, {"position": 8, "content_text": "He has covered every Tour de France since 1992 bar one, as well as numerous other bike races of all shapes and sizes, ranging from the Olympic Games in 2008 to the now sadly defunct Subida a Urkiola hill climb in Spain. As well as working for Cyclingnews, he has also written for The Independent, The Guardian, ProCycling, The Express and Reuters. You must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name. Latest on Cyclingnews\n-\n'The market is both flooded and in limbo' – Uncertainty around Intermarché-Lotto merger, Arkéa-B&B Hotels' future mucking up transfer works\nOver 400 riders and staff still waiting for answers as end of season nears -\n'Still fighting for the win' – Big goals, a near miss, shifting goal posts and hopes for a victory celebration as Alexander Kristoff bids farewell at Le Tour de Langkawi\nNorwegian to end his career after the Malaysian race -\n'In the reverse of Christina Aguilera, our hearts were saying yes, but our bodies saying no' - Larry Warbasse reflects on savage racing and scenic safaris in Kigali\nThe US-team rider writes about his unique experience at the World Championships for Cyclingnews", "content_token_count": 208, "mistral_embedding": null}, {"position": 9, "content_text": "Latest on Cyclingnews\n-\n'The market is both flooded and in limbo' – Uncertainty around Intermarché-Lotto merger, Arkéa-B&B Hotels' future mucking up transfer works\nOver 400 riders and staff still waiting for answers as end of season nears -\n'Still fighting for the win' – Big goals, a near miss, shifting goal posts and hopes for a victory celebration as Alexander Kristoff bids farewell at Le Tour de Langkawi\nNorwegian to end his career after the Malaysian race -\n'In the reverse of Christina Aguilera, our hearts were saying yes, but our bodies saying no' - Larry Warbasse reflects on savage racing and scenic safaris in Kigali\nThe US-team rider writes about his unique experience at the World Championships for Cyclingnews", "content_token_count": 121, "mistral_embedding": null}], "9bc1ced6-828c-4705-a479-170757a5a2aa": [{"position": 0, "content_text": "Introduction\nKMeans is an unsupervised clustering algorithm that groups data based on distances. It is widely recognized for its simplicity and effectiveness as a clustering algorithm. Essentially, the core idea is to partition a dataset into distinct clusters, with each point belonging to the cluster whose centroid is closest to it. While its simplicity often makes it the most preferred clustering algorithm, KMeans has many limitations that hinder its effectiveness in many scenarios. The shortcomings of KMeans Clustering\nKMeans comes with its own set of limitations that can restrict its performance in certain situations.\n#1) KMeans does not account for cluster variance\nOne of the primary limitations is its assumption of spherical clusters. One intuitive and graphical way to understand the KMeans algorithm is to place a circle at the center of each cluster, which encloses the points. As KMeans is all about placing circles, its results aren’t ideal when the dataset has irregular shapes or varying sizes, as shown below:\nInstead, an ideal clustering should cluster the data as follows:\nThis rigidity of KMeans to only cluster globular clusters often leads to misclassification and suboptimal cluster assignments.\n#2) KMeans only relies on distance\nThis limitation is somewhat connected to the one we discussed above. Imagine we have the following dataset:\nClearly, the blue cluster has a larger spread. Therefore, ideally, its influence should be larger as well. However, when assigning a new data point to a cluster, KMeans only considers the distance to the centroid. This means that it creates a margin for assigning a new data point to a cluster that is equidistant from both centroids. But considering the area of influence of the right cluster, having this margin more to the left makes more sense.\n#3) KMeans does not output probabilities\nKMeans clustering performs hard assignments. In simple words, this means that a specific data point can belong to only one cluster. Thus, it does not provide probabilistic estimates of a given data point belonging to each possible cluster. Although this might be a problem per se, it limits its usefulness in uncertainty estimation and downstream applications that require a probabilistic interpretation. These limitations often make KMeans a non-ideal choice for clustering. Therefore, learning about other better algorithms that we can use to address these limitations is extremely important. Therefore, in this article, we will learn about Gaussian mixture models.", "content_token_count": 393, "mistral_embedding": null}, {"position": 1, "content_text": "In simple words, this means that a specific data point can belong to only one cluster. Thus, it does not provide probabilistic estimates of a given data point belonging to each possible cluster. Although this might be a problem per se, it limits its usefulness in uncertainty estimation and downstream applications that require a probabilistic interpretation. These limitations often make KMeans a non-ideal choice for clustering. Therefore, learning about other better algorithms that we can use to address these limitations is extremely important. Therefore, in this article, we will learn about Gaussian mixture models. More specifically, we shall cover:\n- Shortcomings of KMeans (already covered above)\n- What is the motivation behind GMMs?\n- How do GMMs work?\n- The intuition behind GMMs.\n- Plotting some dummy multivariate Gaussian distributions to better understand GMMs.\n- The entire mathematical formulation of GMMs.\n- How to use Expectation-Maximization to model data using GMMs?\n- Coding a GMM from scratch (without sklearn).\n- Comparing results of GMMs with KMeans.\n- How to determine the optimal number of clusters for GMMs?\n- Some practical use cases of GMMs.\n- Takeaways. Let's begin! Gaussian Mixture Models\nAs the name suggests, a Gaussian mixture model clusters a dataset that has a mixture of multiple Gaussian distributions. They can be thought of as a generalized twin of KMeans. Simply put, in 2 dimensions, while KMeans only create circular clusters, gaussian mixture models can create oval-shaped clusters. But how do they do that? Let's understand in more detail.", "content_token_count": 250, "mistral_embedding": null}, {"position": 2, "content_text": "More specifically, we shall cover:\n- Shortcomings of KMeans (already covered above)\n- What is the motivation behind GMMs?\n- How do GMMs work?\n- The intuition behind GMMs.\n- Plotting some dummy multivariate Gaussian distributions to better understand GMMs.\n- The entire mathematical formulation of GMMs.\n- How to use Expectation-Maximization to model data using GMMs?\n- Coding a GMM from scratch (without sklearn).\n- Comparing results of GMMs with KMeans.\n- How to determine the optimal number of clusters for GMMs?\n- Some practical use cases of GMMs.\n- Takeaways. Let's begin! Gaussian Mixture Models\nAs the name suggests, a Gaussian mixture model clusters a dataset that has a mixture of multiple Gaussian distributions. They can be thought of as a generalized twin of KMeans. Simply put, in 2 dimensions, while KMeans only create circular clusters, gaussian mixture models can create oval-shaped clusters. But how do they do that? Let's understand in more detail.", "content_token_count": 156, "mistral_embedding": null}, {"position": 3, "content_text": "Let's begin! Gaussian Mixture Models\nAs the name suggests, a Gaussian mixture model clusters a dataset that has a mixture of multiple Gaussian distributions. They can be thought of as a generalized twin of KMeans. Simply put, in 2 dimensions, while KMeans only create circular clusters, gaussian mixture models can create oval-shaped clusters. But how do they do that? Let's understand in more detail.", "content_token_count": 64, "mistral_embedding": null}, {"position": 4, "content_text": "Gaussian Mixture Models\nAs the name suggests, a Gaussian mixture model clusters a dataset that has a mixture of multiple Gaussian distributions. They can be thought of as a generalized twin of KMeans. Simply put, in 2 dimensions, while KMeans only create circular clusters, gaussian mixture models can create oval-shaped clusters. But how do they do that? Let's understand in more detail.", "content_token_count": 62, "mistral_embedding": null}, {"position": 5, "content_text": "They can be thought of as a generalized twin of KMeans. Simply put, in 2 dimensions, while KMeans only create circular clusters, gaussian mixture models can create oval-shaped clusters. But how do they do that? Let's understand in more detail.", "content_token_count": 40, "mistral_embedding": null}, {"position": 6, "content_text": "Simply put, in 2 dimensions, while KMeans only create circular clusters, gaussian mixture models can create oval-shaped clusters. But how do they do that? Let's understand in more detail.", "content_token_count": 29, "mistral_embedding": null}, {"position": 7, "content_text": "But how do they do that? Let's understand in more detail.", "content_token_count": 11, "mistral_embedding": null}, {"position": 8, "content_text": "Let's understand in more detail.", "content_token_count": 5, "mistral_embedding": null}], "d86fbd20-fd8a-4588-bfd3-0f10e1fc6281": [{"position": 0, "content_text": "Google DeepMind says its upgraded AI models enable robots to complete more complex tasks — and even tap into the web for help. During a press briefing, Google DeepMind’s head of robotics, Carolina Parada, told reporters that the company’s new AI models work in tandem to allow robots to “think multiple steps ahead” before taking action in the physical world. Google DeepMind’s new AI models can search the web to help robots complete tasks\nThe new Gemini Robotics 1.5 models enable robots to carry out multistep tasks and even learn from each other. The system is powered by the newly launched Gemini Robotics 1.5 alongside the embodied reasoning model, Gemini Robotics-ER 1.5, which are updates to AI models that Google DeepMind introduced in March. Now robots can perform more than just singular tasks, such as folding a piece of paper or unzipping a bag. They can now do things like separate laundry by dark and light colors, pack a suitcase based on the current weather in London, as well as help someone sort trash, compost, and recyclables based on a web search tailored to a location’s specific requirements.\n“The models up to now were able to do really well at doing one instruction at a time in a way that is very general,” Parada said. “With this update, we’re now moving from one instruction to actually genuine understanding and problem-solving for physical tasks.”\nTo do this, robots can use the upgraded Gemini Robotics-ER 1.5 model to form an understanding of their surroundings, and use digital tools like Google Search to find more information. Gemini Robotics-ER 1.5 then translates those findings into natural language instructions for Gemini Robotics 1.5, allowing the robot to use the model’s vision and language understanding to carry out each step. Additionally, Google DeepMind announced that Gemini Robotics 1.5 can help robots “learn” from each other, even if they have different configurations.", "content_token_count": 315, "mistral_embedding": null}, {"position": 1, "content_text": "They can now do things like separate laundry by dark and light colors, pack a suitcase based on the current weather in London, as well as help someone sort trash, compost, and recyclables based on a web search tailored to a location’s specific requirements.\n“The models up to now were able to do really well at doing one instruction at a time in a way that is very general,” Parada said. “With this update, we’re now moving from one instruction to actually genuine understanding and problem-solving for physical tasks.”\nTo do this, robots can use the upgraded Gemini Robotics-ER 1.5 model to form an understanding of their surroundings, and use digital tools like Google Search to find more information. Gemini Robotics-ER 1.5 then translates those findings into natural language instructions for Gemini Robotics 1.5, allowing the robot to use the model’s vision and language understanding to carry out each step. Additionally, Google DeepMind announced that Gemini Robotics 1.5 can help robots “learn” from each other, even if they have different configurations. Google DeepMind found that tasks presented to the ALOHA2 robot, which consists of two mechanical arms, “just work” on the bi-arm Franka robot, as well as Apptronik’s humanoid robot Apollo. “This enables two things for us: one is to control very different robots — including a humanoid — with a single model,” Google DeepMind software engineer Kanishka Rao said during the briefing. “And secondly, skills that are learned on one robot can now be transferred to another robot.”\nAs part of the update, Google DeepMind is rolling out Gemini Robotics-ER 1.5 to developers through the Gemini API in Google AI Studio, while only select partners can access Gemini Robotics 1.5. Most Popular\n- Microsoft revamps Xbox Game Pass plans and hikes Ultimate to $29.99 a month\n- Satya Nadella appoints a new CEO to run Microsoft’s biggest businesses\n- Google is blocking AI searches for Trump and dementia\n- Behold: The Lego Game Boy has already been modded to play games for real\n- Google’s Home Speaker looks and sounds better — but you can’t get it yet", "content_token_count": 349, "mistral_embedding": null}, {"position": 2, "content_text": "Google DeepMind found that tasks presented to the ALOHA2 robot, which consists of two mechanical arms, “just work” on the bi-arm Franka robot, as well as Apptronik’s humanoid robot Apollo. “This enables two things for us: one is to control very different robots — including a humanoid — with a single model,” Google DeepMind software engineer Kanishka Rao said during the briefing. “And secondly, skills that are learned on one robot can now be transferred to another robot.”\nAs part of the update, Google DeepMind is rolling out Gemini Robotics-ER 1.5 to developers through the Gemini API in Google AI Studio, while only select partners can access Gemini Robotics 1.5. Most Popular\n- Microsoft revamps Xbox Game Pass plans and hikes Ultimate to $29.99 a month\n- Satya Nadella appoints a new CEO to run Microsoft’s biggest businesses\n- Google is blocking AI searches for Trump and dementia\n- Behold: The Lego Game Boy has already been modded to play games for real\n- Google’s Home Speaker looks and sounds better — but you can’t get it yet", "content_token_count": 178, "mistral_embedding": null}, {"position": 3, "content_text": "Most Popular\n- Microsoft revamps Xbox Game Pass plans and hikes Ultimate to $29.99 a month\n- Satya Nadella appoints a new CEO to run Microsoft’s biggest businesses\n- Google is blocking AI searches for Trump and dementia\n- Behold: The Lego Game Boy has already been modded to play games for real\n- Google’s Home Speaker looks and sounds better — but you can’t get it yet", "content_token_count": 68, "mistral_embedding": null}], "44368d42-55ab-43c5-af64-d88f40a36639": [{"position": 0, "content_text": "Dear friends,\nThere’s a new breed of GenAI Application Engineers who can build more-powerful applications faster than was possible before, thanks to generative AI. Individuals who can play this role are highly sought-after by businesses, but the job description is still coming into focus. Let me describe their key skills, as well as the sorts of interview questions I use to identify them. Skilled GenAI Application Engineers meet two primary criteria: (i) They are able to use the new AI building blocks to quickly build powerful applications. (ii) They are able to use AI assistance to carry out rapid engineering, building software systems in dramatically less time than was possible before. In addition, good product/design instincts are a significant bonus. AI building blocks. If you own a lot of copies of only a single type of Lego brick, you might be able to build some basic structures. But if you own many types of bricks, you can combine them rapidly to form complex, functional structures. Software frameworks, SDKs, and other such tools are like that. If all you know is how to call a large language model (LLM) API, that's a great start. But if you have a broad range of building block types — such as prompting techniques, agentic frameworks, evals, guardrails, RAG, voice stack, async programming, data extraction, embeddings/vectorDBs, model fine tuning, graphDB usage with LLMs, agentic browser/computer use, MCP, reasoning models, and so on — then you can create much richer combinations of building blocks. The number of powerful AI building blocks continues to grow rapidly. But as open-source contributors and businesses make more building blocks available, staying on top of what is available helps you keep on expanding what you can build. Even though new building blocks are created, many building blocks from 1 to 2 years ago (such as eval techniques or frameworks for using vectorDBs) are still very relevant today. AI-assisted coding. AI-assisted coding tools enable developers to be far more productive, and such tools are advancing rapidly. Github Copilot, first announced in 2021 (and made widely available in 2022), pioneered modern code autocompletion. But shortly after, a new breed of AI-enabled IDEs such as Cursor and Windsurf offered much better code-QA and code generation. As LLMs improved, these AI-assisted coding tools that were built on them improved as well.", "content_token_count": 386, "mistral_embedding": null}, {"position": 1, "content_text": "Even though new building blocks are created, many building blocks from 1 to 2 years ago (such as eval techniques or frameworks for using vectorDBs) are still very relevant today. AI-assisted coding. AI-assisted coding tools enable developers to be far more productive, and such tools are advancing rapidly. Github Copilot, first announced in 2021 (and made widely available in 2022), pioneered modern code autocompletion. But shortly after, a new breed of AI-enabled IDEs such as Cursor and Windsurf offered much better code-QA and code generation. As LLMs improved, these AI-assisted coding tools that were built on them improved as well. Now we have highly agentic coding assistants such as OpenAI’s Codex and Anthropic’s Claude Code (which I really enjoy using and find impressive in its ability to write code, test, and debug autonomously for many iterations). In the hands of skilled engineers — who don’t just “vibe code” but deeply understand AI and software architecture fundamentals and can steer a system toward a thoughtfully selected product goal — these tools make it possible to build software with unmatched speed and efficiency. I find that AI-assisted coding techniques become obsolete much faster than AI building blocks, and techniques from 1 or 2 years ago are far from today's best practices. Part of the reason for this might be that, while AI builders might use dozens (hundreds?) of different building blocks, they aren’t likely to use dozens of different coding assistance tools at once, and so the forces of Darwinian competition are stronger among tools. Given the massive investments in this space by Anthropic, Google, OpenAI, and other players, I expect the frenetic pace of development to continue, but keeping up with the latest developments in AI-assisted coding tools will pay off, since each generation is much better than the last. Bonus: Product skills. In some companies, engineers are expected to take pixel-perfect drawings of a product, specified in great detail, and write code to implement it. But if a product manager has to specify even the smallest detail, this slows down the team. The shortage of AI product managers exacerbates this problem.", "content_token_count": 351, "mistral_embedding": null}, {"position": 2, "content_text": "Given the massive investments in this space by Anthropic, Google, OpenAI, and other players, I expect the frenetic pace of development to continue, but keeping up with the latest developments in AI-assisted coding tools will pay off, since each generation is much better than the last. Bonus: Product skills. In some companies, engineers are expected to take pixel-perfect drawings of a product, specified in great detail, and write code to implement it. But if a product manager has to specify even the smallest detail, this slows down the team. The shortage of AI product managers exacerbates this problem. I see teams move much faster if GenAI Engineers also have some user empathy as well at basic skill at designing products, so that, given only high-level guidance on what to build (“a user interface that lets users see their profiles and change their passwords”), they can make a lot of decisions themselves and build at least a prototype to iterate from. When interviewing GenAI Application Engineers, I will usually ask about their mastery of AI building blocks and ability to use AI-assisted coding, and sometimes also their product/design instincts. One additional question I've found highly predictive of their skill is, “How do you keep up with the latest developments in AI?” Because AI is evolving so rapidly, someone with good strategies for keeping up — such as reading The Batch and taking short courses 😃, regular hands-on practice building projects, and having a community to talk to — really does stay ahead of the game much better than those who have less-effective strategies (such as if social media were their main source of info about AI, which typically does not provide the depth needed to keep up). Keep building! Andrew\nA MESSAGE FROM DEEPLEARNING.AI\nThe Data Analytics Professional Certificate is fully launched! In the “Data Storytelling” course, you’ll choose the right medium to present your analysis, design effective visuals, and learn techniques for aligning data insights with business goals. You’ll also receive guidance to build your portfolio and land a job in data analysis. Sign up\nNews\nMore Consistent Characters and Styles\nSame character, new background, new action. That’s the focus of the latest text-to-image models from Germany’s Black Forest Labs. What’s new: The FLUX.1 Kontext family, which comes in versions dubbed max, pro, and dev, is trained to alter images in controlled ways.", "content_token_count": 393, "mistral_embedding": null}, {"position": 3, "content_text": "In the “Data Storytelling” course, you’ll choose the right medium to present your analysis, design effective visuals, and learn techniques for aligning data insights with business goals. You’ll also receive guidance to build your portfolio and land a job in data analysis. Sign up\nNews\nMore Consistent Characters and Styles\nSame character, new background, new action. That’s the focus of the latest text-to-image models from Germany’s Black Forest Labs. What’s new: The FLUX.1 Kontext family, which comes in versions dubbed max, pro, and dev, is trained to alter images in controlled ways. The company plans to release the weights for FLUX.1 Kontext dev but has not yet specified the licensing terms.\n- Input/output: text, image in; image out\n- Architecture: Unspecified text encoders, convolutional neural network image encoder-decoder, transformer. FLUX.1 Kontext dev 12 billion parameters, other parameter counts undisclosed\n- Features: Character consistency, local and global alterations\n- Availability/price: FLUX.1 Kontext max and FLUX.1 Kontext pro available via FLUX Playground and various partners, $0.08 per image (FLUX.1 max) and $0.04 per image (FLUX.1 pro) via Fal, an image-generation platform.\n- Undisclosed: Parameter counts of FLUX.1 Kontext max and FLUX.1 Kontext pro, architecture of text encoders, training data, evaluation protocol, open-weights license\nHow it works: The FLUX.1 Kontext models include encoders that embed input text and/or images, a transformer that processes them, and an image decoder that generates images. The current technical report doesn’t describe how it trained them for character consistency and image editing.\n- The team trained the convolutional neural network encoder-decoder to reproduce images and to fool a discriminator (architecture and training unspecified) into classifying them as real.\n- Having frozen the encoders, they trained the transformer — given a time step, embedding of a text prompt, embedding of a reference image, and noisy image embedding — to remove the noise over a series of steps.\n- They further trained the transformer to encourage it to produce noise-free embeddings that a second discriminator would classify as representing real images. This process, a variant of adversarial diffusion distillation, helps reduce the number of steps needed to produce a good image embedding. Results: The team compared the output of FLUX.1 Kontext models with that of five competing models including OpenAI GPT Image 1 (at three different quality levels) and Google Gemini 2.0 Flash native image generation.", "content_token_count": 386, "mistral_embedding": null}, {"position": 4, "content_text": "The current technical report doesn’t describe how it trained them for character consistency and image editing.\n- The team trained the convolutional neural network encoder-decoder to reproduce images and to fool a discriminator (architecture and training unspecified) into classifying them as real.\n- Having frozen the encoders, they trained the transformer — given a time step, embedding of a text prompt, embedding of a reference image, and noisy image embedding — to remove the noise over a series of steps.\n- They further trained the transformer to encourage it to produce noise-free embeddings that a second discriminator would classify as representing real images. This process, a variant of adversarial diffusion distillation, helps reduce the number of steps needed to produce a good image embedding. Results: The team compared the output of FLUX.1 Kontext models with that of five competing models including OpenAI GPT Image 1 (at three different quality levels) and Google Gemini 2.0 Flash native image generation. An undisclosed number of people evaluated the models according to a proprietary benchmark that highlights altering local and global aspects of an image, editing generated text within an image, maintaining consistent characters, and generating an image according to a reference style. The dataset included roughly 1,000 crowd-sourced pairs of text prompts and reference images.\n- FLUX.1 Kontext max and FLUX.1 Kontext pro outperformed all competing models.\n- FLUX.1 dev outperformed all except other family members and GPT Image 1 set to high or medium quality. Behind the news: Character consistency, also known as personalization, has come a long way since text-to-image generators became popular. In 2022, Textual Inversion showed how to learn an embedding of a character and use that embedding to produce further images. In 2023, DreamBooth showed how to get good results by fine-tuning a model on a few images of the character to be portrayed in a new situation. Since then, image-editing models have improved in quality and generality, including Meta Emu-Edit, OmniGen, and OpenAI gpt-image-1. Why it matters: Consistency and precise editing enable artists to craft stories around specific characters. Such models have become better at generating consistent details across images, but they remain finicky, sometimes changing minute details or entire characters and backgrounds. The more faithfully they help users express their ideas, the more firmly embedded in the creative toolkit they’ll become. We’re thinking: Black Forest Labs announced plans to publish its proprietary benchmark.", "content_token_count": 397, "mistral_embedding": null}, {"position": 5, "content_text": "Since then, image-editing models have improved in quality and generality, including Meta Emu-Edit, OmniGen, and OpenAI gpt-image-1. Why it matters: Consistency and precise editing enable artists to craft stories around specific characters. Such models have become better at generating consistent details across images, but they remain finicky, sometimes changing minute details or entire characters and backgrounds. The more faithfully they help users express their ideas, the more firmly embedded in the creative toolkit they’ll become. We’re thinking: Black Forest Labs announced plans to publish its proprietary benchmark. There’s a real need for common benchmarks to evaluate image generation, and we hope other developers will give it due consideration. AI Market Trends in Charts and Graphs\nRenowned investment analyst Mary Meeker is back with a report on the AI market, six years after publishing her last survey of the internet. What’s new: Meeker, co-founder of the venture capital firm Bond who formerly analyzed technology portfolios for Merrill Lynch, Salomon Brothers, and Morgan Stanley, published “Trends — Artificial Intelligence (May ‘25).” The report, which spans 340 graph-packed pages, revives and updates a series that chronicled the rise of the internet nearly every year from 1995 through 2019. How it works: The new report focuses on a handful of themes that arise from the unprecedented growth and capabilities of deep learning. As Meeker told Axios, AI is an arena for “intense competition the likes of which we’ve never seen before,” and that makes the present time “a period for lots of wealth creation and wealth destruction.”\n- Rapid growth: Change in AI is happening faster than ever. Users of ChatGPT reached 1 million in 5 days — compared to the iPhone’s 74 days — and since then have rocketed to 800 million. Total capital expenditures of the six biggest technology companies (largely driven by AI) rose 63 percent to $212 billion between 2023 and 2024. Training datasets are growing 260 percent per year, processing power devoted to training is growing 360 percent per year, effective processing power is growing at 200 percent annually.\n- Revenues and costs: The economics of this new world are not straightforward. On one hand, revenue is soaring at giants like Amazon, Google, and Nvidia as well as startups like Scale AI. On the other hand, the cost of computation is rising steadily even as the cost per token of output falls precipitously.", "content_token_count": 394, "mistral_embedding": null}, {"position": 6, "content_text": "Training datasets are growing 260 percent per year, processing power devoted to training is growing 360 percent per year, effective processing power is growing at 200 percent annually.\n- Revenues and costs: The economics of this new world are not straightforward. On one hand, revenue is soaring at giants like Amazon, Google, and Nvidia as well as startups like Scale AI. On the other hand, the cost of computation is rising steadily even as the cost per token of output falls precipitously. Meanwhile, rapid turnover of models and proliferation of open-source alternatives are wild cards for AI-powered businesses.\n- Rising performance: AI performance continues to increase. AI’s ability to complete the MMLU benchmark of language understanding outstripped human performance last year. This year, 73 percent of human testers classified responses generated by an LLM as human, according to one study. Synthetic images, video, and speech generation — all are increasingly capable of fooling human testers.\n- Emerging capabilities: Today’s AI is capable of writing and editing, tutoring, brainstorming, automating repetitive work, and providing companionship. Within five years, it will generate code as well as humans, create films and games, operate humanlike robots, and drive scientific discovery. Meeker forecasts that within 10 years, AI will conduct scientific research, design advanced technologies, and build immersive digital worlds.\n- Workforce implications: Industries most likely to be affected by AI include knowledge work, content creation, legal services, software development, financial services, customer service, drug discovery, and manufacturing. Employers are adopting AI to get a boost in workforce productivity that Stanford researchers estimate is an average 14 percent. Companies like Box, Duolingo, and Shopify are adopting an AI-first orientation, while AI-related job titles have risen 200 percent in the past two years.\n- AI gets physical: AI is having a profound impact on the physical world. Lyft’s and Uber’s market share fell around 15 percent while Waymo’s gained 27 percent over the past 18 months. AI-driven mineral exploration is boosting mine efficiency, and AI-powered agriculture is cutting the use of pesticides. And, sadly, AI-equipped attack drones are wreaking destruction upon Ukraine and elsewhere, even as they play a critical role in defense. Behind the news: Meeker published her first “Internet Trends” report in 1995, anticipating the coming online boom, and she issued new editions annually throughout the 2000s and much of the coming decade.", "content_token_count": 389, "mistral_embedding": null}, {"position": 7, "content_text": "Lyft’s and Uber’s market share fell around 15 percent while Waymo’s gained 27 percent over the past 18 months. AI-driven mineral exploration is boosting mine efficiency, and AI-powered agriculture is cutting the use of pesticides. And, sadly, AI-equipped attack drones are wreaking destruction upon Ukraine and elsewhere, even as they play a critical role in defense. Behind the news: Meeker published her first “Internet Trends” report in 1995, anticipating the coming online boom, and she issued new editions annually throughout the 2000s and much of the coming decade. Her final internet report arrived in 2019, the year after she founded Bond, when the report highlighted the rise of visual social media like Instagram, wearable technology, and digital payments. Why it matters: “Trends — Artificial Intelligence” offers a wealth of market data culled from analyst reports, consumer surveys, and academic studies. The AI community has a number of excellent annual surveys, including Stanford’s AI Index and Air Street Capital’s State of AI. Meeker, who has been watching technology markets since the dawning of the web, adds another valuable perspective. We’re thinking: One implication of the report: There has never been a better time to build software applications. For developers, it’s time to hone and update skills. For tech companies, it’s time to cast the net for talent. As Meeker said in her interview with Axios, “Companies that get the best developers often win.”\nBenchmarking Costs Climb\nAn independent AI test lab detailed the rising cost of benchmarking reasoning models. What’s new: Artificial Analysis, an organization that tracks model performance and cost, revealed its budgets for evaluating a few recent models that improve their output by producing chains of thought, which use extra computation and thus boost the cost of inference. The expense is making it difficult for startups, academic labs, and other organizations that have limited resources to reproduce results reported by model developers, TechCrunch reported.", "content_token_count": 315, "mistral_embedding": null}, {"position": 8, "content_text": "As Meeker said in her interview with Axios, “Companies that get the best developers often win.”\nBenchmarking Costs Climb\nAn independent AI test lab detailed the rising cost of benchmarking reasoning models. What’s new: Artificial Analysis, an organization that tracks model performance and cost, revealed its budgets for evaluating a few recent models that improve their output by producing chains of thought, which use extra computation and thus boost the cost of inference. The expense is making it difficult for startups, academic labs, and other organizations that have limited resources to reproduce results reported by model developers, TechCrunch reported. (Disclosure: Andrew Ng is an investor in Artificial Analysis.)\nHow it works: Artificial Analysis tested reasoning and non-reasoning models on popular benchmarks that gauge model performance in responding to queries that require specialized knowledge or multi-step reasoning, solving math problems, generating computer programs, and the like.\n- Running a group of seven popular benchmarks, OpenAI o1 (which produces chains of thought) produced more than 44 million tokens, while GPT-4o (which doesn’t take explicit reasoning steps) produced around 5.5 million tokens.\n- Benchmarking o1 cost $2,767, while benchmarking Anthropic Claude 3.7 Sonnet (which allows users to allocate a number of reasoning tokens per query; TechCrunch doesn’t provide the number in this case) cost $1,485. Smaller reasoning models are significantly less expensive: o3-mini (at high effort, which uses the highest number of reasoning tokens per query) cost $345, and o1-mini cost $141.\n- Non-reasoning models are less expensive to test. Evaluating GPT-4o cost $109, Claude 3.5 Sonnet was $81.\n- Artificial Analysis spent around $5,200 to test 12 reasoning models versus around $2,400 to test more than 80 non-reasoning models. Behind the news: Generally, the cost per token of using AI models has been falling even as their performance has been rising. However, two factors complicate that trend. (i) Reasoning models produce more tokens and thus cost more to run, and (ii) developers are charging higher per-token prices to use their latest models. For example, o1-pro and GPT-4.5 (a non-reasoning model), both released in early 2025, cost $600 per million output tokens, while Claude 3.5 Sonnet (released in July 2024) costs $15 per million tokens of output. Emerging techniques that allow users to allocate numbers of tokens to reasoning (whether “high” or “low” or a specific tally) also make benchmarking more costly and complicated.", "content_token_count": 391, "mistral_embedding": null}, {"position": 9, "content_text": "(i) Reasoning models produce more tokens and thus cost more to run, and (ii) developers are charging higher per-token prices to use their latest models. For example, o1-pro and GPT-4.5 (a non-reasoning model), both released in early 2025, cost $600 per million output tokens, while Claude 3.5 Sonnet (released in July 2024) costs $15 per million tokens of output. Emerging techniques that allow users to allocate numbers of tokens to reasoning (whether “high” or “low” or a specific tally) also make benchmarking more costly and complicated. Why it matters: Benchmarks aren’t entirely sufficient for evaluating models, but they are a critical indicator of relative performance, and independent benchmarking helps to ensure that tests are run in a fair and consistent way. As the cost of benchmarking climbs, fewer labs are likely to confirm or challenge results obtained by the original developer, making it harder to compare models and recognize progress. We’re thinking: Verifying performance claims in independent, open, fair tests is essential to marking progress in general and choosing the right models for particular projects. It's time for the industry to support independent benchmarking organizations. Better Video, Fewer Tokens\nResearchers reduced the number of tokens needed to represent video frames to be fed to a transformer. What’s new: Jindong Jiang, Xiuyu Li, and collaborators at Nvidia, Rutgers University, UC Berkeley, Massachusetts Institute of Technology, Nanjing University, and Korea Advanced Institute of Science and Technology built STORM, a text-video system that performs well in tests of video understanding while processing fewer tokens. Key insight: In a multimodal system, a large language model (LLM) that receives video tokens may struggle to process long videos. However, sequences of video frames often contain lots of redundancy, since few pixels may change from one frame to the next. Instead of forcing the LLM to process long sequences of redundant video tokens, mamba layers can enrich the token embeddings that represent one frame with information from other frames in the same clip. That way, the system can average token embeddings across frames without losing crucial information, making it possible to feed fewer tokens to the LLM without compromising performance. How it works: The authors built STORM by training three components: (1) a pretrained SigLIP vision transformer, (2) untrained mamba layers, and (3) the pretrained large language model (LLM) from Qwen2-VL.", "content_token_count": 384, "mistral_embedding": null}, {"position": 10, "content_text": "Instead of forcing the LLM to process long sequences of redundant video tokens, mamba layers can enrich the token embeddings that represent one frame with information from other frames in the same clip. That way, the system can average token embeddings across frames without losing crucial information, making it possible to feed fewer tokens to the LLM without compromising performance. How it works: The authors built STORM by training three components: (1) a pretrained SigLIP vision transformer, (2) untrained mamba layers, and (3) the pretrained large language model (LLM) from Qwen2-VL. They trained the system to predict the next token in image-text pairs and video-text pairs with 32-frame videos, and video-text pairs with 128-frame videos.\n- SigLIP learned to turn each video frame into 256 image tokens.\n- Given a sequence of image tokens, mamba layers learned to process them in both directions – left-to-right and right-to-left – so each output token embedding encoded information from the entire video.\n- The system averaged the token embeddings of 4 consecutive frames, reducing by a factor of 4 the number of tokens processed by Qwen2-VL’s LLM.\n- Given the averaged token embeddings, Qwen2-VL LLM learned to predict the next word in the video’s associated text.\n- At inference, the system fed to the LLM the tokens that represented every second frame (a process the authors call temporal sampling), which further halved the input to the LLM. Results: STORM outperformed proprietary and open models on measures of video understanding.\n- On MVBench, which asks multiple-choice questions about actions, object interactions, and scene transitions in 16-second videos, STORM achieved 70.6 percent accuracy. That’s better than GPT-4o (64.6 percent accuracy) and Qwen2-VL (67.0 percent accuracy). A baseline system (STORM’s SigLIP and Qwen2-VL LLM without mamba layers, averaging image tokens, and temporal sampling) achieved 69.5 percent.\n- On MLVU, which asks multiple-choice and open-ended questions about videos that range from 3 minutes to over 2 hours long, STORM reached 72.9 percent accuracy, topping GPT-4o (66.2 percent accuracy). The baseline model achieved 70.2 percent. Why it matters: STORM compresses video at the input to the LLM, so the LLM processes 1/8 as many video tokens and uses 1/8 as much compute to process them. This enables the system to work more than 3 times faster than the baseline while performing better.", "content_token_count": 384, "mistral_embedding": null}, {"position": 11, "content_text": "A baseline system (STORM’s SigLIP and Qwen2-VL LLM without mamba layers, averaging image tokens, and temporal sampling) achieved 69.5 percent.\n- On MLVU, which asks multiple-choice and open-ended questions about videos that range from 3 minutes to over 2 hours long, STORM reached 72.9 percent accuracy, topping GPT-4o (66.2 percent accuracy). The baseline model achieved 70.2 percent. Why it matters: STORM compresses video at the input to the LLM, so the LLM processes 1/8 as many video tokens and uses 1/8 as much compute to process them. This enables the system to work more than 3 times faster than the baseline while performing better. We’re thinking: Initial work on the mamba architecture positioned it as a replacement for the transformer, but this work, along with other projects, combines them to get the benefits of both. A MESSAGE FROM DEEPLEARNING.AI\nIn “Orchestrating Workflows for GenAI Applications” you’ll learn to orchestrate generative AI workflows using Apache Airflow 3.0. You’ll build and schedule RAG pipelines, run tasks in parallel, and add retries and alerts for reliability. No prior Airflow experience is needed! Enroll for free", "content_token_count": 182, "mistral_embedding": null}, {"position": 12, "content_text": "This enables the system to work more than 3 times faster than the baseline while performing better. We’re thinking: Initial work on the mamba architecture positioned it as a replacement for the transformer, but this work, along with other projects, combines them to get the benefits of both. A MESSAGE FROM DEEPLEARNING.AI\nIn “Orchestrating Workflows for GenAI Applications” you’ll learn to orchestrate generative AI workflows using Apache Airflow 3.0. You’ll build and schedule RAG pipelines, run tasks in parallel, and add retries and alerts for reliability. No prior Airflow experience is needed! Enroll for free", "content_token_count": 95, "mistral_embedding": null}, {"position": 13, "content_text": "We’re thinking: Initial work on the mamba architecture positioned it as a replacement for the transformer, but this work, along with other projects, combines them to get the benefits of both. A MESSAGE FROM DEEPLEARNING.AI\nIn “Orchestrating Workflows for GenAI Applications” you’ll learn to orchestrate generative AI workflows using Apache Airflow 3.0. You’ll build and schedule RAG pipelines, run tasks in parallel, and add retries and alerts for reliability. No prior Airflow experience is needed! Enroll for free", "content_token_count": 78, "mistral_embedding": null}, {"position": 14, "content_text": "A MESSAGE FROM DEEPLEARNING.AI\nIn “Orchestrating Workflows for GenAI Applications” you’ll learn to orchestrate generative AI workflows using Apache Airflow 3.0. You’ll build and schedule RAG pipelines, run tasks in parallel, and add retries and alerts for reliability. No prior Airflow experience is needed! Enroll for free", "content_token_count": 47, "mistral_embedding": null}, {"position": 15, "content_text": "You’ll build and schedule RAG pipelines, run tasks in parallel, and add retries and alerts for reliability. No prior Airflow experience is needed! Enroll for free", "content_token_count": 26, "mistral_embedding": null}, {"position": 16, "content_text": "No prior Airflow experience is needed! Enroll for free", "content_token_count": 9, "mistral_embedding": null}, {"position": 17, "content_text": "Enroll for free", "content_token_count": 3, "mistral_embedding": null}], "fbf749b3-88f5-437b-b682-d077cd975706": [{"position": 0, "content_text": "We recently released Claude Code, a command line tool for agentic coding. Developed as a research project, Claude Code gives Anthropic engineers and researchers a more native way to integrate Claude into their coding workflows. Claude Code is intentionally low-level and unopinionated, providing close to raw model access without forcing specific workflows. This design philosophy creates a flexible, customizable, scriptable, and safe power tool. While powerful, this flexibility presents a learning curve for engineers new to agentic coding tools—at least until they develop their own best practices. This post outlines general patterns that have proven effective, both for Anthropic's internal teams and for external engineers using Claude Code across various codebases, languages, and environments. Nothing in this list is set in stone nor universally applicable; consider these suggestions as starting points. We encourage you to experiment and find what works best for you! Looking for more detailed information? Our comprehensive documentation at claude.ai/code covers all the features mentioned in this post and provides additional examples, implementation details, and advanced techniques. 1. Customize your setup\nClaude Code is an agentic coding assistant that automatically pulls context into prompts. This context gathering consumes time and tokens, but you can optimize it through environment tuning.\na. Create CLAUDE.md\nfiles\nCLAUDE.md\nis a special file that Claude automatically pulls into context when starting a conversation. This makes it an ideal place for documenting:\n- Common bash commands\n- Core files and utility functions\n- Code style guidelines\n- Testing instructions\n- Repository etiquette (e.g., branch naming, merge vs. rebase, etc.)\n- Developer environment setup (e.g., pyenv use, which compilers work)\n- Any unexpected behaviors or warnings particular to the project\n- Other information you want Claude to remember\nThere’s no required format for CLAUDE.md\nfiles. We recommend keeping them concise and human-readable. For example:\n# Bash commands\n- npm run build: Build the project\n- npm run typecheck: Run the typechecker\n# Code style\n- Use ES modules (import/export) syntax, not CommonJS (require)\n- Destructure imports when possible (eg. import { foo } from 'bar')\n# Workflow\n- Be sure to typecheck when you’re done making a series of code changes\n- Prefer running single tests, and not the whole test suite, for performance\nYou can place CLAUDE.md\nfiles in several locations:\n- The root of your repo, or wherever you run\nclaude\nfrom (the most common usage).", "content_token_count": 394, "mistral_embedding": null}, {"position": 1, "content_text": "For example:\n# Bash commands\n- npm run build: Build the project\n- npm run typecheck: Run the typechecker\n# Code style\n- Use ES modules (import/export) syntax, not CommonJS (require)\n- Destructure imports when possible (eg. import { foo } from 'bar')\n# Workflow\n- Be sure to typecheck when you’re done making a series of code changes\n- Prefer running single tests, and not the whole test suite, for performance\nYou can place CLAUDE.md\nfiles in several locations:\n- The root of your repo, or wherever you run\nclaude\nfrom (the most common usage). Name itCLAUDE.md\nand check it into git so that you can share it across sessions and with your team (recommended), or name itCLAUDE.local.md\nand.gitignore\nit - Any parent of the directory where you run\nclaude\n. This is most useful for monorepos, where you might runclaude\nfromroot/foo\n, and haveCLAUDE.md\nfiles in bothroot/CLAUDE.md\nandroot/foo/CLAUDE.md\n. Both of these will be pulled into context automatically - Any child of the directory where you run\nclaude\n. This is the inverse of the above, and in this case, Claude will pull inCLAUDE.md\nfiles on demand when you work with files in child directories - Your home folder (\n~/.claude/CLAUDE.md\n), which applies it to all your claude sessions\nWhen you run the /init\ncommand, Claude will automatically generate a CLAUDE.md\nfor you.\nb. Tune your CLAUDE.md\nfiles\nYour CLAUDE.md\nfiles become part of Claude’s prompts, so they should be refined like any frequently used prompt. A common mistake is adding extensive content without iterating on its effectiveness. Take time to experiment and determine what produces the best instruction following from the model. You can add content to your CLAUDE.md\nmanually or press the #\nkey to give Claude an instruction that it will automatically incorporate into the relevant CLAUDE.md\n. Many engineers use #\nfrequently to document commands, files, and style guidelines while coding, then include CLAUDE.md\nchanges in commits so team members benefit as well. At Anthropic, we occasionally run CLAUDE.md\nfiles through the prompt improver and often tune instructions (e.g. adding emphasis with \"IMPORTANT\" or \"YOU MUST\") to improve adherence.\nc. Curate Claude's list of allowed tools\nBy default, Claude Code requests permission for any action that might modify your system: file writes, many bash commands, MCP tools, etc. We designed Claude Code with this deliberately conservative approach to prioritize safety.", "content_token_count": 397, "mistral_embedding": null}, {"position": 2, "content_text": "Many engineers use #\nfrequently to document commands, files, and style guidelines while coding, then include CLAUDE.md\nchanges in commits so team members benefit as well. At Anthropic, we occasionally run CLAUDE.md\nfiles through the prompt improver and often tune instructions (e.g. adding emphasis with \"IMPORTANT\" or \"YOU MUST\") to improve adherence.\nc. Curate Claude's list of allowed tools\nBy default, Claude Code requests permission for any action that might modify your system: file writes, many bash commands, MCP tools, etc. We designed Claude Code with this deliberately conservative approach to prioritize safety. You can customize the allowlist to permit additional tools that you know are safe, or to allow potentially unsafe tools that are easy to undo (e.g., file editing, git commit\n). There are four ways to manage allowed tools:\n- Select \"Always allow\" when prompted during a session.\n- Use the\n/permissions\ncommand after starting Claude Code to add or remove tools from the allowlist. For example, you can addEdit\nto always allow file edits,Bash(git commit:*)\nto allow git commits, ormcp__puppeteer__puppeteer_navigate\nto allow navigating with the Puppeteer MCP server. - Manually edit your\n.claude/settings.json\nor~/.claude.json\n(we recommend checking the former into source control to share with your team). - Use the\n--allowedTools\nCLI flag for session-specific permissions.\nd. If using GitHub, install the gh CLI\nClaude knows how to use the gh\nCLI to interact with GitHub for creating issues, opening pull requests, reading comments, and more. Without gh\ninstalled, Claude can still use the GitHub API or MCP server (if you have it installed). 2. Give Claude more tools\nClaude has access to your shell environment, where you can build up sets of convenience scripts and functions for it just like you would for yourself. It can also leverage more complex tools through MCP and REST APIs.\na. Use Claude with bash tools\nClaude Code inherits your bash environment, giving it access to all your tools. While Claude knows common utilities like unix tools and gh\n, it won't know about your custom bash tools without instructions:\n- Tell Claude the tool name with usage examples\n- Tell Claude to run\n--help\nto see tool documentation - Document frequently used tools in\nCLAUDE.md\nb. Use Claude with MCP\nClaude Code functions as both an MCP server and client.", "content_token_count": 382, "mistral_embedding": null}, {"position": 3, "content_text": "Use Claude with bash tools\nClaude Code inherits your bash environment, giving it access to all your tools. While Claude knows common utilities like unix tools and gh\n, it won't know about your custom bash tools without instructions:\n- Tell Claude the tool name with usage examples\n- Tell Claude to run\n--help\nto see tool documentation - Document frequently used tools in\nCLAUDE.md\nb. Use Claude with MCP\nClaude Code functions as both an MCP server and client. As a client, it can connect to any number of MCP servers to access their tools in three ways:\n- In project config (available when running Claude Code in that directory)\n- In global config (available in all projects)\n- In a checked-in\n.mcp.json\nfile (available to anyone working in your codebase). For example, you can add Puppeteer and Sentry servers to your.mcp.json\n, so that every engineer working on your repo can use these out of the box. When working with MCP, it can also be helpful to launch Claude with the --mcp-debug\nflag to help identify configuration issues.\nc. Use custom slash commands\nFor repeated workflows—debugging loops, log analysis, etc.—store prompt templates in Markdown files within the .claude/commands\nfolder. These become available through the slash commands menu when you type /\n. You can check these commands into git to make them available for the rest of your team. Custom slash commands can include the special keyword $ARGUMENTS\nto pass parameters from command invocation. For example, here’s a slash command that you could use to automatically pull and fix a Github issue:\nPlease analyze and fix the GitHub issue: $ARGUMENTS. Follow these steps:\n1. Use `gh issue view` to get the issue details\n2. Understand the problem described in the issue\n3. Search the codebase for relevant files\n4. Implement the necessary changes to fix the issue\n5. Write and run tests to verify the fix\n6. Ensure code passes linting and type checking\n7. Create a descriptive commit message\n8. Push and create a PR\nRemember to use the GitHub CLI (`gh`) for all GitHub-related tasks. Putting the above content into .claude/commands/fix-github-issue.md\nmakes it available as the /project:fix-github-issue\ncommand in Claude Code. You could then for example use /project:fix-github-issue 1234\nto have Claude fix issue #1234.", "content_token_count": 379, "mistral_embedding": null}, {"position": 4, "content_text": "Search the codebase for relevant files\n4. Implement the necessary changes to fix the issue\n5. Write and run tests to verify the fix\n6. Ensure code passes linting and type checking\n7. Create a descriptive commit message\n8. Push and create a PR\nRemember to use the GitHub CLI (`gh`) for all GitHub-related tasks. Putting the above content into .claude/commands/fix-github-issue.md\nmakes it available as the /project:fix-github-issue\ncommand in Claude Code. You could then for example use /project:fix-github-issue 1234\nto have Claude fix issue #1234. Similarly, you can add your own personal commands to the ~/.claude/commands\nfolder for commands you want available in all of your sessions. 3. Try common workflows\nClaude Code doesn’t impose a specific workflow, giving you the flexibility to use it how you want. Within the space this flexibility affords, several successful patterns for effectively using Claude Code have emerged across our community of users:\na. Explore, plan, code, commit\nThis versatile workflow suits many problems:\n- Ask Claude to read relevant files, images, or URLs, providing either general pointers (\"read the file that handles logging\") or specific filenames (\"read logging.py\"), but explicitly tell it not to write any code just yet.\n- This is the part of the workflow where you should consider strong use of subagents, especially for complex problems. Telling Claude to use subagents to verify details or investigate particular questions it might have, especially early on in a conversation or task, tends to preserve context availability without much downside in terms of lost efficiency.\n- Ask Claude to make a plan for how to approach a specific problem. We recommend using the word \"think\" to trigger extended thinking mode, which gives Claude additional computation time to evaluate alternatives more thoroughly. These specific phrases are mapped directly to increasing levels of thinking budget in the system: \"think\" < \"think hard\" < \"think harder\" < \"ultrathink.\" Each level allocates progressively more thinking budget for Claude to use.\n- If the results of this step seem reasonable, you can have Claude create a document or a GitHub issue with its plan so that you can reset to this spot if the implementation (step 3) isn’t what you want.\n- Ask Claude to implement its solution in code.", "content_token_count": 372, "mistral_embedding": null}, {"position": 5, "content_text": "These specific phrases are mapped directly to increasing levels of thinking budget in the system: \"think\" < \"think hard\" < \"think harder\" < \"ultrathink.\" Each level allocates progressively more thinking budget for Claude to use.\n- If the results of this step seem reasonable, you can have Claude create a document or a GitHub issue with its plan so that you can reset to this spot if the implementation (step 3) isn’t what you want.\n- Ask Claude to implement its solution in code. This is also a good place to ask it to explicitly verify the reasonableness of its solution as it implements pieces of the solution.\n- Ask Claude to commit the result and create a pull request. If relevant, this is also a good time to have Claude update any READMEs or changelogs with an explanation of what it just did. Steps #1-#2 are crucial—without them, Claude tends to jump straight to coding a solution. While sometimes that's what you want, asking Claude to research and plan first significantly improves performance for problems requiring deeper thinking upfront.\nb. Write tests, commit; code, iterate, commit\nThis is an Anthropic-favorite workflow for changes that are easily verifiable with unit, integration, or end-to-end tests. Test-driven development (TDD) becomes even more powerful with agentic coding:\n- Ask Claude to write tests based on expected input/output pairs. Be explicit about the fact that you’re doing test-driven development so that it avoids creating mock implementations, even for functionality that doesn’t exist yet in the codebase.\n- Tell Claude to run the tests and confirm they fail. Explicitly telling it not to write any implementation code at this stage is often helpful.\n- Ask Claude to commit the tests when you’re satisfied with them.\n- Ask Claude to write code that passes the tests, instructing it not to modify the tests. Tell Claude to keep going until all tests pass. It will usually take a few iterations for Claude to write code, run the tests, adjust the code, and run the tests again.\n- At this stage, it can help to ask it to verify with independent subagents that the implementation isn’t overfitting to the tests\n- Ask Claude to commit the code once you’re satisfied with the changes. Claude performs best when it has a clear target to iterate against—a visual mock, a test case, or another kind of output.", "content_token_count": 397, "mistral_embedding": null}, {"position": 6, "content_text": "It will usually take a few iterations for Claude to write code, run the tests, adjust the code, and run the tests again.\n- At this stage, it can help to ask it to verify with independent subagents that the implementation isn’t overfitting to the tests\n- Ask Claude to commit the code once you’re satisfied with the changes. Claude performs best when it has a clear target to iterate against—a visual mock, a test case, or another kind of output. By providing expected outputs like tests, Claude can make changes, evaluate results, and incrementally improve until it succeeds.\nc. Write code, screenshot result, iterate\nSimilar to the testing workflow, you can provide Claude with visual targets:\n- Give Claude a way to take browser screenshots (e.g., with the Puppeteer MCP server, an iOS simulator MCP server, or manually copy / paste screenshots into Claude).\n- Give Claude a visual mock by copying / pasting or drag-dropping an image, or giving Claude the image file path.\n- Ask Claude to implement the design in code, take screenshots of the result, and iterate until its result matches the mock.\n- Ask Claude to commit when you're satisfied. Like humans, Claude's outputs tend to improve significantly with iteration. While the first version might be good, after 2-3 iterations it will typically look much better. Give Claude the tools to see its outputs for best results.\nd. Safe YOLO mode\nInstead of supervising Claude, you can use claude --dangerously-skip-permissions\nto bypass all permission checks and let Claude work uninterrupted until completion. This works well for workflows like fixing lint errors or generating boilerplate code. Letting Claude run arbitrary commands is risky and can result in data loss, system corruption, or even data exfiltration (e.g., via prompt injection attacks). To minimize these risks, use --dangerously-skip-permissions\nin a container without internet access. You can follow this reference implementation using Docker Dev Containers.\ne. Codebase Q&A\nWhen onboarding to a new codebase, use Claude Code for learning and exploration. You can ask Claude the same sorts of questions you would ask another engineer on the project when pair programming.", "content_token_count": 353, "mistral_embedding": null}, {"position": 7, "content_text": "Letting Claude run arbitrary commands is risky and can result in data loss, system corruption, or even data exfiltration (e.g., via prompt injection attacks). To minimize these risks, use --dangerously-skip-permissions\nin a container without internet access. You can follow this reference implementation using Docker Dev Containers.\ne. Codebase Q&A\nWhen onboarding to a new codebase, use Claude Code for learning and exploration. You can ask Claude the same sorts of questions you would ask another engineer on the project when pair programming. Claude can agentically search the codebase to answer general questions like:\n- How does logging work?\n- How do I make a new API endpoint?\n- What does\nasync move { ... }\ndo on line 134 offoo.rs\n? - What edge cases does\nCustomerOnboardingFlowImpl\nhandle? - Why are we calling\nfoo()\ninstead ofbar()\non line 333? - What’s the equivalent of line 334 of\nbaz.py\nin Java? At Anthropic, using Claude Code in this way has become our core onboarding workflow, significantly improving ramp-up time and reducing load on other engineers. No special prompting is required! Simply ask questions, and Claude will explore the code to find answers.\nf. Use Claude to interact with git\nClaude can effectively handle many git operations. Many Anthropic engineers use Claude for 90%+ of our git interactions:\n- Searching git history to answer questions like \"What changes made it into v1.2.3?\", \"Who owns this particular feature?\", or \"Why was this API designed this way?\" It helps to explicitly prompt Claude to look through git history to answer queries like these.\n- Writing commit messages. Claude will look at your changes and recent history automatically to compose a message taking all the relevant context into account\n- Handling complex git operations like reverting files, resolving rebase conflicts, and comparing and grafting patches\ng.", "content_token_count": 301, "mistral_embedding": null}, {"position": 8, "content_text": "Many Anthropic engineers use Claude for 90%+ of our git interactions:\n- Searching git history to answer questions like \"What changes made it into v1.2.3?\", \"Who owns this particular feature?\", or \"Why was this API designed this way?\" It helps to explicitly prompt Claude to look through git history to answer queries like these.\n- Writing commit messages. Claude will look at your changes and recent history automatically to compose a message taking all the relevant context into account\n- Handling complex git operations like reverting files, resolving rebase conflicts, and comparing and grafting patches\ng. Use Claude to interact with GitHub\nClaude Code can manage many GitHub interactions:\n- Creating pull requests: Claude understands the shorthand \"pr\" and will generate appropriate commit messages based on the diff and surrounding context.\n- Implementing one-shot resolutions for simple code review comments: just tell it to fix comments on your PR (optionally, give it more specific instructions) and push back to the PR branch when it's done.\n- Fixing failing builds or linter warnings\n- Categorizing and triaging open issues by asking Claude to loop over open GitHub issues\nThis eliminates the need to remember gh\ncommand line syntax while automating routine tasks.\nh. Use Claude to work with Jupyter notebooks\nResearchers and data scientists at Anthropic use Claude Code to read and write Jupyter notebooks. Claude can interpret outputs, including images, providing a fast way to explore and interact with data. There are no required prompts or workflows, but a workflow we recommend is to have Claude Code and a .ipynb\nfile open side-by-side in VS Code. You can also ask Claude to clean up or make aesthetic improvements to your Jupyter notebook before you show it to colleagues. Specifically telling it to make the notebook or its data visualizations “aesthetically pleasing” tends to help remind it that it’s optimizing for a human viewing experience. 4. Optimize your workflow\nThe suggestions below apply across all workflows:\na. Be specific in your instructions\nClaude Code’s success rate improves significantly with more specific instructions, especially on first attempts. Giving clear directions upfront reduces the need for course corrections later.", "content_token_count": 356, "mistral_embedding": null}, {"position": 9, "content_text": "You can also ask Claude to clean up or make aesthetic improvements to your Jupyter notebook before you show it to colleagues. Specifically telling it to make the notebook or its data visualizations “aesthetically pleasing” tends to help remind it that it’s optimizing for a human viewing experience. 4. Optimize your workflow\nThe suggestions below apply across all workflows:\na. Be specific in your instructions\nClaude Code’s success rate improves significantly with more specific instructions, especially on first attempts. Giving clear directions upfront reduces the need for course corrections later. For example:\n| Poor | Good |\n|---|---|\n| add tests for foo.py | write a new test case for foo.py, covering the edge case where the user is logged out. avoid mocks |\n| why does ExecutionFactory have such a weird api? | look through ExecutionFactory's git history and summarize how its api came to be |\n| add a calendar widget | look at how existing widgets are implemented on the home page to understand the patterns and specifically how code and interfaces are separated out. HotDogWidget.php is a good example to start with. then, follow the pattern to implement a new calendar widget that lets the user select a month and paginate forwards/backwards to pick a year. Build from scratch without libraries other than the ones already used in the rest of the codebase. |\nClaude can infer intent, but it can't read minds. Specificity leads to better alignment with expectations.\nb. Give Claude images\nClaude excels with images and diagrams through several methods:\n- Paste screenshots (pro tip: hit cmd+ctrl+shift+4 in macOS to screenshot to clipboard and ctrl+v to paste. Note that this is not cmd+v like you would usually use to paste on mac and does not work remotely.)\n- Drag and drop images directly into the prompt input\n- Provide file paths for images\nThis is particularly useful when working with design mocks as reference points for UI development, and visual charts for analysis and debugging. If you are not adding visuals to context, it can still be helpful to be clear with Claude about how important it is for the result to be visually appealing.\nc. Mention files you want Claude to look at or work on\nUse tab-completion to quickly reference files or folders anywhere in your repository, helping Claude find or update the right resources.\nd.", "content_token_count": 395, "mistral_embedding": null}, {"position": 10, "content_text": "Note that this is not cmd+v like you would usually use to paste on mac and does not work remotely.)\n- Drag and drop images directly into the prompt input\n- Provide file paths for images\nThis is particularly useful when working with design mocks as reference points for UI development, and visual charts for analysis and debugging. If you are not adding visuals to context, it can still be helpful to be clear with Claude about how important it is for the result to be visually appealing.\nc. Mention files you want Claude to look at or work on\nUse tab-completion to quickly reference files or folders anywhere in your repository, helping Claude find or update the right resources.\nd. Give Claude URLs\nPaste specific URLs alongside your prompts for Claude to fetch and read. To avoid permission prompts for the same domains (e.g., docs.foo.com), use /permissions\nto add domains to your allowlist.\ne. Course correct early and often\nWhile auto-accept mode (shift+tab to toggle) lets Claude work autonomously, you'll typically get better results by being an active collaborator and guiding Claude's approach. You can get the best results by thoroughly explaining the task to Claude at the beginning, but you can also course correct Claude at any time. These four tools help with course correction:\n- Ask Claude to make a plan before coding. Explicitly tell it not to code until you’ve confirmed its plan looks good.\n- Press Escape to interrupt Claude during any phase (thinking, tool calls, file edits), preserving context so you can redirect or expand instructions.\n- Double-tap Escape to jump back in history, edit a previous prompt, and explore a different direction. You can edit the prompt and repeat until you get the result you're looking for.\n- Ask Claude to undo changes, often in conjunction with option #2 to take a different approach. Though Claude Code occasionally solves problems perfectly on the first attempt, using these correction tools generally produces better solutions faster.\nf. Use /clear\nto keep context focused\nDuring long sessions, Claude's context window can fill with irrelevant conversation, file contents, and commands. This can reduce performance and sometimes distract Claude. Use the /clear\ncommand frequently between tasks to reset the context window.\ng.", "content_token_count": 374, "mistral_embedding": null}, {"position": 11, "content_text": "You can edit the prompt and repeat until you get the result you're looking for.\n- Ask Claude to undo changes, often in conjunction with option #2 to take a different approach. Though Claude Code occasionally solves problems perfectly on the first attempt, using these correction tools generally produces better solutions faster.\nf. Use /clear\nto keep context focused\nDuring long sessions, Claude's context window can fill with irrelevant conversation, file contents, and commands. This can reduce performance and sometimes distract Claude. Use the /clear\ncommand frequently between tasks to reset the context window.\ng. Use checklists and scratchpads for complex workflows\nFor large tasks with multiple steps or requiring exhaustive solutions—like code migrations, fixing numerous lint errors, or running complex build scripts—improve performance by having Claude use a Markdown file (or even a GitHub issue!) as a checklist and working scratchpad:\nFor example, to fix a large number of lint issues, you can do the following:\n- Tell Claude to run the lint command and write all resulting errors (with filenames and line numbers) to a Markdown checklist\n- Instruct Claude to address each issue one by one, fixing and verifying before checking it off and moving to the next\nh. Pass data into Claude\nSeveral methods exist for providing data to Claude:\n- Copy and paste directly into your prompt (most common approach)\n- Pipe into Claude Code (e.g.,\ncat foo.txt | claude\n), particularly useful for logs, CSVs, and large data - Tell Claude to pull data via bash commands, MCP tools, or custom slash commands\n- Ask Claude to read files or fetch URLs (works for images too)\nMost sessions involve a combination of these approaches. For example, you can pipe in a log file, then tell Claude to use a tool to pull in additional context to debug the logs. 5. Use headless mode to automate your infra\nClaude Code includes headless mode for non-interactive contexts like CI, pre-commit hooks, build scripts, and automation. Use the -p\nflag with a prompt to enable headless mode, and --output-format stream-json\nfor streaming JSON output. Note that headless mode does not persist between sessions. You have to trigger it each session.\na. Use Claude for issue triage\nHeadless mode can power automations triggered by GitHub events, such as when a new issue is created in your repository.", "content_token_count": 389, "mistral_embedding": null}, {"position": 12, "content_text": "Use headless mode to automate your infra\nClaude Code includes headless mode for non-interactive contexts like CI, pre-commit hooks, build scripts, and automation. Use the -p\nflag with a prompt to enable headless mode, and --output-format stream-json\nfor streaming JSON output. Note that headless mode does not persist between sessions. You have to trigger it each session.\na. Use Claude for issue triage\nHeadless mode can power automations triggered by GitHub events, such as when a new issue is created in your repository. For example, the public Claude Code repository uses Claude to inspect new issues as they come in and assign appropriate labels.\nb. Use Claude as a linter\nClaude Code can provide subjective code reviews beyond what traditional linting tools detect, identifying issues like typos, stale comments, misleading function or variable names, and more. 6. Uplevel with multi-Claude workflows\nBeyond standalone usage, some of the most powerful applications involve running multiple Claude instances in parallel:\na. Have one Claude write code; use another Claude to verify\nA simple but effective approach is to have one Claude write code while another reviews or tests it. Similar to working with multiple engineers, sometimes having separate context is beneficial:\n- Use Claude to write code\n- Run\n/clear\nor start a second Claude in another terminal - Have the second Claude review the first Claude's work\n- Start another Claude (or\n/clear\nagain) to read both the code and review feedback - Have this Claude edit the code based on the feedback\nYou can do something similar with tests: have one Claude write tests, then have another Claude write code to make the tests pass. You can even have your Claude instances communicate with each other by giving them separate working scratchpads and telling them which one to write to and which one to read from. This separation often yields better results than having a single Claude handle everything.\nb. Have multiple checkouts of your repo\nRather than waiting for Claude to complete each step, something many engineers at Anthropic do is:\n- Create 3-4 git checkouts in separate folders\n- Open each folder in separate terminal tabs\n- Start Claude in each folder with different tasks\n- Cycle through to check progress and approve/deny permission requests\nc. Use git worktrees\nThis approach shines for multiple independent tasks, offering a lighter-weight alternative to multiple checkouts.", "content_token_count": 394, "mistral_embedding": null}, {"position": 13, "content_text": "This separation often yields better results than having a single Claude handle everything.\nb. Have multiple checkouts of your repo\nRather than waiting for Claude to complete each step, something many engineers at Anthropic do is:\n- Create 3-4 git checkouts in separate folders\n- Open each folder in separate terminal tabs\n- Start Claude in each folder with different tasks\n- Cycle through to check progress and approve/deny permission requests\nc. Use git worktrees\nThis approach shines for multiple independent tasks, offering a lighter-weight alternative to multiple checkouts. Git worktrees allow you to check out multiple branches from the same repository into separate directories. Each worktree has its own working directory with isolated files, while sharing the same Git history and reflog. Using git worktrees enables you to run multiple Claude sessions simultaneously on different parts of your project, each focused on its own independent task. For instance, you might have one Claude refactoring your authentication system while another builds a completely unrelated data visualization component. Since the tasks don't overlap, each Claude can work at full speed without waiting for the other's changes or dealing with merge conflicts:\n- Create worktrees:\ngit worktree add ../project-feature-a feature-a\n- Launch Claude in each worktree:\ncd ../project-feature-a && claude\n- Create additional worktrees as needed (repeat steps 1-2 in new terminal tabs)\nSome tips:\n- Use consistent naming conventions\n- Maintain one terminal tab per worktree\n- If you’re using iTerm2 on Mac, set up notifications for when Claude needs attention\n- Use separate IDE windows for different worktrees\n- Clean up when finished:\ngit worktree remove ../project-feature-a\nd. Use headless mode with a custom harness\nclaude -p\n(headless mode) integrates Claude Code programmatically into larger workflows while leveraging its built-in tools and system prompt. There are two primary patterns for using headless mode:\n1. Fanning out handles large migrations or analyses (e.g., analyzing sentiment in hundreds of logs or analyzing thousands of CSVs):\n- Have Claude write a script to generate a task list. For example, generate a list of 2k files that need to be migrated from framework A to framework B.\n- Loop through tasks, calling Claude programmatically for each and giving it a task and a set of tools it can use. For example:\nclaude -p “migrate foo.py from React to Vue.", "content_token_count": 385, "mistral_embedding": null}, {"position": 14, "content_text": "Fanning out handles large migrations or analyses (e.g., analyzing sentiment in hundreds of logs or analyzing thousands of CSVs):\n- Have Claude write a script to generate a task list. For example, generate a list of 2k files that need to be migrated from framework A to framework B.\n- Loop through tasks, calling Claude programmatically for each and giving it a task and a set of tools it can use. For example:\nclaude -p “migrate foo.py from React to Vue. When you are done, you MUST return the string OK if you succeeded, or FAIL if the task failed.” --allowedTools Edit Bash(git commit:*)\n- Run the script several times and refine your prompt to get the desired outcome. 2. Pipelining integrates Claude into existing data/processing pipelines:\n- Call\nclaude -p “<your prompt>” --json | your_command\n, whereyour_command\nis the next step of your processing pipeline - That’s it! JSON output (optional) can help provide structure for easier automated processing. For both of these use cases, it can be helpful to use the --verbose\nflag for debugging the Claude invocation. We generally recommend turning verbose mode off in production for cleaner output. What are your tips and best practices for working with Claude Code? Tag @AnthropicAI so we can see what you're building! Acknowledgements\nWritten by Boris Cherny. This work draws upon best practices from across the broader Claude Code user community, whose creative approaches and workflows continue to inspire us. Special thanks also to Daisy Hollman, Ashwin Bhat, Cat Wu, Sid Bidasaria, Cal Rueb, Nodir Turakulov, Barry Zhang, Drew Hodun and many other Anthropic engineers whose valuable insights and practical experience with Claude Code helped shape these recommendations.", "content_token_count": 279, "mistral_embedding": null}, {"position": 15, "content_text": "What are your tips and best practices for working with Claude Code? Tag @AnthropicAI so we can see what you're building! Acknowledgements\nWritten by Boris Cherny. This work draws upon best practices from across the broader Claude Code user community, whose creative approaches and workflows continue to inspire us. Special thanks also to Daisy Hollman, Ashwin Bhat, Cat Wu, Sid Bidasaria, Cal Rueb, Nodir Turakulov, Barry Zhang, Drew Hodun and many other Anthropic engineers whose valuable insights and practical experience with Claude Code helped shape these recommendations.", "content_token_count": 87, "mistral_embedding": null}, {"position": 16, "content_text": "Tag @AnthropicAI so we can see what you're building! Acknowledgements\nWritten by Boris Cherny. This work draws upon best practices from across the broader Claude Code user community, whose creative approaches and workflows continue to inspire us. Special thanks also to Daisy Hollman, Ashwin Bhat, Cat Wu, Sid Bidasaria, Cal Rueb, Nodir Turakulov, Barry Zhang, Drew Hodun and many other Anthropic engineers whose valuable insights and practical experience with Claude Code helped shape these recommendations.", "content_token_count": 75, "mistral_embedding": null}, {"position": 17, "content_text": "Acknowledgements\nWritten by Boris Cherny. This work draws upon best practices from across the broader Claude Code user community, whose creative approaches and workflows continue to inspire us. Special thanks also to Daisy Hollman, Ashwin Bhat, Cat Wu, Sid Bidasaria, Cal Rueb, Nodir Turakulov, Barry Zhang, Drew Hodun and many other Anthropic engineers whose valuable insights and practical experience with Claude Code helped shape these recommendations.", "content_token_count": 66, "mistral_embedding": null}, {"position": 18, "content_text": "This work draws upon best practices from across the broader Claude Code user community, whose creative approaches and workflows continue to inspire us. Special thanks also to Daisy Hollman, Ashwin Bhat, Cat Wu, Sid Bidasaria, Cal Rueb, Nodir Turakulov, Barry Zhang, Drew Hodun and many other Anthropic engineers whose valuable insights and practical experience with Claude Code helped shape these recommendations.", "content_token_count": 61, "mistral_embedding": null}, {"position": 19, "content_text": "Special thanks also to Daisy Hollman, Ashwin Bhat, Cat Wu, Sid Bidasaria, Cal Rueb, Nodir Turakulov, Barry Zhang, Drew Hodun and many other Anthropic engineers whose valuable insights and practical experience with Claude Code helped shape these recommendations.", "content_token_count": 38, "mistral_embedding": null}], "e23f5217-427c-4f5a-95a8-b1654525b345": [{"position": 0, "content_text": "Vision Language Models (VLMs) enable visual understanding alongside textual inputs. They are typically built by passing visual tokens from a pretrained vision encoder to a pretrained Large Language Model (LLM) through a projection layer. By leveraging the rich visual representations of the vision encoder and the world knowledge and reasoning capabilities of the LLM, VLMs can be useful for a wide range of applications, including accessibility assistants, UI navigation, robotics, and gaming. Update - September 22, 2025:\n- FastVLM models, including checkpoints for MLX and CoreML, are available on HuggingFace here.\n- HuggingFace has also shared a FastVLM demo that works in real-time directly in the browser, powered by transformers.js and WebGPU, available here. Videos of the demo in action are available in this thread.\n- MobileCLIP2 is now also available on HuggingFace, here. MobileCLIP2 models are a family of image-text models that can be used as the image encoder in fast VLM models such as FastVLM. For more detail, please see the paper: MobileCLIP2: Improving Multi-Modal Reinforced Training, which has been accepted to Transactions on Machine Learning Research with Featured certification. VLM accuracy generally improves with higher input image resolution, creating a tradeoff between accuracy and efficiency. For many production use-cases, VLMs need to be both accurate and efficient to meet the low-latency demands of real-time applications and run on-device for privacy-preserving AI experiences. In a paper accepted to CVPR 2025, Apple ML researchers recently shared a new technique to address this challenge: FastVLM, a new type of VLM that significantly improves accuracy-latency trade-offs with a simple design. Leveraging a hybrid architecture visual encoder designed for high-resolution images, FastVLM delivers accurate, fast, and efficient visual query processing, making it suitable for powering real-time applications on-device. The inference code, model checkpoints, and an iOS/macOS demo app based on MLX are available here. Image Resolution and the Accuracy-Latency Tradeoff\nGenerally, VLM accuracy improves with higher image resolution, especially for tasks needing detailed understanding, such as document analysis, UI recognition, or answering natural language queries about images. For example, in Figure 1 below, we ask our VLM about the street sign visible in the image. On the left, the model receives a low-resolution image and cannot respond correctly. On the right, the VLM receives a high-resolution image and correctly identifies the traffic sign which is a “Do Not Enter”. High resolution significantly increases time-to-first-token in VLMs.", "content_token_count": 394, "mistral_embedding": null}, {"position": 1, "content_text": "Image Resolution and the Accuracy-Latency Tradeoff\nGenerally, VLM accuracy improves with higher image resolution, especially for tasks needing detailed understanding, such as document analysis, UI recognition, or answering natural language queries about images. For example, in Figure 1 below, we ask our VLM about the street sign visible in the image. On the left, the model receives a low-resolution image and cannot respond correctly. On the right, the VLM receives a high-resolution image and correctly identifies the traffic sign which is a “Do Not Enter”. High resolution significantly increases time-to-first-token in VLMs. While using high-resolution images improves accuracy, it also reduces efficiency in two ways: 1) higher resolution images take longer for the vision encoder to process, and 2) the encoder creates more visual tokens, which increases the pre-filling time for the LLM. Both factors increase the time-to-first-token (TTFT), which is the sum of vision encoding time and LLM pre-filling time. As shown in Figure 2 below, both vision encoding and LLM pre-filling times grow as image resolution increases, and at high resolutions, vision encoder latency becomes the dominant bottleneck. To address this, our research introduces FastVLM, a new vision language model that significantly improves efficiency without sacrificing accuracy. Hybrid Vision Encoders Deliver the Best Accuracy-Latency Tradeoff\nTo identify which architecture delivers the best accuracy-latency tradeoff, we systematically compared existing pre-trained vision encoders with an experiment in which everything (training data, recipe, LLM, etc.) was kept the same, and only the vision encoder was changed. In Figure 3 below, the x-axis shows TTFT, and the y-axis shows the average accuracy across different VLM tasks. We show two points for popular transformer-based encoders, ViT-L/14and SigLIP-SO400, pre-trained on image-text data at their native resolutions. We also show curves for ConvNeXT(fully convolutional encoder) and FastViT (a hybrid encoder combining convolutional and transformer blocks) at various resolutions. FastViT, which is based on two of our previous works (FastViT, ICCV 2023; and MobileCLIP, CVPR 2024), achieves the best accuracy-latency trade-off compared to other vision encoders—about 8 times smaller and 20 times faster than ViT-L/14. FastViTHD: An Optimal Vision Encoder for VLMs\nWhile the FastViT hybrid backbone is a great choice for efficient VLMs, larger vision encoders are needed for improved accuracy on challenging tasks. Initially, we simply increased the size of each FastViT layer. However, this naive scaling made FastViT even less efficient than fully convolutional encoders at higher resolutions.", "content_token_count": 395, "mistral_embedding": null}, {"position": 2, "content_text": "FastViT, which is based on two of our previous works (FastViT, ICCV 2023; and MobileCLIP, CVPR 2024), achieves the best accuracy-latency trade-off compared to other vision encoders—about 8 times smaller and 20 times faster than ViT-L/14. FastViTHD: An Optimal Vision Encoder for VLMs\nWhile the FastViT hybrid backbone is a great choice for efficient VLMs, larger vision encoders are needed for improved accuracy on challenging tasks. Initially, we simply increased the size of each FastViT layer. However, this naive scaling made FastViT even less efficient than fully convolutional encoders at higher resolutions. To address this, we designed a new backbone, FastViTHD, specifically for high-resolution images. FastViTHD includes an extra stage compared to FastViT and is pre-trained using the MobileCLIP recipe to produce fewer but higher-quality visual tokens. FastViTHD has better latency at high resolution images compared to FastViT, but to evaluate which is best in a VLM, we compared their performance when combined with LLMs of various sizes. We evaluated different pairs of (image resolution, LLM size), and three LLMs with 0.5B, 1.5B, and 7B parameters (corresponding to each curve in Figure 4 below) and pair it with vision backbone running at different resolutions. As shown in Figure 4, using very high resolution images with a small LLM is not always the optimal choice; sometimes it is better to switch the LLM to a larger one instead of increasing the resolution. For each case, we show the Pareto-optimal curve with dashed lines, which shows the optimal (image resolution, LLM size) for a given runtime budget (TTFT here). Comparing Pareto-optimal curves, FastVLM (based on FastViTHD) offers a much better accuracy-latency trade-off than the FastViT-based model. It can be up to 3x faster for the same accuracy. Note that we had already shown that FastViT is significantly better than purely transformer-based or convolutional-based encoders. FastVLM: a New VLM Based on FastViTHD\nFastViTHD is a hybrid convolutional-transformer architecture comprising a convolutional stem, three convolutional stages, and two subsequent stages of transformer blocks. Each stage is preceded by a patch embedding layer that reduces the spatial dimensions of the input tensor by a factor of two. Using FastViTHD as the vision encoder, we built FastVLM, with a simple Multi-Layer Perceptron (MLP) module to project visual tokens to the embedding space of LLM, as shown in Figure 5.", "content_token_count": 383, "mistral_embedding": null}, {"position": 3, "content_text": "FastVLM: a New VLM Based on FastViTHD\nFastViTHD is a hybrid convolutional-transformer architecture comprising a convolutional stem, three convolutional stages, and two subsequent stages of transformer blocks. Each stage is preceded by a patch embedding layer that reduces the spatial dimensions of the input tensor by a factor of two. Using FastViTHD as the vision encoder, we built FastVLM, with a simple Multi-Layer Perceptron (MLP) module to project visual tokens to the embedding space of LLM, as shown in Figure 5. FastVLM Outperforms Token Pruning and Merging Methods\nPrior research works in accelerating VLMs have employed complex merging or pruning techniques to reduce visual token counts to speed up LLM prefilling (and thus reduce the time to first token). As shown in Figure 6 below, FastVLM achieves higher overall accuracy across different visual token counts (corresponding to different input resolutions) compared to these approaches. This is due to the high-quality visual tokens from its FastViTHD encoder, and because FastVLM does not require complicated token pruning or merging, it is simpler to deploy. FastVLM and Dynamic Tiling\nAs noted earlier, VLM accuracy increases with input resolution, particularly for tasks requiring understanding fine-grain details. Dynamic tiling (for example, in AnyRes) is a popular way to handle very high-resolution images. This approach divides an image into smaller tiles, processes each tile separately through the vision encoder, and then sends all tokens to the LLM, as in Figure 7 shown below. Since FastVLM naturally handles high-resolution images, we explored if combining FastVLM with dynamic tiling improves its accuracy-latency tradeoff. Figure 8 below shows that FastVLM without tiling (blue curve) achieves a better accuracy-latency trade-off compared to dynamic tiling (pink points), up to very high image resolutions, at which point combing FastVLM and AnyRes can be beneficial. FastVLM is Faster and More Accurate Than Popular VLMs of the Same Size\nFinally, we compared FastVLM with other popular VLMs. In Figure 9 below, we show two curves for FastVLM: one with AnyRes (to achieve the highest accuracy) and one without tiling (for the best accuracy-latency tradeoff), each tested with three different LLM sizes. FastVLM is significantly faster and more accurate than popular models of the same size as indicated by the arrows: it is 85x faster than LLava-OneVision(0.5B LLM), 5.2x faster than SmolVLM(~0.5B LLM), and 21x faster than Cambrian-1(7B LLM).", "content_token_count": 384, "mistral_embedding": null}, {"position": 4, "content_text": "FastVLM is Faster and More Accurate Than Popular VLMs of the Same Size\nFinally, we compared FastVLM with other popular VLMs. In Figure 9 below, we show two curves for FastVLM: one with AnyRes (to achieve the highest accuracy) and one without tiling (for the best accuracy-latency tradeoff), each tested with three different LLM sizes. FastVLM is significantly faster and more accurate than popular models of the same size as indicated by the arrows: it is 85x faster than LLava-OneVision(0.5B LLM), 5.2x faster than SmolVLM(~0.5B LLM), and 21x faster than Cambrian-1(7B LLM). To further show the on-device efficiency of FastVLM, we released an iOS/macOS demo app based on MLX. Figure 10 shows examples of FastVLM running locally on an iPhone GPU. FastVLM’s near real-time performance can enable new on-device features and experiences. Conclusion\nBy combining visual and textual understanding, VLMs can power a range of useful applications. Because the accuracy of these models generally corresponds to the resolution of input images, there has often been a performance tradeoff between accuracy and efficiency, which has limited the value of VLMs for applications that require both high accuracy and great efficiency. FastVLM addresses this tradeoff by leveraging a hybrid-architecture vision encoder built for high-resolution images, FastViTHD. With a simple design, FastVLM outperforms prior approaches in both accuracy and efficiency, enabling on-device visual query processing suitable for real-time on-device applications. Related readings and updates. FastVLM: Efficient Vision encoding for Vision Language Models\nApril 18, 2025research area Computer Visionconference CVPR\nScaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing…\nA Multi-Task Neural Architecture for On-Device Scene Analysis\nJune 7, 2022research area Computer Vision, research area Methods and Algorithms\nScene analysis is an integral core technology that powers many features and experiences in the Apple ecosystem. From visual content search to powerful memories marking special occasions in one’s life, outputs (or “signals”) produced by scene analysis are critical to how users interface with the photos on their devices.", "content_token_count": 380, "mistral_embedding": null}, {"position": 5, "content_text": "At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing…\nA Multi-Task Neural Architecture for On-Device Scene Analysis\nJune 7, 2022research area Computer Vision, research area Methods and Algorithms\nScene analysis is an integral core technology that powers many features and experiences in the Apple ecosystem. From visual content search to powerful memories marking special occasions in one’s life, outputs (or “signals”) produced by scene analysis are critical to how users interface with the photos on their devices. Deploying dedicated models for each of these individual features is inefficient as many of these models can benefit from sharing resources. We present how we developed Apple Neural Scene Analyzer (ANSA), a unified backbone to build and maintain scene analysis workflows in production. This was an important step towards enabling Apple to be among the first in the industry to deploy fully client-side scene analysis in 2016.", "content_token_count": 156, "mistral_embedding": null}, {"position": 6, "content_text": "From visual content search to powerful memories marking special occasions in one’s life, outputs (or “signals”) produced by scene analysis are critical to how users interface with the photos on their devices. Deploying dedicated models for each of these individual features is inefficient as many of these models can benefit from sharing resources. We present how we developed Apple Neural Scene Analyzer (ANSA), a unified backbone to build and maintain scene analysis workflows in production. This was an important step towards enabling Apple to be among the first in the industry to deploy fully client-side scene analysis in 2016.", "content_token_count": 99, "mistral_embedding": null}, {"position": 7, "content_text": "Deploying dedicated models for each of these individual features is inefficient as many of these models can benefit from sharing resources. We present how we developed Apple Neural Scene Analyzer (ANSA), a unified backbone to build and maintain scene analysis workflows in production. This was an important step towards enabling Apple to be among the first in the industry to deploy fully client-side scene analysis in 2016.", "content_token_count": 67, "mistral_embedding": null}, {"position": 8, "content_text": "We present how we developed Apple Neural Scene Analyzer (ANSA), a unified backbone to build and maintain scene analysis workflows in production. This was an important step towards enabling Apple to be among the first in the industry to deploy fully client-side scene analysis in 2016.", "content_token_count": 46, "mistral_embedding": null}, {"position": 9, "content_text": "This was an important step towards enabling Apple to be among the first in the industry to deploy fully client-side scene analysis in 2016.", "content_token_count": 24, "mistral_embedding": null}], "d7915f1b-d0de-4b94-b3ee-b786204e8aff": [{"position": 0, "content_text": "Codestral Embed\nThe new state-of-the-art embedding model for code. We are excited to release Codestral Embed, our first embedding model specialized for code. It performs especially well for retrieval use cases on real-world code data. Codestral Embed significantly outperforms leading code embedders in the market today: Voyage Code 3, Cohere Embed v4.0 and OpenAI’s large embedding model. Codestral Embed can output embeddings with different dimensions and precisions, and the figure below illustrates the trade-offs between retrieval quality and storage costs. Codestral Embed with dimension 256 and int8 precision still performs better than any model from our competitors. The dimensions of our embeddings are ordered by relevance. For any integer target dimension n, you can choose to keep the first n dimensions for a smooth trade-off between quality and cost. Results\nBelow we show the performance of Codestral Embed for several categories. The details of the benchmarks corresponding to each category can be found in the table in the “Benchmarks details” section. SWE-Bench is based on a dataset of real-world GitHub issues and corresponding fixes, and is especially relevant for retrieval-augmented generation for coding agents. Text2Code (GitHub) contains benchmarks relevant for giving context for code completion or edition. We believe that these two categories are especially relevant to code assistants. Use cases\nCodestral Embed is optimized for high-performance code retrieval and semantic understanding. It enables a range of practical applications across development workflows, especially when working with large-scale code corpora. 1. Retrieval-augmented generation\nCodestral Embed facilitates rapid and efficient context retrieval for code completion, editing, or explanation tasks. It is ideal for AI-powered software engineering in copilots or coding agent frameworks. 2. Semantic code search\nEmbed enables accurate search of relevant code snippets from natural language or code queries. It is suitable for use within developer tools, documentation systems, and copilots. 3. Similarity search and duplicate detection\nThe model’s embeddings can be used to identify near-duplicate or functionally similar code segments, even with significant lexical variation. This supports use cases such as identifying reusable code to avoid duplicates, or detecting copy-paste reuse to enforce licensing policies. 4. Semantic clustering and code analytics\nCodestral Embed supports unsupervised grouping of code based on functionality or structure. This is useful for analyzing repository composition, identifying emergent architecture patterns, or feeding into automated documentation and categorization systems.", "content_token_count": 382, "mistral_embedding": null}, {"position": 1, "content_text": "Similarity search and duplicate detection\nThe model’s embeddings can be used to identify near-duplicate or functionally similar code segments, even with significant lexical variation. This supports use cases such as identifying reusable code to avoid duplicates, or detecting copy-paste reuse to enforce licensing policies. 4. Semantic clustering and code analytics\nCodestral Embed supports unsupervised grouping of code based on functionality or structure. This is useful for analyzing repository composition, identifying emergent architecture patterns, or feeding into automated documentation and categorization systems. Availability\nCodestral Embed is available on our API under the name `codestral-embed-2505` at a price of $0.15 per million tokens. It is also available on our batch API at a 50% discount. For on-prem deployments, please contact us to connect with our applied AI team. Please check our docs to get started and our cookbook for examples of how to use Codestral Embed for code agent retrieval. Chunking parameters\nFor retrieval use cases, while you can use the full context size of 8192 tokens, it is often more efficient to chunk your dataset. We recommend using chunks of 3000 characters with 1000 characters overlap. Larger chunks tend to adversely affect the performance of the retrieval system. Refer to our cookbook for more information about chunking. Benchmark details\nYou can find the details of the benchmarks that we used to evaluate our model in the table below. We report the average score per category, and the macro average (average of the scores of each category).\n| Benchmark | Description | Category |\n|---|---|---|\n| SWE-Bench lite | Examples from SWE-Bench lite: given real github issues, retrieve the files that should be modified to fix the issue from the given state of the repository.", "content_token_count": 283, "mistral_embedding": null}, {"position": 2, "content_text": "Refer to our cookbook for more information about chunking. Benchmark details\nYou can find the details of the benchmarks that we used to evaluate our model in the table below. We report the average score per category, and the macro average (average of the scores of each category).\n| Benchmark | Description | Category |\n|---|---|---|\n| SWE-Bench lite | Examples from SWE-Bench lite: given real github issues, retrieve the files that should be modified to fix the issue from the given state of the repository. Most relevant for code agent RAG. |\nswebench_lite |\n|\nCodeSearchNet Code -> Code |\nGiven real-world code from GitHub, retrieve the code that appears in the same context |\ncode2code |\n|\nCodeSearchNet doc2code |\nGiven a docstring from real-world GitHub code, retrieve the corresponding code |\nText2code (github) |\n|\nCommitPack |\nGiven a commit message from real-world GitHub code, retrieve the corresponding modified files |\nText2code (github) |\n|\nSpider |\nRetrieve SQL code given a query |\nText2SQL |\n|\nWikiSQL |\nRetrieve SQL code given a query |\nText2SQL |\n|\nSynthetic Text2SQL |\nRetrieve SQL code given a query |\nText2SQL |\n|\nDM code contests |\nMatch problem descriptions to correct solutions for programming competition websites (corpus is correct + incorrect solutions for each problem). |\nText2Code (Algorithms) |\n|\nAPPS |\nMatch problem descriptions to solutions for programming competition websites. |\nText2Code (Algorithms) |\n|\nCodeChef |\nMatch problem descriptions to solutions for programming competition websites. |\nText2Code (Algorithms) |\n|\nMBPP+ |\nMatch algorithmic questions to solutions for mostly basic python programs |\nText2Code (Algorithms) |\n|\nDS 1000 |\nMatch data science questions to implementations |\nText2Code (Data Science) |", "content_token_count": 286, "mistral_embedding": null}, {"position": 3, "content_text": "Most relevant for code agent RAG. |\nswebench_lite |\n|\nCodeSearchNet Code -> Code |\nGiven real-world code from GitHub, retrieve the code that appears in the same context |\ncode2code |\n|\nCodeSearchNet doc2code |\nGiven a docstring from real-world GitHub code, retrieve the corresponding code |\nText2code (github) |\n|\nCommitPack |\nGiven a commit message from real-world GitHub code, retrieve the corresponding modified files |\nText2code (github) |\n|\nSpider |\nRetrieve SQL code given a query |\nText2SQL |\n|\nWikiSQL |\nRetrieve SQL code given a query |\nText2SQL |\n|\nSynthetic Text2SQL |\nRetrieve SQL code given a query |\nText2SQL |\n|\nDM code contests |\nMatch problem descriptions to correct solutions for programming competition websites (corpus is correct + incorrect solutions for each problem). |\nText2Code (Algorithms) |\n|\nAPPS |\nMatch problem descriptions to solutions for programming competition websites. |\nText2Code (Algorithms) |\n|\nCodeChef |\nMatch problem descriptions to solutions for programming competition websites. |\nText2Code (Algorithms) |\n|\nMBPP+ |\nMatch algorithmic questions to solutions for mostly basic python programs |\nText2Code (Algorithms) |\n|\nDS 1000 |\nMatch data science questions to implementations |\nText2Code (Data Science) |", "content_token_count": 200, "mistral_embedding": null}], "ec2c40d8-ea1d-4280-9fb9-e8771424083f": [{"position": 0, "content_text": "Computer Science > Artificial Intelligence\nTitle:AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges\nView PDF HTML (experimental)Abstract:This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven by Large Language Models (LLMs) and Large Image Models (LIMs) for narrow, task-specific automation. Generative AI is positioned as a precursor, with AI Agents advancing through tool integration, prompt engineering, and reasoning enhancements. In contrast, Agentic AI systems represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Through a sequential evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both paradigms. Application domains such as customer support, scheduling, and data summarization are contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions such as ReAct loops, RAG, orchestration layers, and causal modeling. This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision Support System, Agentic-AI Applications\nSubmission history\nFrom: Ranjan Sapkota [view email][v1] Thu, 15 May 2025 16:21:33 UTC (13,055 KB)\n[v2] Fri, 16 May 2025 23:31:18 UTC (13,058 KB)\n[v3] Tue, 20 May 2025 04:49:56 UTC (13,059 KB)\n[v4] Wed, 28 May 2025 01:28:08 UTC (13,076 KB)\n[v5] Tue, 30 Sep 2025 04:21:32 UTC (5,973 KB)\nBibliographic and Citation Tools\nCode, Data and Media Associated with this Article\nDemos\nRecommenders and Search Tools\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.", "content_token_count": 372, "mistral_embedding": null}, {"position": 1, "content_text": "This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision Support System, Agentic-AI Applications\nSubmission history\nFrom: Ranjan Sapkota [view email][v1] Thu, 15 May 2025 16:21:33 UTC (13,055 KB)\n[v2] Fri, 16 May 2025 23:31:18 UTC (13,058 KB)\n[v3] Tue, 20 May 2025 04:49:56 UTC (13,059 KB)\n[v4] Wed, 28 May 2025 01:28:08 UTC (13,076 KB)\n[v5] Tue, 30 Sep 2025 04:21:32 UTC (5,973 KB)\nBibliographic and Citation Tools\nCode, Data and Media Associated with this Article\nDemos\nRecommenders and Search Tools\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.", "content_token_count": 177, "mistral_embedding": null}, {"position": 2, "content_text": "Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.", "content_token_count": 54, "mistral_embedding": null}, {"position": 3, "content_text": "Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.", "content_token_count": 17, "mistral_embedding": null}, {"position": 4, "content_text": "Learn more about arXivLabs.", "content_token_count": 4, "mistral_embedding": null}], "cfc80b7d-7d6e-49ae-b55f-b3ef1ce8ac77": [{"position": 0, "content_text": "Announcing Codestral 25.08 and the Complete Mistral Coding Stack for Enterprise\nHow the world’s leading enterprises are using integrated coding solutions from Mistral AI to cut development, review, and testing time by 50%—and why the playbook now fits every company that wants AI-native software development. AI-powered coding is taking off, but enterprise adoption still lags due to critical limitations\nOver the past year, AI coding assistants have introduced powerful capabilities, such as multi-file reasoning, contextual suggestions, and natural-language agents, all directly within the IDE. Despite these improvements, however, adoption inside enterprise environments has been slow. The reasons have less to do with model performance or the interface, and more with how these tools are built, deployed, and governed. Key limitations holding back enterprise teams include:\n-\nDeployment constraints: Most AI coding tools are SaaS-only, with no options for VPC, on-prem, or air-gapped environments. This is a hard blocker for organizations in finance, defense, healthcare, and other regulated industries.\n-\nLimited customization: Enterprises often need to adapt models to their own codebases and development conventions. Without access to model weights, pos-training workflows, or extensibility, teams are locked out of leveraging the best of their codebases.\n-\nFragmented architecture: Agents, embeddings, completions, and plugins are frequently decoupled across vendors—leading to integration drift, inconsistent context handling, and operational overhead. Moreover, coding copilots are not well-integrated into full enterprise platforms, such as product development tools, CRMs, and customer issue trackers.\n-\nNo unified observability or control: Teams lack visibility into how AI is being used across the development lifecycle. Without telemetry, audit trails, and centralized controls, it’s difficult to scale AI usage responsibly or measure real ROI.\n-\nIncompatibility with internal toolchains: Many assistants operate in closed environments, making it hard to connect with internal CI/CD pipelines, knowledge bases, or static analysis frameworks. For enterprises, these limitations aren’t edge cases—they’re baseline requirements. Solving them is what separates a good developer tool from an AI-native software development platform. A Full-Stack Approach Built for AI-Native Software Development\nOur approach to enterprise coding isn’t a bundle of isolated tools. It’s an integrated system designed to support enterprise-grade software development across every stage—from code suggestion to autonomous pull requests. It starts with fast, reliable completion—and scales up to full codebase understanding and multi-file automation. 1.", "content_token_count": 377, "mistral_embedding": null}, {"position": 1, "content_text": "Without telemetry, audit trails, and centralized controls, it’s difficult to scale AI usage responsibly or measure real ROI.\n-\nIncompatibility with internal toolchains: Many assistants operate in closed environments, making it hard to connect with internal CI/CD pipelines, knowledge bases, or static analysis frameworks. For enterprises, these limitations aren’t edge cases—they’re baseline requirements. Solving them is what separates a good developer tool from an AI-native software development platform. A Full-Stack Approach Built for AI-Native Software Development\nOur approach to enterprise coding isn’t a bundle of isolated tools. It’s an integrated system designed to support enterprise-grade software development across every stage—from code suggestion to autonomous pull requests. It starts with fast, reliable completion—and scales up to full codebase understanding and multi-file automation. 1. Fast, High-Fidelity Code Completion\nAt the foundation of the stack is Codestral, Mistral’s family of code generation models built specifically for high-precision fill-in-the-middle (FIM) completion. These models are optimized for production engineering environments: latency-sensitive, context-aware, and self-deployable. Today, we announce its latest update. Codestral 25.08 delivers measurable upgrades over prior versions:\n-\n+30% increase in accepted completions\n-\n+10% more retained code after suggestion\n-\n50% fewer runaway generations, improving confidence in longer edits\n-\nImproved performance on academic benchmarks for short and long-context FIM completion\nThese improvements were validated in live IDE usage across production codebases. The model supports a wide range of languages and tasks, and is deployable across cloud, VPC, or on-prem environments—with no architectural changes required. Codestral-2508 also brings improvements to chat mode:\n-\nInstruction following: +5% on IF eval v8\n-\nCode abilities: +5% in average MultiplE\n2. Codebase-Scale Search and Semantic Retrieval\nAutocomplete accelerates, but only if the model understands your codebase. Codestral Embed sets a new standard in this domain. Designed specifically for code rather than general text, it outperforms leading embedding models from OpenAI and Cohere in real-world code retrieval benchmarks. Key advantages include:\n-\nHigh-recall, low-latency search across massive monorepos and poly-repos. Developers can find internal logic, validation routines, or domain-specific utilities using natural language.\n-\nFlexible embedding outputs, with configurable dimensions (e.g., 256-dim, INT8) that balance retrieval quality with storage efficiency—while outperforming alternatives even at lower dimensionality\n-\nPrivate deployment for maximum control, ensuring no data leakage via third-party APIs.", "content_token_count": 372, "mistral_embedding": null}, {"position": 2, "content_text": "Designed specifically for code rather than general text, it outperforms leading embedding models from OpenAI and Cohere in real-world code retrieval benchmarks. Key advantages include:\n-\nHigh-recall, low-latency search across massive monorepos and poly-repos. Developers can find internal logic, validation routines, or domain-specific utilities using natural language.\n-\nFlexible embedding outputs, with configurable dimensions (e.g., 256-dim, INT8) that balance retrieval quality with storage efficiency—while outperforming alternatives even at lower dimensionality\n-\nPrivate deployment for maximum control, ensuring no data leakage via third-party APIs. All embedding inference and index storage can run within enterprise infrastructure\nThis embedding layer serves as both the context foundation for agentic workflows and the retrieval engine powering in‑IDE code search features—without sacrificing privacy, performance, or precision. 3. Autonomous Multi-Step Development with Agentic Workflows\nWith relevant context surfaced, AI can take meaningful action. Devstral, powered by the OpenHands agent scaffold, enables enterprise-ready agentic coding workflows. It’s built specifically for engineering tasks—cross-file refactors, test generation, and PR authoring—using structured, context-rich reasoning. Standout capabilities include:\n-\nTop open‑model performance on SWE‑Bench Verified: Devstral Small 1.1 scores 53.6%, and Devstral Medium reaches 61.6%, outperforming Claude 3.5, GPT‑4.1‑mini, and other open models by wide margins\n-\nFlexible architecture for any environment: Devstral is available in multiple sizes. The open-weight Devstral Small (24B, Apache-2.0) runs efficiently on a single Nvidia RTX 4090 or Mac with 32 GB RAM—ideal for self-hosted, air-gapped, or experimental workflows. The larger Devstral Medium is available through enterprise partnerships and our API for more advanced code understanding and planning capabilities.\n-\nOpen model for extensibility: Teams can fine-tune Devstral Small on proprietary code, build custom agents, or embed it directly into CI/CD workflows—without licensing lock-in. For production environments requiring higher model performance, Devstral Medium is available with enterprise-grade support, including the ability for companies to post-train and fine-tune. Delivering agentic automation within private infrastructure lets engineering organizations reduce friction, ensure compliance, and speed up delivery with repeatable, auditable AI workflows. 4. IDE Integration and Operational Control\nAll capabilities in the Mistral stack—completion, semantic search, and agentic workflows—are surfaced through Mistral Code, a native plugin for JetBrains and VS Code.", "content_token_count": 351, "mistral_embedding": null}, {"position": 3, "content_text": "The larger Devstral Medium is available through enterprise partnerships and our API for more advanced code understanding and planning capabilities.\n-\nOpen model for extensibility: Teams can fine-tune Devstral Small on proprietary code, build custom agents, or embed it directly into CI/CD workflows—without licensing lock-in. For production environments requiring higher model performance, Devstral Medium is available with enterprise-grade support, including the ability for companies to post-train and fine-tune. Delivering agentic automation within private infrastructure lets engineering organizations reduce friction, ensure compliance, and speed up delivery with repeatable, auditable AI workflows. 4. IDE Integration and Operational Control\nAll capabilities in the Mistral stack—completion, semantic search, and agentic workflows—are surfaced through Mistral Code, a native plugin for JetBrains and VS Code. It provides:\n-\nInline completions using Codestral 25.08, optimized for FIM and multi-line editing\n-\nOne-click task automations like “Write commit message”, “Fix function”, or “Add docstring”, powered by Devstral\n-\nContext awareness from Git diffs, terminal history, and static analysis tools\n-\nIntegrated semantic search, backed by Codestral Embed\nMistral Code is built to support enterprise deployment requirements:\n-\nDeploy in any environment: cloud, self-managed VPC, or fully on-prem (GA in Q3)\n-\nNo mandatory telemetry, and no external API calls for inference or search\n-\nSSO, audit logging, and usage controls for secure, policy-compliant adoption\n-\nUsage observability via the Mistral Console, including metrics on AI-generated code, suggestion acceptance, and agent usage\nThese features give engineering, platform, and security teams the ability to roll out AI tooling safely, incrementally, and with full visibility. How It All Fits Together: From Developer Actions to Organizational Impact\nThe Mistral coding stack integrates autocomplete, semantic retrieval, and agentic workflows directly into the IDE—while giving platform teams control over deployment, observability, and security. In a typical development task:\nSay a developer is working on a payments service written in Python. A recent update to a third-party billing API means they need to update the integration logic and add proper error handling.\n-\nThey start by navigating to the billing handler. As they modify the function signature, Codestral fills in the expected parameters and suggests a first-pass implementation, reducing the need to copy patterns from other services.\n-\nBefore changing the retry logic, they need to understand how similar failures are handled elsewhere.", "content_token_count": 378, "mistral_embedding": null}, {"position": 4, "content_text": "In a typical development task:\nSay a developer is working on a payments service written in Python. A recent update to a third-party billing API means they need to update the integration logic and add proper error handling.\n-\nThey start by navigating to the billing handler. As they modify the function signature, Codestral fills in the expected parameters and suggests a first-pass implementation, reducing the need to copy patterns from other services.\n-\nBefore changing the retry logic, they need to understand how similar failures are handled elsewhere. Instead of switching to Slack or searching GitHub manually, they enter a query directly in the IDE: “How do we handle Stripe timeouts in the checkout flow?” The embedding index, running locally, returns a helper module from another service that wraps retry logic with exponential backoff.\n-\nThey copy the pattern into their own handler—but realize three other services are using outdated retry code. They invoke a Devstral-powered agent from within the IDE: “Replace all uses of retry_with_sleep in the billing and checkout services with the new retry_exponential helper, and update the docs.” Devstral scans the codebase using the same embeddings, makes the required edits across files, and generates a draft PR. The agent also writes a changelog and updates the README section on error handling. The developer reviews the PR, confirms the logic, and merges it. A cross-service update that previously would have required search, coordination, and hand-written boilerplate now completes in one editing session—with traceable, reviewable output. At the organization level, this same workflow unlocks broader advantages:\n-\nEvery component in the stack can be self-hosted or run on-prem, giving teams control over data, latency, and deployment architecture.\n-\nObservability is built in. The Mistral Console tracks usage patterns, model acceptance rates, and agent adoption, providing the data needed to tune rollout and measure ROI.\n-\nSecurity and compliance controls—including SSO, audit logging, and telemetry configuration—make it easy to integrate with internal policies and infrastructure.\n-\nNo stitching required. Because completion, search, and agents share architecture, context handling, and support boundaries, teams avoid the drift, overhead, and security gaps of piecing together third-party tools. The result is a development workflow that’s both faster and easier to govern—designed for individual productivity and organizational scale.", "content_token_count": 374, "mistral_embedding": null}, {"position": 5, "content_text": "The Mistral Console tracks usage patterns, model acceptance rates, and agent adoption, providing the data needed to tune rollout and measure ROI.\n-\nSecurity and compliance controls—including SSO, audit logging, and telemetry configuration—make it easy to integrate with internal policies and infrastructure.\n-\nNo stitching required. Because completion, search, and agents share architecture, context handling, and support boundaries, teams avoid the drift, overhead, and security gaps of piecing together third-party tools. The result is a development workflow that’s both faster and easier to govern—designed for individual productivity and organizational scale. Adopted by Leading Enterprises Across Diverse Environments\nThe Mistral coding stack is already being used in production by organizations across consulting, finance, transportation, and industry—each with different requirements, but shared constraints around data control, deployment flexibility, and internal code complexity.\n-\nCapgemini has rolled out the stack across global delivery teams to accelerate development while maintaining code ownership and compliance across clients in defense, telecom, and energy.\n-\nAbanca, a leading bank in Spain operating under European banking regulations, uses Mistral’s models in a fully self-hosted deployment to meet data residency and network isolation requirements—without sacrificing usability.\n-\nSNCF, the French national railway company, uses agentic workflows to modernize legacy Java systems safely and incrementally, with human oversight built into the loop.\n“Leveraging Mistral’s Codestral has been a game changer in the adoption of private coding assistant for our client projects in regulated industries. We have evolved from basic support for some development activities to systematic value for our development teams“. Alban Alev, VP head of Solutioning at Capgemini France. In addition, several tier-1 global banks and industrial manufacturers are actively piloting or scaling adoption across their engineering teams—driven by requirements that hosted copilots and fragmented tooling can’t support. These use cases reflect a growing shift: organizations are no longer looking for isolated assistants—they’re adopting integrated AI systems that match the complexity, security posture, and velocity of modern enterprise software development. Get Started\nThe full Mistral coding stack—Codestral 25.08, Devstral, Codestral Embed, and the Mistral Code IDE extension—is available today for enterprise deployment. Teams can start with autocomplete and semantic search, then expand to agentic workflows and private deployments at their own pace.", "content_token_count": 363, "mistral_embedding": null}, {"position": 6, "content_text": "In addition, several tier-1 global banks and industrial manufacturers are actively piloting or scaling adoption across their engineering teams—driven by requirements that hosted copilots and fragmented tooling can’t support. These use cases reflect a growing shift: organizations are no longer looking for isolated assistants—they’re adopting integrated AI systems that match the complexity, security posture, and velocity of modern enterprise software development. Get Started\nThe full Mistral coding stack—Codestral 25.08, Devstral, Codestral Embed, and the Mistral Code IDE extension—is available today for enterprise deployment. Teams can start with autocomplete and semantic search, then expand to agentic workflows and private deployments at their own pace. To begin:\n-\nInstall Mistral Code from the JetBrains or VS Code marketplace\n-\nConnect to your preferred deployment modality (cloud, VPC, or on-prem)\nIf you would like to use the models for your own copilot, get your keys at console.mistral.ai. For more information on Mistral’s coding solutions, please visit our website and documentation. To evaluate on-prem options, enterprise-scale deployments, or schedule a hands-on pilot, fill out the demand form on this page. A member of the Mistral team will follow up to help tailor the rollout to your environment. Get in touch. Explore Codestral and Mistral Code.", "content_token_count": 201, "mistral_embedding": null}, {"position": 7, "content_text": "To begin:\n-\nInstall Mistral Code from the JetBrains or VS Code marketplace\n-\nConnect to your preferred deployment modality (cloud, VPC, or on-prem)\nIf you would like to use the models for your own copilot, get your keys at console.mistral.ai. For more information on Mistral’s coding solutions, please visit our website and documentation. To evaluate on-prem options, enterprise-scale deployments, or schedule a hands-on pilot, fill out the demand form on this page. A member of the Mistral team will follow up to help tailor the rollout to your environment. Get in touch. Explore Codestral and Mistral Code.", "content_token_count": 98, "mistral_embedding": null}, {"position": 8, "content_text": "For more information on Mistral’s coding solutions, please visit our website and documentation. To evaluate on-prem options, enterprise-scale deployments, or schedule a hands-on pilot, fill out the demand form on this page. A member of the Mistral team will follow up to help tailor the rollout to your environment. Get in touch. Explore Codestral and Mistral Code.", "content_token_count": 57, "mistral_embedding": null}, {"position": 9, "content_text": "To evaluate on-prem options, enterprise-scale deployments, or schedule a hands-on pilot, fill out the demand form on this page. A member of the Mistral team will follow up to help tailor the rollout to your environment. Get in touch. Explore Codestral and Mistral Code.", "content_token_count": 44, "mistral_embedding": null}, {"position": 10, "content_text": "A member of the Mistral team will follow up to help tailor the rollout to your environment. Get in touch. Explore Codestral and Mistral Code.", "content_token_count": 25, "mistral_embedding": null}, {"position": 11, "content_text": "Get in touch. Explore Codestral and Mistral Code.", "content_token_count": 8, "mistral_embedding": null}, {"position": 12, "content_text": "Explore Codestral and Mistral Code.", "content_token_count": 5, "mistral_embedding": null}], "9b305d74-baed-43dd-b9cd-409848d1f3a6": [{"position": 0, "content_text": "- Published\nThe global calendar of Olympic sports may need to be \"re-engineered\" amid the challenges posed by climate change after athletics' biggest stars had to contend with searing temperatures in Tokyo, World Athletics president Sebastian Coe says. The World Athletics Championships concluded on Sunday night, following nine days of competition during which the sport's household names lit up Japan's National stadium, which four years ago hosted the Olympics behind closed doors because of the coronavirus pandemic. The opening days of the championships were dominated by discussion around the conditions the athletes had to contend with - particularly in the endurance events - as temperatures exceeding 30C were accompanied by stifling humidity above 90%. In addition to the quadrennial Olympic Games and biennial World Championships, World Athletics will next year stage its inaugural Ultimate Championships in Budapest to bridge the gap between its major events and provide a definitive conclusion to the 2026 season. But Lord Coe said events such as the marathon may need to be held separately, at a different time of the year, at such competitions to protect athletes from unsafe conditions in future. \"Yes, I do see that,\" Coe told BBC Sport. \"That's not easy because you're going even into the autumn, early winter months to cities that are still very hot. \"But I do think this is probably going to have to happen at some stage, and sooner rather than later.\"\nAccording to Coe, World Athletics research has found that 70% of athletes report that climate change and the heat is impacting their training and competition programmes. The 68-year-old said \"governments haven't really stepped up to the plate\" on the issue and added: \"I can't see beyond the inevitability of having to collectively, as Olympic sports and probably the Olympic movement, really re-engineering what the international calendar looks like. \"From World Athletics' point of view, there will be some branding issues but I'm not sure that we can go on asking some of our endurance-based athletes to be competing at times of the year which are really are going to hit their performances and are probably putting them at risk as well. \"This has to be addressed.\"\nCoe also confirmed that all athletes who competed in the female category at the World Championships had undergone gene testing - and that the test is here to stay.", "content_token_count": 391, "mistral_embedding": null}, {"position": 1, "content_text": "\"From World Athletics' point of view, there will be some branding issues but I'm not sure that we can go on asking some of our endurance-based athletes to be competing at times of the year which are really are going to hit their performances and are probably putting them at risk as well. \"This has to be addressed.\"\nCoe also confirmed that all athletes who competed in the female category at the World Championships had undergone gene testing - and that the test is here to stay. However, he reiterated that details of any athletes who may have been prevented from competing as a result of the test, carried out via cheek swab, will remain confidential. Its introduction comes amid reports, external that between 50 and 60 athletes who went through male puberty have been finalists in the female category at global and continental track and field championships since 2000. Athletics 'moving in right direction', says Bolt\nPole vault superstar Armand Duplantis, Americans Sydney McLaughlin-Levrone and Noah Lyles, and Kenya's Faith Kipyegon were among the sport's big names who captivated often sell-out 60,000 crowds at Japan's National Stadium. Swedish 25-year-old Duplantis produced arguably the standout moment when he soared to his 14th world record with his final attempt after winning a third consecutive world title. McLaughlin-Levrone went close to the controversial 40-year women's 400m world record, while compatriot Lyles emulated Jamaican great Usain Bolt with a fourth consecutive world 200m title and Kipyegon won a historic fourth world 1500m title. Speaking at a closing news conference on Sunday, Coe said: \"I think our sport is in really good shape. We have broader appeal and a greater level of following than we have had for a long, long time. \"We have athletes that are performing as well as any generation I have witnessed.\"\nNext year, they will compete in Budapest as World Athletics hosts its three-day Ultimate Championships featuring world champions, Diamond League winners and the top performers of 2026 - and which offers a $10m (£7.5m) prize pot. Earlier this year, the inaugural season of Michael Johnson's Grand Slam Track - which excluded field events - ended with the final leg being cancelled because of financial concerns and its future appears in doubt with some athletes still yet to receive money.", "content_token_count": 381, "mistral_embedding": null}, {"position": 2, "content_text": "\"We have athletes that are performing as well as any generation I have witnessed.\"\nNext year, they will compete in Budapest as World Athletics hosts its three-day Ultimate Championships featuring world champions, Diamond League winners and the top performers of 2026 - and which offers a $10m (£7.5m) prize pot. Earlier this year, the inaugural season of Michael Johnson's Grand Slam Track - which excluded field events - ended with the final leg being cancelled because of financial concerns and its future appears in doubt with some athletes still yet to receive money. Coe said World Athletics intend to use the Ultimate Championship to test ideas which could later become part of the sport's established major stages. \"We've got a four-year cycle and one of those years we don't get the billion eyeballs on our sport. As a global sport we can't afford that,\" said Coe. \"We thought about what we can do that is different and can be an incubator for change, where we can introduce and test things and maybe they turn up here in a World Championship. \"I think it will be it'll be a great addition but there is a lot more work to do.\"\nSporting icon Bolt, winner of eight Olympic gold medals, told BBC Sport he believes the Ultimate Championships is a \"good start\" as the sport seeks to grow. \"The sport needs to evolve and I feel like track and field is moving in the right direction,\" Bolt said. \"Track and field is probably one of the poorest sports and it's one of the biggest sports, so the incentive is very big and I'm happy for the athletes because they deserve it.\"\nRegarding the high level of performances witnessed in Tokyo, and increasing standards overall, Coe pinpointed technological advances in the synthetic track surface, the shoes worn by the athletes, and improving coaching skills and methods. Coe said: \"Some of the performances we've seen here are simply jaw-dropping. \"The lovely thing about it is that the sport is moving ahead both as track and field in pretty much the same progress.\"\nHe added: \"There's never enough prize money for the athletes, we always want more. Where possible, we want the athletes to benefit from the proceeds of growth.\"", "content_token_count": 375, "mistral_embedding": null}, {"position": 3, "content_text": "\"Track and field is probably one of the poorest sports and it's one of the biggest sports, so the incentive is very big and I'm happy for the athletes because they deserve it.\"\nRegarding the high level of performances witnessed in Tokyo, and increasing standards overall, Coe pinpointed technological advances in the synthetic track surface, the shoes worn by the athletes, and improving coaching skills and methods. Coe said: \"Some of the performances we've seen here are simply jaw-dropping. \"The lovely thing about it is that the sport is moving ahead both as track and field in pretty much the same progress.\"\nHe added: \"There's never enough prize money for the athletes, we always want more. Where possible, we want the athletes to benefit from the proceeds of growth.\"", "content_token_count": 129, "mistral_embedding": null}, {"position": 4, "content_text": "Coe said: \"Some of the performances we've seen here are simply jaw-dropping. \"The lovely thing about it is that the sport is moving ahead both as track and field in pretty much the same progress.\"\nHe added: \"There's never enough prize money for the athletes, we always want more. Where possible, we want the athletes to benefit from the proceeds of growth.\"", "content_token_count": 62, "mistral_embedding": null}, {"position": 5, "content_text": "\"The lovely thing about it is that the sport is moving ahead both as track and field in pretty much the same progress.\"\nHe added: \"There's never enough prize money for the athletes, we always want more. Where possible, we want the athletes to benefit from the proceeds of growth.\"", "content_token_count": 50, "mistral_embedding": null}, {"position": 6, "content_text": "Where possible, we want the athletes to benefit from the proceeds of growth.\"", "content_token_count": 13, "mistral_embedding": null}], "adb9a52a-4c0a-4028-86c9-1b623b14f864": [{"position": 0, "content_text": "Donald Trump’s administration has announced an annual $100,000 fee on H-1B visa applications, a type of US visa that allows companies to hire foreign workers in skilled occupations such as IT, healthcare and engineering. The new proclamation, signed on Friday, brought a mixed bag of emotions for hundreds of thousands of workers on H-1B visas, as well as many question. Here’s what you need to know. What is an H-1B visa? The H-1B program dates back to 1990, when George HW Bush signed a new bill into law in an effort to encourage the immigration of highly skilled workers, including scientists, engineers and educators, to address labor shortages in specialized fields. This temporary visa category allows employers to petition for “highly educated” foreign professionals to work in “specialty occupations” that require at least a bachelor’s degree or the equivalent. The H-1B visa is typically granted for an initial period of three years and can be extended up to a maximum of six years. There are as many as 730,000 H-1B holders in the US, and an additional 550,000 dependents, including spouses and children, who together make up nearly 1.3 million residents, according to a January 2025 report from fwd.us, an immigration and criminal justice advocacy group. Previously, the cost for an H-1B ranged from about $1,700 to $4,500, depending on whether the visa was expedited. Each year, Congress caps H-1B visas at 85,000, awarded through a lottery system. To enter, companies pay a $215 registration fee, followed by the thousands of dollars more in application fees and legal costs if selected. What changes did Trump announce? The Trump administration increased the fee for skilled foreign workers applying for H-1B visas to $100,000, claiming the visas were being “abused” to undercut American wages and outsource IT jobs. The new fee went into effect on Sunday. The fees are intended to be paid by companies sponsoring the visas, rather than applicants themselves. The Trump administration implemented the fee in an effort to encourage companies to hire more American workers.\n“Either the person is very valuable to the company and America, or they are going to depart and the company is going to hire an American,” said US commerce secretary Howard Lutnick at Friday’s briefing. “And that’s the point of immigration. Hire Americans and make sure the people coming in are the top, top people.", "content_token_count": 392, "mistral_embedding": null}, {"position": 1, "content_text": "The fees are intended to be paid by companies sponsoring the visas, rather than applicants themselves. The Trump administration implemented the fee in an effort to encourage companies to hire more American workers.\n“Either the person is very valuable to the company and America, or they are going to depart and the company is going to hire an American,” said US commerce secretary Howard Lutnick at Friday’s briefing. “And that’s the point of immigration. Hire Americans and make sure the people coming in are the top, top people. Stop the nonsense.”\nOn Tuesday, the Trump administration proposed a new process that would move away from the current random selection process to a weighted selection process, according to a Federal Register notice. The proposed process would give more weight to H-1B applications for roles with the highest wages if applications for the visas exceed the annual cap of 85,000. Roles that meet or exceed the highest level of wages for the industry, as laid out by the Bureau of Labor Statistics, would be entered into the visa selection pool four times, roles that meet or exceed the second-highest level of wages would be entered three times and so on. DHS said that, if the proposal is finalized, the goal would be to incentivize “employers to offer higher wages, or to petition for positions requiring higher skills and higher skilled aliens, that are commensurate with higher wage levels”.\n“The proposed process would favor the allocation of H-1B visas to higher skilled and higher paid aliens, while maintaining the opportunity for employers to secure H-1B workers at all wage levels,” the proposal reads. Trump thoughts on H-1B have been anything but steady. Last year, the president told the New York Post he supported the program, calling himself a “believer” in the visas, and that he has used it “many times”. But, more recently, he said that adding such costly hurdles to the visa process will help protect American jobs. “I think it’s going to be a fantastic thing, and we’re going to take that money and we’re going to reduce taxes, we’re going to reduce debt,” Trump said on Friday. What industries are affected?", "content_token_count": 361, "mistral_embedding": null}, {"position": 2, "content_text": "Last year, the president told the New York Post he supported the program, calling himself a “believer” in the visas, and that he has used it “many times”. But, more recently, he said that adding such costly hurdles to the visa process will help protect American jobs. “I think it’s going to be a fantastic thing, and we’re going to take that money and we’re going to reduce taxes, we’re going to reduce debt,” Trump said on Friday. What industries are affected? Tech workers make up one of the largest groups of H-1B visa recipients, and the fee announcement sent the tech industry scrambling, with businesses in Silicon Valley urging staff not to travel outside the country amid early confusion over the new process. Stem (science, technology, engineering and mathematics) industries rely heavily on the H-1B program, with roughly two-thirds of H-1B jobs in computer-related roles. According to US Citizenship and Immigration Services, Amazon, Google, Meta, Microsoft and Apple were among the companies that employed the most H-1B visa holders last year. Several prominent tech industry leaders, including Elon Musk, Alphabet chief executive Sundar Pichai and Microsoft chief executive Satya Nadella, were at one point H-1B visa holders. Employers also use the program to bring in educators and healthcare workers to the US. What has the reaction been? Economists have warned that the move could lead to weaker economic growth, since higher costs for employers will make it harder to attract foreign talent. They have also pointed to a potential “brain drain” as skilled international graduates are forced to leave the US. In India, the largest beneficiary of H-1B visas last year, officials said the fee would have humanitarian consequences “by way of the disruption caused for families”. The Indian government said it “hopes that these disruptions can be addressed suitably by the US authorities” and emphasised that the exchange of skilled workers had “contributed enormously” to both nations. Other business leaders, however, have praised Trump’s move, including Netflix co-founder Reed Hastings. In a post on X, Hastings said he had worked on H-1B politics for 30 years and said the added cost would allot visas for “very high-value jobs”. He said the move would eliminate the lottery and give employers more certainty. Most major Silicon Valley companies have not publicly commented. Companies have sent internal memos to employees.", "content_token_count": 389, "mistral_embedding": null}, {"position": 3, "content_text": "The Indian government said it “hopes that these disruptions can be addressed suitably by the US authorities” and emphasised that the exchange of skilled workers had “contributed enormously” to both nations. Other business leaders, however, have praised Trump’s move, including Netflix co-founder Reed Hastings. In a post on X, Hastings said he had worked on H-1B politics for 30 years and said the added cost would allot visas for “very high-value jobs”. He said the move would eliminate the lottery and give employers more certainty. Most major Silicon Valley companies have not publicly commented. Companies have sent internal memos to employees. Before the new fee was announced, the visa had been contested across the political spectrum. The democratic socialist senator Bernie Sanders, for example, has been a longtime critic of the H-1B visa, arguing that the program replaces American jobs with cheaper international workers. Do people who already hold an H-1B visa have to worry? Lutnick said the fee would be paid annually, and would only apply to people seeking a new visa. Amid initial confusion over the weekend, White House press secretary Karoline Leavitt clarified that the fee applies only to new applicants, and that those who already hold H-1B visas and are currently outside of the country would not be charged $100,000 to re-enter.\n“H-1B visa holders can leave and re-enter the country to the same extent as they normally would; whatever ability they have to do that is not impacted by yesterday’s proclamation,” Leavitt said. “This applies only to new visas, not renewals, and not current visa holders.”", "content_token_count": 261, "mistral_embedding": null}, {"position": 4, "content_text": "Amid initial confusion over the weekend, White House press secretary Karoline Leavitt clarified that the fee applies only to new applicants, and that those who already hold H-1B visas and are currently outside of the country would not be charged $100,000 to re-enter.\n“H-1B visa holders can leave and re-enter the country to the same extent as they normally would; whatever ability they have to do that is not impacted by yesterday’s proclamation,” Leavitt said. “This applies only to new visas, not renewals, and not current visa holders.”", "content_token_count": 88, "mistral_embedding": null}], "13667bdc-32e2-4138-869f-aabba5f9911b": [{"position": 0, "content_text": "While seemingly every company is talking about building agents this year, far fewer have done it. It’s easy to let your imagination run wild with how agents can transform your business, but many teams are unsure where to begin, how to make progress, and where to set expectations. In this guide, we’ll walk through a framework for going from idea to impact— illustrated with a real-world example of building an email agent. Step 1: Define your agent’s job with examples\nChoose something realistic and something that requires an agent. Pick something you could teach a smart intern. If your best intern could never complete the task given enough time and resources, the task may be unrealistic or too ambitious. Prove you can get the basics down before activating expert mode. Start by coming up with 5-10 concrete examples of the task. This serves two purposes:\n- First, it validates that your idea is well-scoped - not too trivial or vague\n- Second, gives you a benchmark for measuring performance later. Example: Building an Email Agent\nAt this step, we’d define what tasks our agent needs to handle, which likely includes:\n- Prioritize urgent emails from key stakeholders\n- Schedule meetings based on calendar availability\n- Ignore spam or emails that don't require responses\n- Answer product questions based on company documentation\nRed flags to avoid:\n- If you can’t come up with concrete examples, your scope is probably too broad.\n- Using an agent when traditional software would work better (e.g., when the logic is simple, fixed, and already implemented elsewhere). Agents are slow, expensive, and can be finicky at times. If traditional software gets the job done - just use that!\n- Expecting magic that doesn't exist (e.g., connecting to APIs or datasets that don’t exist or can’t be built yet)\nStep 2: Design operating procedure\nWrite up a detailed standard operating procedure (SOP), with step-by-step instructions for how a human would perform the task or process. This step helps confirm that you’ve chosen a problem with a clear, reasonable scope. It also surfaces the key steps, decisions, and tools your agent will likely need to handle—laying the groundwork for what to build.", "content_token_count": 364, "mistral_embedding": null}, {"position": 1, "content_text": "If traditional software gets the job done - just use that!\n- Expecting magic that doesn't exist (e.g., connecting to APIs or datasets that don’t exist or can’t be built yet)\nStep 2: Design operating procedure\nWrite up a detailed standard operating procedure (SOP), with step-by-step instructions for how a human would perform the task or process. This step helps confirm that you’ve chosen a problem with a clear, reasonable scope. It also surfaces the key steps, decisions, and tools your agent will likely need to handle—laying the groundwork for what to build. Example: Building an Email Agent\nFor our email agent, a step-by-step procedure could look like below:\n- Analyze email content and sender context to categorize response priority\n- Checks calendar availability; schedules video conference meeting\n- Draft a response based on the email, sender, and scheduling context\n- Send the email after a quick human review and approval\nWriting this out helps ensure the task is scoped appropriately, and surfaces the tools and logic our agent will need to handle. Step 3: Build MVP with prompt\nChoosing a place to start is important. If your agent is complex, trying to do it all in one go is too ambitious. Start by designing the agent’s architecture outlined by the SOP: how it will flow, what decisions it needs to make, and where LLM reasoning is essential. Then, build an MVP by focusing on the most critical LLM reasoning task(s) (e.g., classification, decision-making) and creating a prompt that handles them well. Most agents fail because the LLM can't reason well enough for the task. Getting a single prompt working with hand-fed data will help you build up confidence before proceeding to build the full agent. Prompt engineering tools like LangSmith can help streamline this process, from managing prompt versions, to testing across scenarios or datasets, and tracking performance over time as you iterate. Keep it simple by:\n- Starting with manual inputs for any data or context the prompt needs (hold off on automation for now)\n- Testing against your outlined examples from Step 1 to validate performance across common use cases\n- Focusing on getting the LLM reasoning right\nExample: Building an Email Agent\nAt this stage, we’re identifying and solving one high-leverage reasoning task to start with.", "content_token_count": 380, "mistral_embedding": null}, {"position": 2, "content_text": "Prompt engineering tools like LangSmith can help streamline this process, from managing prompt versions, to testing across scenarios or datasets, and tracking performance over time as you iterate. Keep it simple by:\n- Starting with manual inputs for any data or context the prompt needs (hold off on automation for now)\n- Testing against your outlined examples from Step 1 to validate performance across common use cases\n- Focusing on getting the LLM reasoning right\nExample: Building an Email Agent\nAt this stage, we’re identifying and solving one high-leverage reasoning task to start with. For our email agent, that might mean focusing just on classifying emails by urgency and intent (e.g., meeting request, support questions), as this is a foundational step that the rest of the agent depends on. Start by writing a core prompt that does just this, with hand-fed inputs like:\n- Email content: “Can we meet next week about LangChain’s product roadmap?”\n- Sender: “Jeff Bezos”, Title: “CEO of Amazon”\n- Output: Intent = “Meeting Request”, Urgency = “High”\nOnce the model consistently gets this right across your test cases, you’ll have confidence that the core logic is sound—and a strong foundation to build on. Step 4: Connect & Orchestrate\nNow that we have a working prompt, it’s time to connect the prompt to real data and user inputs. Start by identifying what context or data the prompt needs—such as email content, calendar availability, and documentation of products—and plan how to access it programmatically (e.g., via APIs, databases, or file systems). Then, write orchestration logic to connect the right data into your prompt. In simple cases, this might just mean passing inputs directly. For more complex workflows, you may need agentic logic to decide which data sources to query, when to call them, and how to combine their outputs before prompting the LLM. Example: Building an Email Agent\nFor our email agent, this step could involve integrating with the Gmail API (to read incoming emails), Google Calendar API (to check availability), and a CRM or contact database (to enrich sender context).", "content_token_count": 344, "mistral_embedding": null}, {"position": 3, "content_text": "Then, write orchestration logic to connect the right data into your prompt. In simple cases, this might just mean passing inputs directly. For more complex workflows, you may need agentic logic to decide which data sources to query, when to call them, and how to combine their outputs before prompting the LLM. Example: Building an Email Agent\nFor our email agent, this step could involve integrating with the Gmail API (to read incoming emails), Google Calendar API (to check availability), and a CRM or contact database (to enrich sender context). We’d then build orchestration logic like the following :\n- A new email triggers the agent\n- The agent fetches sender info from the CRM or via web search\n- It passes the full context into the prompt to determine urgency and whether a response is needed\n- If a meeting is appropriate, it checks calendar availability and proposes times\n- The agent drafts a response\n- After human review, it sends the email\nStep 5: Test & Iterate\nBegin by manually testing your MVP using the examples you defined in Step 1. The goal is to verify that your agent is producing reasonable, accurate outputs for your core use cases. If your system involves multiple LLM calls or steps, it’s helpful to set up tracing using tools like LangSmith to visualize the flow and debug how decisions are made at each stage. Once manual testing is solid, scale to automated testing to ensure consistency and catch edge cases. Teams will often beef up examples to a few dozen to get a better sense of the agent’s strengths and weaknesses.", "content_token_count": 270, "mistral_embedding": null}, {"position": 4, "content_text": "The goal is to verify that your agent is producing reasonable, accurate outputs for your core use cases. If your system involves multiple LLM calls or steps, it’s helpful to set up tracing using tools like LangSmith to visualize the flow and debug how decisions are made at each stage. Once manual testing is solid, scale to automated testing to ensure consistency and catch edge cases. Teams will often beef up examples to a few dozen to get a better sense of the agent’s strengths and weaknesses. This also helps you quantify performance before adding more complexity:\n- Run all examples (original + new) programmatically through your agent\n- Define automated success metrics — this forces clarity around your agent’s expected behavior\n- Use human review selectively to catch issues that metrics might miss\nExample: Building an Email Agent\nFor the email agent, we’d want to define and test success across several key areas:\n- Tone and Safety: Responses should be professional, respectful, and free of hallucinated or inappropriate content\n- Intent & Priority Detection: Emails should be correctly categorized and prioritized based on sender and content\n- Tool Usage Efficiency: The agent should trigger only the necessary tools (e.g., avoid checking the calendar if no scheduling is required)\n- Draft Quality: Suggested replies should be clear, relevant, and accurate based on the input context\nStep 6: Deploy, Scale, and Refine\nOnce your MVP is performing reliably, begin expanding its scope—adding new capabilities, broader use cases, or even multi-agent workflows. For every new feature or integration, repeat the testing process from Step 5 to ensure you’re not breaking existing functionality. When ready, deploy to production in users' hands. LangGraph Platform allows you to quickly ship, scale, and manage your agents with one-click deployment. Monitor how people actually use your agent. Tools like LangSmith let you trace your agent’s actions in real time, making it easier to spot spikes in cost, accuracy issues, or latency. Real-world usage often differs from your initial assumptions, and these insights can reveal gaps, surface unexpected needs, and guide prioritization during your next iteration. The key is treating launch as the beginning of iteration, not the end of development. Example: Building an Email Agent\nAfter deploying our email agent, we might discover unaddressed use cases through monitoring traffic and common use cases. These emerging patterns signal opportunities to expand scope.", "content_token_count": 394, "mistral_embedding": null}, {"position": 5, "content_text": "Tools like LangSmith let you trace your agent’s actions in real time, making it easier to spot spikes in cost, accuracy issues, or latency. Real-world usage often differs from your initial assumptions, and these insights can reveal gaps, surface unexpected needs, and guide prioritization during your next iteration. The key is treating launch as the beginning of iteration, not the end of development. Example: Building an Email Agent\nAfter deploying our email agent, we might discover unaddressed use cases through monitoring traffic and common use cases. These emerging patterns signal opportunities to expand scope. From there, we can iteratively add new integrations and update our prompts and orchestration logic—always validating each addition with tests and user feedback before scaling further. Conclusion\nThis process is designed to help you build agents that are grounded in clear use cases, tested against real examples, and shaped by real-world feedback. It’s not just about getting an agent to run, but about building something useful, reliable, and aligned with how people actually work. Whether you're automating email triage or orchestrating complex workflows, these six steps offer a practical path from idea to impact. But the work doesn’t stop at deployment—the best agents are built through iteration. So start small, stay user-focused, and keep refining.", "content_token_count": 209, "mistral_embedding": null}, {"position": 6, "content_text": "Conclusion\nThis process is designed to help you build agents that are grounded in clear use cases, tested against real examples, and shaped by real-world feedback. It’s not just about getting an agent to run, but about building something useful, reliable, and aligned with how people actually work. Whether you're automating email triage or orchestrating complex workflows, these six steps offer a practical path from idea to impact. But the work doesn’t stop at deployment—the best agents are built through iteration. So start small, stay user-focused, and keep refining.", "content_token_count": 89, "mistral_embedding": null}, {"position": 7, "content_text": "It’s not just about getting an agent to run, but about building something useful, reliable, and aligned with how people actually work. Whether you're automating email triage or orchestrating complex workflows, these six steps offer a practical path from idea to impact. But the work doesn’t stop at deployment—the best agents are built through iteration. So start small, stay user-focused, and keep refining.", "content_token_count": 63, "mistral_embedding": null}, {"position": 8, "content_text": "Whether you're automating email triage or orchestrating complex workflows, these six steps offer a practical path from idea to impact. But the work doesn’t stop at deployment—the best agents are built through iteration. So start small, stay user-focused, and keep refining.", "content_token_count": 41, "mistral_embedding": null}, {"position": 9, "content_text": "But the work doesn’t stop at deployment—the best agents are built through iteration. So start small, stay user-focused, and keep refining.", "content_token_count": 21, "mistral_embedding": null}, {"position": 10, "content_text": "So start small, stay user-focused, and keep refining.", "content_token_count": 8, "mistral_embedding": null}], "71f4e0cb-9573-49f4-a735-d98a16e05de9": [{"position": 0, "content_text": "Nonverbal behaviors such as posture, gestures, and gaze are essential for conveying internal states, both consciously and unconsciously, in human interaction. For robots to interact more naturally with humans, robot movement design should likewise integrate expressive qualities—such as intention, attention, and emotions—alongside traditional functional considerations like task fulfillment, spatial constraints, and time efficiency. In this paper, we present the design and prototyping of a lamp-like robot that explores the interplay between functional and expressive objectives in movement design. Using a research-through-design methodology, we document the hardware design process, define expressive movement primitives, and outline a set of interaction scenario storyboards. We propose a framework that incorporates both functional and expressive utilities during movement generation, and implement the robot behavior sequences in different function- and social- oriented tasks. Through a user study comparing expression-driven versus function-driven movements across six task scenarios, our findings indicate that expression-driven movements significantly enhance user engagement and perceived robot qualities. This effect is especially pronounced in social-oriented tasks. Related readings and updates. EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning\nJanuary 24, 2025research area Human-Computer Interaction\nThis paper introduces a framework, called EMOTION, for generating expressive motion sequences in humanoid robots, enhancing their ability to engage in human-like non-verbal communication. Non-verbal cues such as facial expressions, gestures, and body movements play a crucial role in effective interpersonal interactions. Despite the advancements in robotic behaviors, existing methods often fall short in mimicking the diversity and subtlety of…\nARMADA: Augmented Reality for Robot Manipulation and Robot-Free Data Acquisition\nDecember 17, 2024research area Data Science and Annotation, research area Human-Computer Interaction\nTeleoperation for robot imitation learning is bottlenecked by hardware availability. Can high-quality robot data be collected without a physical robot? We present a system for augmenting Apple Vision Pro with real-time virtual robot feedback. By providing users with an intuitive understanding of how their actions translate to robot motions, we enable the collection of natural barehanded human data that is compatible with the limitations of…", "content_token_count": 330, "mistral_embedding": null}, {"position": 1, "content_text": "Despite the advancements in robotic behaviors, existing methods often fall short in mimicking the diversity and subtlety of…\nARMADA: Augmented Reality for Robot Manipulation and Robot-Free Data Acquisition\nDecember 17, 2024research area Data Science and Annotation, research area Human-Computer Interaction\nTeleoperation for robot imitation learning is bottlenecked by hardware availability. Can high-quality robot data be collected without a physical robot? We present a system for augmenting Apple Vision Pro with real-time virtual robot feedback. By providing users with an intuitive understanding of how their actions translate to robot motions, we enable the collection of natural barehanded human data that is compatible with the limitations of…", "content_token_count": 105, "mistral_embedding": null}, {"position": 2, "content_text": "Can high-quality robot data be collected without a physical robot? We present a system for augmenting Apple Vision Pro with real-time virtual robot feedback. By providing users with an intuitive understanding of how their actions translate to robot motions, we enable the collection of natural barehanded human data that is compatible with the limitations of…", "content_token_count": 55, "mistral_embedding": null}, {"position": 3, "content_text": "We present a system for augmenting Apple Vision Pro with real-time virtual robot feedback. By providing users with an intuitive understanding of how their actions translate to robot motions, we enable the collection of natural barehanded human data that is compatible with the limitations of…", "content_token_count": 45, "mistral_embedding": null}, {"position": 4, "content_text": "By providing users with an intuitive understanding of how their actions translate to robot motions, we enable the collection of natural barehanded human data that is compatible with the limitations of…", "content_token_count": 31, "mistral_embedding": null}], "6d8c8ad9-b74e-4335-a34f-01fe2ce1be3a": [{"position": 0, "content_text": "After centuries of trying, we’ve yet to arrive at a perfect way to map colour\nCan colour be understood geometrically? If so, what’s the best way to map it out, capturing the variables of hue, brightness and saturation? These questions have deep implications for art, physics and perception, and have been pondered for centuries. In this extraordinary dive into how thinkers from Isaac Newton to today have mapped colour, the French video essayist Alessandro Roussel of the YouTube channel ScienceClic makes the case that there’s not just one way to map colour in two or three dimensions, but many – each of them communicating different truths about the nature of the phenomenon. To capture the mutable nature of colour in the human experience, and our always-evolving understanding of it, Roussel concludes with the fascinating story of ‘olo’ – a new ‘impossible colour’, outside of the usual visible spectrum, which scientists were only recently able to produce in the laboratory.\nvideoAstronomy\nVisualisations explore what the deep future holds for our night sky\n6 minutes\nvideoBiology\nDive deep into an egg cell to see how ageing reboots when a new life begins\n2 minutes\nvideoMusic\nA riveting audiovisual dive into what makes sounds harmonious, or not\n28 minutes\nvideoMathematics\nSpiral into the ‘golden ratio’ – and separate the myths from the maths\n4 minutes\nvideoHistory of science\nHow we came to know the size of the Universe – and what mysteries remain\n26 minutes\nvideoOceans and water\nA stunning visualisation explores the intricate circulatory system of our oceans\n5 minutes\nvideoHistory of science\nIdeas ‘of pure genius’ – how astronomers have measured the Universe across history\n29 minutes\nvideoAnimals and humans\nOne man’s quest to save an orphaned squirrel, as narrated by David Attenborough\n14 minutes\nvideoComputing and artificial intelligence\nA future in which ‘artificial scientists’ make discoveries may not be far away\n9 minutes", "content_token_count": 313, "mistral_embedding": null}, {"position": 1, "content_text": "To capture the mutable nature of colour in the human experience, and our always-evolving understanding of it, Roussel concludes with the fascinating story of ‘olo’ – a new ‘impossible colour’, outside of the usual visible spectrum, which scientists were only recently able to produce in the laboratory.\nvideoAstronomy\nVisualisations explore what the deep future holds for our night sky\n6 minutes\nvideoBiology\nDive deep into an egg cell to see how ageing reboots when a new life begins\n2 minutes\nvideoMusic\nA riveting audiovisual dive into what makes sounds harmonious, or not\n28 minutes\nvideoMathematics\nSpiral into the ‘golden ratio’ – and separate the myths from the maths\n4 minutes\nvideoHistory of science\nHow we came to know the size of the Universe – and what mysteries remain\n26 minutes\nvideoOceans and water\nA stunning visualisation explores the intricate circulatory system of our oceans\n5 minutes\nvideoHistory of science\nIdeas ‘of pure genius’ – how astronomers have measured the Universe across history\n29 minutes\nvideoAnimals and humans\nOne man’s quest to save an orphaned squirrel, as narrated by David Attenborough\n14 minutes\nvideoComputing and artificial intelligence\nA future in which ‘artificial scientists’ make discoveries may not be far away\n9 minutes", "content_token_count": 201, "mistral_embedding": null}], "52e79d22-fd8d-4bbe-b5df-19ee10c9a5ab": [{"position": 0, "content_text": "Every day, millions of customers search for books in various formats (audiobooks, e-books, and physical books) across Amazon and Audible. Traditional keyword autocomplete suggestions, while helpful, usually require several steps before customers find their desired content. Audible took on the challenge of making book discovery more intuitive and personalized while reducing the number of steps to purchase. We developed an instant visual autocomplete system that enhances the search experience across Amazon and Audible. As the user begins typing a query, our solution provides visual previews with book covers, enabling direct navigation to relevant landing pages instead of the search result page. It also delivers real-time personalized format recommendations and incorporates multiple searchable entities, such as book pages, author pages, and series pages. Our system needed to understand user intent from just a few keystrokes and determine the most relevant books to display, all while maintaining low latency for millions of queries. Using historical search data, we match keystrokes to products, transforming partial inputs into meaningful search suggestions. To ensure quality, we implemented confidence-based filtering mechanisms, which are particularly important for distinguishing between general queries like \"mystery\" and specific title searches. To reflect customers’ most recent interests, the system applies time-decay functions to long historical user interaction data. To meet the unique requirements of each use case, we developed two distinct technical approaches. On Audible, we deployed a deep pairwise-learning-to-rank (DeepPLTR) model. The DeepPLTR model considers pairs of books and learns to assign a higher score to the one that better matches the customer query. The DeepPLTR model’s architecture consists of three specialized towers. The left tower factors in contextual features and recent search patterns using a long-short-term-memory model, which processes data sequentially and considers its prior decisions when issuing a new term in the sequence. The middle tower handles keyword and item engagement history. The right tower factors in customer taste preferences and product descriptions to enable personalization. The model learns from paired examples, but at runtime, it relies on books’ absolute scores to assemble a ranked list. For Amazon, we implemented a two-stage modeling approach involving a probabilistic information-retrieval model to determine the book title that best matches each keyword and a second model that personalizes the book format (audiobooks, e-books, and physical books). This dual-strategy approach maintains low latency while still enabling personalization.", "content_token_count": 385, "mistral_embedding": null}, {"position": 1, "content_text": "The right tower factors in customer taste preferences and product descriptions to enable personalization. The model learns from paired examples, but at runtime, it relies on books’ absolute scores to assemble a ranked list. For Amazon, we implemented a two-stage modeling approach involving a probabilistic information-retrieval model to determine the book title that best matches each keyword and a second model that personalizes the book format (audiobooks, e-books, and physical books). This dual-strategy approach maintains low latency while still enabling personalization. In practice, a customer who types \"dungeon craw\" in the search bar now sees a visual recommendation for the book Dungeon Crawler Carl, complete with book cover, reducing friction by bypassing a search results page and sending the customer directly to the product detail page. On Audible, the system also personalizes autocomplete results and enriches the discovery experience with relevant connections. These include links to the author's complete works (Matt Dinniman's author page) and, for titles that belong to a series, links to the full collection (such as the Dungeon Crawler Carl series). On Amazon, when the customer clicks on the title, the model personalizes the right book-format (audiobooks, e-books, physical books) recommendation and directs the customer to the right product detail page. In both cases, after the customer has entered a certain number of keystrokes, the system employs a model to detect customer intent (e.g., book title intent for Amazon or author intent for Audible) and determine which visual widget should be displayed. Audible and Amazon books’ visual autocomplete provides customers with more relevant content more rapidly than traditional autocomplete, and its direct navigation reduces the number of steps to find and access desired books — all while handling millions of queries at low latency. This technology is not just about making book discovery easier; it is laying the foundation for future improvements in search personalization and visual discovery across Amazon's ecosystem. Acknowledgements: Jiun Kim, Sumit Khetan, Armen Stepanyan, Jack Xuan, Nathan Brothers, Eddie Chen, Vincent Lee, Soumy Ladha, Justine Luo, Yuchen Zeng, David Torres, Gali Deutsch, Chaitra Ramdas, Christopher Gomez, Sharmila Tamby, Melissa Ma, Cheng Luo, Jeffrey Jiang, Pavel Fedorov, Ronald Denaux, Aishwarya Vasanth, Azad Bajaj, Mary Heer, Adam Lowe, Jenny Wang, Cameron Cramer, Emmanuel Ankrah, Lydia Diaz, Suzette Islam, Fei Gu, Phil Weaver, Huan Xue, Kimmy Dai, Evangeline Yang, Chao Zhu, Anvy Tran, Jessica Wu, Xiaoxiong Huang, Jiushan Yang", "content_token_count": 393, "mistral_embedding": null}, {"position": 2, "content_text": "This technology is not just about making book discovery easier; it is laying the foundation for future improvements in search personalization and visual discovery across Amazon's ecosystem. Acknowledgements: Jiun Kim, Sumit Khetan, Armen Stepanyan, Jack Xuan, Nathan Brothers, Eddie Chen, Vincent Lee, Soumy Ladha, Justine Luo, Yuchen Zeng, David Torres, Gali Deutsch, Chaitra Ramdas, Christopher Gomez, Sharmila Tamby, Melissa Ma, Cheng Luo, Jeffrey Jiang, Pavel Fedorov, Ronald Denaux, Aishwarya Vasanth, Azad Bajaj, Mary Heer, Adam Lowe, Jenny Wang, Cameron Cramer, Emmanuel Ankrah, Lydia Diaz, Suzette Islam, Fei Gu, Phil Weaver, Huan Xue, Kimmy Dai, Evangeline Yang, Chao Zhu, Anvy Tran, Jessica Wu, Xiaoxiong Huang, Jiushan Yang", "content_token_count": 106, "mistral_embedding": null}, {"position": 3, "content_text": "Acknowledgements: Jiun Kim, Sumit Khetan, Armen Stepanyan, Jack Xuan, Nathan Brothers, Eddie Chen, Vincent Lee, Soumy Ladha, Justine Luo, Yuchen Zeng, David Torres, Gali Deutsch, Chaitra Ramdas, Christopher Gomez, Sharmila Tamby, Melissa Ma, Cheng Luo, Jeffrey Jiang, Pavel Fedorov, Ronald Denaux, Aishwarya Vasanth, Azad Bajaj, Mary Heer, Adam Lowe, Jenny Wang, Cameron Cramer, Emmanuel Ankrah, Lydia Diaz, Suzette Islam, Fei Gu, Phil Weaver, Huan Xue, Kimmy Dai, Evangeline Yang, Chao Zhu, Anvy Tran, Jessica Wu, Xiaoxiong Huang, Jiushan Yang", "content_token_count": 79, "mistral_embedding": null}], "459297f4-3c2e-4009-8df9-c3694e16a4d4": [{"position": 0, "content_text": "The Agent Client Protocol standardizes communication between code editors (IDEs, text-editors, etc.) and coding agents (programs that use generative AI to autonomously modify code).The protocol is still under development, but it should be complete enough to build interesting user experiences using it. AI coding agents and editors are tightly coupled but interoperability isn’t the default. Each editor must build custom integrations for every agent they want to support, and agents must implement editor-specific APIs to reach users. This creates several problems:\nIntegration overhead: Every new agent-editor combination requires custom work\nLimited compatibility: Agents work with only a subset of available editors\nDeveloper lock-in: Choosing an agent often means accepting their available interfaces\nACP solves this by providing a standardized protocol for agent-editor communication, similar to how the Language Server Protocol (LSP) standardized language server integration.Agents that implement ACP work with any compatible editor. Editors that support ACP gain access to the entire ecosystem of ACP-compatible agents. This decoupling allows both sides to innovate independently while giving developers the freedom to choose the best tools for their workflow. ACP assumes that the user is primarily in their editor, and wants to reach out and use agents to assist them with specific tasks.Agents run as sub-processes of the code editor, and communicate using JSON-RPC over stdio. The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.", "content_token_count": 264, "mistral_embedding": null}, {"position": 1, "content_text": "ACP assumes that the user is primarily in their editor, and wants to reach out and use agents to assist them with specific tasks.Agents run as sub-processes of the code editor, and communicate using JSON-RPC over stdio. The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.", "content_token_count": 87, "mistral_embedding": null}, {"position": 2, "content_text": "The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.", "content_token_count": 50, "mistral_embedding": null}]}, "user_settings": {}}